<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Android DispSync 详解]]></title>
    <url>%2F2019%2F10%2F07%2Fall-about-dispsync%2F</url>
    <content type="text"><![CDATA[DispSync 是什么？在 Android 4.1 的时候，Google 提出了著名的 “Project Butter”，引入了 VSYNC，把 app 画图，SurfaceFlinger 合成的时间点都规范了起来，减少了掉帧，增强了渲染的流畅度。但是这里有个问题，因为 VSYNC 是由硬件产生的，一旦产生了你就必须开始干活，不灵活。假设有这么一种需求，我希望在 VSYNC 偏移一段时间以后再干活，那么这个是硬件 VSYNC 提供不了，所以这个时候就必须引入软件模型。而 DispSync 就是为了解决这个需求引入的软件模型。DispSync 类似于一个 PLL（phase lock loop，锁相回路），它通过接收硬件 VSYNC，然后给其他关心硬件 VSYNC 的组件（SurfaceFlinger 和需要渲染的 app）在指定的偏移以后发送软件 VSYNC，并且当误差在可接受的范围内，将会关闭硬件 VSYNC。谷歌的这篇文档里面详细有一张非常准确的图： （为了方便，后面所有的硬件 VSYNC 使用 HW-VSYNC 代指，软件 VSYNC 使用 SW-VSYNC 代指） 综述前面提到 DispSync 是一个模拟 HW-VSYNC 的软件模型，在这个模型里面包含几个部分： DispSync DispSync 的主体，主要负责启动 DispSyncThread，接收 HW-VSYNC 并且更新计算出 SW-VSYNC 间隔—— mPeriod DispSyncThread DispSync 的一个内部线程类，主要功能是模拟 HW-VSYNC 的行为，大部分时间都处于阻塞状态，利用 DispSync 算出的 mPeriod，周期性地在下一个 SW-VSYNC 时间点（加了偏移的）醒来去通知对 VSYNC 感兴趣的 Listener —— DispSyncSource DispSyncSource SurfaceFlinger 的一个内部类，实现了 DispSync::Callback 的接口，DispSyncThread 和 EventThread 的中间人 EventThread VSYNC 的接收实体，收到 DispSync 的 SF-VSYNC 再进行分发，SurfaceFlinger 和 app 分别有自己的 EventThread—— sfEventThread 和 appEventThread Connection EventThread 内部类，任何一个对 VSYNC 感兴趣的（SurfaceFlinger，需要渲染画面的 app）都会在 EventThread 里面抽象为一个 Connection EventControlThread 大部分博客都将其描述为硬件 VSYNC 的“闸刀”，也就是负责控制硬件 VSYNC 的开关 MessageQueue SurfaceFlinger 用来在 sfEventThread 注册 DisplayEventReceiver app 用来在 appEventThread 注册 下面来详细描述一下整个初始化的流程。 初始化首先说明一下，DispSync 的初始化流程初看是十分复杂的，首先它涉及到比较多的线程，并且线程在很多时候是处于阻塞状态的，导致整个流程处于一个不连续的状态。因此谁把哪个线程唤醒了就变得十分重要，这也是理解整个初始化过程中的一个难点。 DispSync 和 DispSyncThread-01DispSync 在 SurfaceFlinger 里只有一个实例 —— mPrimaryDipsSync，它在 SurfaceFlinger 的初始化分两部分，创建实例 mPrimaryDispSync 然后执行其 init() 方法。DispSync 的构造函数非常简单，都是一些赋值： 1234567891011DispSync::DispSync(const char* name) : mName(name), mRefreshSkipCount(0), mThread(new DispSyncThread(name)) &#123;&#125; explicit DispSyncThread(const char* name) : mName(name), mStop(false), mPeriod(0), mPhase(0), mReferenceTime(0), mWakeupLatency(0), mFrameNumber(0) &#123;&#125; 然后来看 init() 方法： 12345678910111213141516void DispSync::init(bool hasSyncFramework, int64_t dispSyncPresentTimeOffset) &#123; mIgnorePresentFences = !hasSyncFramework; mPresentTimeOffset = dispSyncPresentTimeOffset; mThread-&gt;run(&quot;DispSync&quot;, PRIORITY_URGENT_DISPLAY + PRIORITY_MORE_FAVORABLE); // set DispSync to SCHED_FIFO to minimize jitter struct sched_param param = &#123;0&#125;; param.sched_priority = 2; if (sched_setscheduler(mThread-&gt;getTid(), SCHED_FIFO, &amp;param) != 0) &#123; ALOGE(&quot;Couldn&apos;t set SCHED_FIFO for DispSyncThread&quot;); &#125; reset(); beginResync(); ...&#125; DispSycn::init() 最主要的就是工作就是让 DispSyncThread 运行起来，并且将其调度优先级改为 SCHED_FIFO，这样做的目的是什么呢？我们前面提到，DispSyncThread 大部分时间都在阻塞，它会“睡”到下次 SW-VSYNC 开始的时间戳，因此当其被唤醒的时候，高优先级能够保证其尽快地被调度，减少误差。执行完 mThread-&gt;run() 以后，就会开始执行 DispSyncThread::threadLoop()： 123456789101112131415161718192021virtual bool threadLoop() &#123; status_t err; nsecs_t now = systemTime(SYSTEM_TIME_MONOTONIC); while (true) &#123; Vector&lt;CallbackInvocation&gt; callbackInvocations; nsecs_t targetTime = 0; &#123; // Scope for lock Mutex::Autolock lock(mMutex); ... if (mStop) &#123; return false; &#125; if (mPeriod == 0) &#123; err = mCond.wait(mMutex); // 第一次初始化，由于 mPeriod 为 0，所以会先 block 在这里 目前 DispSyncThread 会阻塞在这里，我们接下去看。 EventThread-01在 SurfaceFlinger 初始化的时候，会创建两个 EventThread，一个给 SurfaceFlinger，一个给 app： 123456789101112131415161718192021void SurfaceFlinger::init() &#123; ... // start the EventThread mEventThreadSource = std::make_unique&lt;DispSyncSource&gt;(&amp;mPrimaryDispSync, SurfaceFlinger::vsyncPhaseOffsetNs, true, &quot;app&quot;); mEventThread = std::make_unique&lt;impl::EventThread&gt;(mEventThreadSource.get(), [this]() &#123; resyncWithRateLimit(); &#125;, impl::EventThread::InterceptVSyncsCallback(), &quot;appEventThread&quot;); mSfEventThreadSource = std::make_unique&lt;DispSyncSource&gt;(&amp;mPrimaryDispSync, SurfaceFlinger::sfVsyncPhaseOffsetNs, true, &quot;sf&quot;); mSFEventThread = std::make_unique&lt;impl::EventThread&gt;(mSfEventThreadSource.get(), [this]() &#123; resyncWithRateLimit(); &#125;, [this](nsecs_t timestamp) &#123; mInterceptor-&gt;saveVSyncEvent(timestamp); &#125;, &quot;sfEventThread&quot;); 前面提到，DispSyncSource 是 DispSyncThread 和 EventThread 的中间人，先来看一下 DispSyncSource 的构造函数： 1234567891011121314class DispSyncSource final : public VSyncSource, private DispSync::Callback &#123;public: DispSyncSource(DispSync* dispSync, nsecs_t phaseOffset, bool traceVsync, const char* name) : mName(name), mValue(0), mTraceVsync(traceVsync), mVsyncOnLabel(String8::format(&quot;VsyncOn-%s&quot;, name)), mVsyncEventLabel(String8::format(&quot;VSYNC-%s&quot;, name)), mDispSync(dispSync), mCallbackMutex(), mVsyncMutex(), mPhaseOffset(phaseOffset), mEnabled(false) &#123;&#125; 请注意这里有一个非常重要的点，就是 mVsyncEventLabel(String8::format(&quot;VSYNC-%s&quot;, name))。SurfaceFlinger 的 DispSyncSource 传进来的 name 是 “sf”，app 的 DispSyncSource 传进来的 name 是 “app”，所以连起来就是 “VSYNC-sf” 和 “VSYNC-app”。为什么说重要呢？来看一段 systrace： 这里面的 VSYNC-app 和 VSYNC-sf 就是说的 DispSyncSource，至于它的意义，后面会提到。 然后 DispSyncSource 作为参数传给 EventThread 的构造函数： 12345678910111213141516171819202122232425262728EventThread::EventThread(VSyncSource* src, ResyncWithRateLimitCallback resyncWithRateLimitCallback, InterceptVSyncsCallback interceptVSyncsCallback, const char* threadName) : mVSyncSource(src), mResyncWithRateLimitCallback(resyncWithRateLimitCallback), mInterceptVSyncsCallback(interceptVSyncsCallback) &#123; for (auto&amp; event : mVSyncEvent) &#123; event.header.type = DisplayEventReceiver::DISPLAY_EVENT_VSYNC; event.header.id = 0; event.header.timestamp = 0; event.vsync.count = 0; &#125; mThread = std::thread(&amp;EventThread::threadMain, this); pthread_setname_np(mThread.native_handle(), threadName); pid_t tid = pthread_gettid_np(mThread.native_handle()); // Use SCHED_FIFO to minimize jitter constexpr int EVENT_THREAD_PRIORITY = 2; struct sched_param param = &#123;0&#125;; param.sched_priority = EVENT_THREAD_PRIORITY; if (pthread_setschedparam(mThread.native_handle(), SCHED_FIFO, &amp;param) != 0) &#123; ALOGE(&quot;Couldn&apos;t set SCHED_FIFO for EventThread&quot;); &#125; set_sched_policy(tid, SP_FOREGROUND);&#125; 构造函数的最主要功能就是把 EventThread 的线程主体 threadMain 运行起来并且设置其优先级为 SCHED_FIFO，接下来看 threadMain： 12345678910111213141516void EventThread::threadMain() NO_THREAD_SAFETY_ANALYSIS &#123; std::unique_lock&lt;std::mutex&gt; lock(mMutex); while (mKeepRunning) &#123; DisplayEventReceiver::Event event; Vector&lt;sp&lt;EventThread::Connection&gt; &gt; signalConnections; signalConnections = waitForEventLocked(&amp;lock, &amp;event); // dispatch events to listeners... const size_t count = signalConnections.size(); for (size_t i = 0; i &lt; count; i++) &#123; const sp&lt;Connection&gt;&amp; conn(signalConnections[i]); // now see if we still need to report this event status_t err = conn-&gt;postEvent(event); ... &#125;&#125; threadMain 的主要工作是调用 waitForEventLocked 等待一个 Event，然后在一个个地通知 signalConnections。至于这个 Event 和 signalConnections 分别是什么，后面会具体描述。现在先来看一下 waitForEventLocked 的逻辑： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// This will return when (1) a vsync event has been received, and (2) there was// at least one connection interested in receiving it when we started waiting.Vector&lt;sp&lt;EventThread::Connection&gt; &gt; EventThread::waitForEventLocked( std::unique_lock&lt;std::mutex&gt;* lock, DisplayEventReceiver::Event* event) &#123; Vector&lt;sp&lt;EventThread::Connection&gt; &gt; signalConnections; while (signalConnections.isEmpty() &amp;&amp; mKeepRunning) &#123; bool eventPending = false; bool waitForVSync = false; size_t vsyncCount = 0; nsecs_t timestamp = 0; // 在前面 EventThread 的构造函数里面已经把 mVSyncEvent 数组内的所有 timestamp 都置为 0 // 因此在第一次初始化的时候，这个循环会直接退出 for (int32_t i = 0; i &lt; DisplayDevice::NUM_BUILTIN_DISPLAY_TYPES; i++) &#123; ... &#125; // 第一次初始化的时候 mDisplayEventConnections 的数组也为空，count 为 0 size_t count = mDisplayEventConnections.size(); if (!timestamp &amp;&amp; count) &#123; ... &#125; // 第一次初始化不执行这个循环 for (size_t i = 0; i &lt; count;) &#123; ... &#125; // timestamp 为 0， waitForVSync 为 false if (timestamp &amp;&amp; !waitForVSync) &#123; ... &#125; else if (!timestamp &amp;&amp; waitForVSync) &#123; ... &#125; // eventPending 为 false，符合条件 if (!timestamp &amp;&amp; !eventPending) &#123; if (waitForVSync) &#123; ... // waitForVSync 为 false，进入 else &#125; else &#123; // 最终，在第一次初始化的时候，EventThread 就阻塞在这里了 mCondition.wait(*lock); &#125; &#125; &#125; ...&#125; 好，到这里，SurfaceFlinger 创建的两个 EventThread 都会阻塞在上面代码提到的地方，SurfaceFlinger 的初始化继续执行。 补充：SurfaceFlinger 的启动首先说明一下 mEventQueue 是在哪里被初始化的。是在 SurfaceFlinger 的另一个方法： 提到这里就需要 SurfaceFlinger 是怎么启动和初始化的。SurfaceFlinger 作为系统最基本最核心的服务之一，是通过 init.rc 的方式进行启动的（内容在 frameworks/native/services/surfaceflinger/surfaceflinger.rc）: 123456service surfaceflinger /system/bin/surfaceflinger class core animation user system group graphics drmrpc readproc input onrestart restart zygote ... 然后就需要提到 SurfaceFlinger 的组成部分，init.rc 里面提到的 /system/bin/surfaceflinger 这个二进制文件，由 main_surfaceflinger.cpp 这个文件编译得到；而上面提到 DispSync，EventThread 等，都被编译到了 libsurfaceflinger.so 这个库。这也给了我们一个启示：当我们在自己调试 SurfaceFlinger 的时候，大部分时间都只需要重新编译 libsurfaceflinger.so 这个文件即可。 回来简单看一下 SurfaceFlinger 是如何启动的，来看看 main_surfaceflinger.cpp： 123456int main(int, char **) &#123; ... sp&lt;SurfaceFlinger&gt; flinger = DisplayUtils::getInstance()-&gt;getSFInstance(); ... flinger-&gt;init(); ... 这里的重点就是这个 sp&lt;SurfaceFlinger&gt;，当被 sp 指针引用的时候，会触发 onFirstRef() 函数： 1234void SurfaceFlinger::onFirstRef()&#123; mEventQueue-&gt;init(this);&#125; 这样，就走到了 MessageQueue 部分了： MessageQueue接着 EventThread，然后就执行到这里： 123void SurfaceFlinger::init() &#123; ... mEventQueue-&gt;setEventThread(mSFEventThread.get()); mEventQueue 在前面的 SurfaceFlinger::onFirstRef() 中完成了初始化： 12345void MessageQueue::init(const sp&lt;SurfaceFlinger&gt;&amp; flinger) &#123; mFlinger = flinger; mLooper = new Looper(true); mHandler = new Handler(*this);&#125; 接着来看一下很重要的 setEventThread()： 123456789101112131415void MessageQueue::setEventThread(android::EventThread* eventThread) &#123; if (mEventThread == eventThread) &#123; return; &#125; if (mEventTube.getFd() &gt;= 0) &#123; mLooper-&gt;removeFd(mEventTube.getFd()); &#125; mEventThread = eventThread; mEvents = eventThread-&gt;createEventConnection(); mEvents-&gt;stealReceiveChannel(&amp;mEventTube); mLooper-&gt;addFd(mEventTube.getFd(), 0, Looper::EVENT_INPUT, MessageQueue::cb_eventReceiver, this);&#125; 重点来了，前面创建的 SurfaceFlinger 的 EventThread 被作为参数传给了 setEventThread，并且执行了 EventThread 的 createEventConnection()。（注意，需要时时刻刻地记住，现在处理的 SurfaceFlinger 的 EventThread） （后面为了方便，将使用 sfEventThread 指代 SurfaceFlinger 的 EventThread；使用 appEventThread 指代 app 的 EventThread） EventThread::Connection123sp&lt;BnDisplayEventConnection&gt; EventThread::createEventConnection() const &#123; return new Connection(const_cast&lt;EventThread*&gt;(this));&#125; 在这里，sfEventThread 迎来了第一个（同时也是唯一的） Connection： 123456789101112131415EventThread::Connection::Connection(EventThread* eventThread) : count(-1), mEventThread(eventThread), mChannel(gui::BitTube::DefaultSize) &#123;&#125;void EventThread::Connection::onFirstRef() &#123; // NOTE: mEventThread doesn&apos;t hold a strong reference on us mEventThread-&gt;registerDisplayEventConnection(this);&#125;status_t EventThread::registerDisplayEventConnection( const sp&lt;EventThread::Connection&gt;&amp; connection) &#123; std::lock_guard&lt;std::mutex&gt; lock(mMutex); mDisplayEventConnections.add(connection); mCondition.notify_all(); return NO_ERROR;&#125; MessageQueue 调用 sfEventThread 的 createEventConnection 创建一个 Connection。由于 sp 指针的作用，将会调用 Connection::onFirstRef，最终这个 Connection 会被添加到 mDisplayEventConnections 并且唤醒在 EventThread - 01 中阻塞的线程。 EventThread-02在前面把 EventThread 唤醒后，由于 signalConnections 为空，继续循环。然后由于新加入的 Connection count 为 -1，所以这个 EventThread 会继续阻塞，不过此时 mDisplayEventConnections 里面已经有一个 Connection 了。接着看下去。 EventControlThread-01SurfaceFlinger::init() 接着运行到这里： 1234void SurfaceFlinger::init() &#123; ... mEventControlThread = std::make_unique&lt;impl::EventControlThread&gt;( [this](bool enabled) &#123; setVsyncEnabled(HWC_DISPLAY_PRIMARY, enabled); &#125;); 主要提一下的是，这个传进来的参数是一个 Lambda 表达式，具体的语法不讲。稍微解释一下这里传进来的 Lambda 表达式的意义就是，捕获列表为 SurfaceFlinger 本身，接受一个布尔参数，当这个 Lamda 表达式被调用的时候，会调用 SurfaceFlinger::setVsyncEnabled() 这个函数，这个函数后面会提到，也是一个很重要的函数。 EventControlThread 的构造函数的主要内容也是启动一个线程： 12345678910111213141516171819202122232425EventControlThread::EventControlThread(EventControlThread::SetVSyncEnabledFunction function) : mSetVSyncEnabled(function) &#123; pthread_setname_np(mThread.native_handle(), &quot;EventControlThread&quot;); pid_t tid = pthread_gettid_np(mThread.native_handle()); setpriority(PRIO_PROCESS, tid, ANDROID_PRIORITY_URGENT_DISPLAY); set_sched_policy(tid, SP_FOREGROUND);&#125;void EventControlThread::threadMain() NO_THREAD_SAFETY_ANALYSIS &#123; auto keepRunning = true; auto currentVsyncEnabled = false; while (keepRunning) &#123; mSetVSyncEnabled(currentVsyncEnabled); std::unique_lock&lt;std::mutex&gt; lock(mMutex); // keepRunning 为 true，currentVsyncEnabled 为 false，mVsyncEnabled 默认值为 false，mKeepRunning 默认值为 true，因此 Lambda 表达式为 false，线程阻塞 mCondition.wait(lock, [this, currentVsyncEnabled, keepRunning]() NO_THREAD_SAFETY_ANALYSIS &#123; return currentVsyncEnabled != mVsyncEnabled || keepRunning != mKeepRunning; &#125;); currentVsyncEnabled = mVsyncEnabled; keepRunning = mKeepRunning; &#125;&#125; 此时，EventControlThread 也会陷入阻塞之中。而 SurfaceFlinger 也将迎来初始化中最为复杂的一步。 唤醒所有线程至此，SurfaceFlinger 总共起了四个线程 —— DispSyncThread，两个 EvenThread 和 EventControlThread，并且这四个线程全都处于阻塞状态。导致这些线程处于阻塞状态的原因是： DispSyncThread: mPeriod 为 0 EventThread: Connection-&gt;count 为 -1 EventControlThread: mVsyncEnabled 为 false 然后让我们一个个将其唤醒。 EventThread-03接下来的 SurfaceFlinger 会进行非常复杂的初始化操作，EventThread 唤醒相关的调用流程如下（这里借用了这位大佬《Android SurfaceFlinger SW Vsync模型》的内容，写得非常棒，在学习的过程中能够得到了很大的启发）： 12345678initializeDisplays(); flinger-&gt;onInitializeDisplays(); setTransactionState(state, displays, 0); setTransactionFlags(transactionFlags); signalTransaction(); mEventQueue-&gt;invalidate(); mEvents-&gt;requestNextVsync() //mEvents是Connection实例 EventThread-&gt;requestNextVsync(this); 1234567void EventThread::requestNextVsync(const sp&lt;EventThread::Connection&gt;&amp; connection) &#123; ... if (connection-&gt;count &lt; 0) &#123; connection-&gt;count = 0; mCondition.notify_all(); &#125;&#125; 在这里把前面创建的那个 Connection 的 count 置为 0，并且唤醒阻塞的 EventThread，这个时候，mDisplayEventConnections 不为空并且 count 不为 -1，可以正常地运行了，EventThread::waitForEventLocked() 走到了这里： 12345678910111213141516171819202122 &#125; else if (!timestamp &amp;&amp; waitForVSync) &#123; // we have at least one client, so we want vsync enabled // (TODO: this function is called right after we finish // notifying clients of a vsync, so this call will be made // at the vsync rate, e.g. 60fps. If we can accurately // track the current state we could avoid making this call // so often.) enableVSyncLocked(); &#125; void EventThread::enableVSyncLocked() &#123; // 一般都为 false if (!mUseSoftwareVSync) &#123; // never enable h/w VSYNC when screen is off if (!mVsyncEnabled) &#123; mVsyncEnabled = true; mVSyncSource-&gt;setCallback(this); mVSyncSource-&gt;setVSyncEnabled(true); &#125; &#125; mDebugVsyncEnabled = true;&#125; 调用了 DispSyncSource::setCallback()，将 EventThread 和 DispSyncSource 联系在了一起： 1234void setCallback(VSyncSource::Callback* callback) override&#123; Mutex::Autolock lock(mCallbackMutex); mCallback = callback;&#125; 接着调用 DispSyncSource::setVSyncEnabled： 12345678void setVSyncEnabled(bool enable) override &#123; Mutex::Autolock lock(mVsyncMutex); // true if (enable) &#123; status_t err = mDispSync-&gt;addEventListener(mName, mPhaseOffset, static_cast&lt;DispSync::Callback*&gt;(this)); ...&#125; 最终调用了 DispSync::addEventListener： 12345678910111213141516171819202122232425status_t addEventListener(const char* name, nsecs_t phase, DispSync::Callback* callback) &#123; if (kTraceDetailedInfo) ATRACE_CALL(); Mutex::Autolock lock(mMutex); // 保证了 mEventListeners 的唯一性 for (size_t i = 0; i &lt; mEventListeners.size(); i++) &#123; if (mEventListeners[i].mCallback == callback) &#123; return BAD_VALUE; &#125; &#125; EventListener listener; listener.mName = name; listener.mPhase = phase; listener.mCallback = callback; listener.mLastEventTime = systemTime() - mPeriod / 2 + mPhase - mWakeupLatency; mEventListeners.push(listener); // 唤醒 DispSyncThread mCond.signal(); return NO_ERROR;&#125; 把 DispSyncSource 加到 mEventListeners，将 DispSync 和 DispSyncSource 联系在了一起，并且把前面阻塞的 DispSyncThread 唤醒，但是由于 mPeriod 还是为 0，因此 DispSyncThread 还是会继续阻塞。 不过此时从调用关系已经初步可以看到前面我说的那句 DispSyncSource 是 DispSync 和 EventThread 的中间人 是正确的了。 接着来看 DispSyncThread。 DispSync 和 DispSyncThread-02设置 mPeriod 的流程如下（依旧引用了这位大佬的《Android SurfaceFlinger SW Vsync模型》的内容，再次感谢）： 12345initializeDisplays(); flinger-&gt;onInitializeDisplays(); setPowerModeInternal() resyncToHardwareVsync(true); repaintEverything(); 这里把 SurfaceFlinger::resyncToHardwareVsync() 分为两部分，先看上部分： 12345678910111213141516171819202122void SurfaceFlinger::resyncToHardwareVsync(bool makeAvailable) &#123; Mutex::Autolock _l(mHWVsyncLock); if (makeAvailable) &#123; mHWVsyncAvailable = true; &#125; else if (!mHWVsyncAvailable) &#123; // Hardware vsync is not currently available, so abort the resync // attempt for now return; &#125; const auto&amp; activeConfig = getBE().mHwc-&gt;getActiveConfig(HWC_DISPLAY_PRIMARY); const nsecs_t period = activeConfig-&gt;getVsyncPeriod(); mPrimaryDispSync.reset(); // 设置 mPeriod mPrimaryDispSync.setPeriod(period); // 默认为 false if (!mPrimaryHWVsyncEnabled) &#123; mPrimaryDispSync.beginResync(); // 上部分结束 在 DispSync::setPeriod() 里面给 mPeriod 赋值，并且把 DispSyncThread 唤醒： 1234567891011121314151617181920void DispSync::setPeriod(nsecs_t period) &#123; Mutex::Autolock lock(mMutex); mPeriod = period; mPhase = 0; mReferenceTime = 0; mThread-&gt;updateModel(mPeriod, mPhase, mReferenceTime);&#125;void updateModel(nsecs_t period, nsecs_t phase, nsecs_t referenceTime) &#123; if (kTraceDetailedInfo) ATRACE_CALL(); Mutex::Autolock lock(mMutex); mPeriod = period; mPhase = phase; mReferenceTime = referenceTime; ALOGV(&quot;[%s] updateModel: mPeriod = %&quot; PRId64 &quot;, mPhase = %&quot; PRId64 &quot; mReferenceTime = %&quot; PRId64, mName, ns2us(mPeriod), ns2us(mPhase), ns2us(mReferenceTime)); // 这里把 DispSyncThread 唤醒 mCond.signal();&#125; 至此，DispSyncThread 也开始运转。 EventControlThread-02接着看 SurfaceFlinger::resyncToHardwareVsync() 的下半部分： 123456789101112 ... mEventControlThread-&gt;setVsyncEnabled(true); mPrimaryHWVsyncEnabled = true; &#125;&#125;void EventControlThread::setVsyncEnabled(bool enabled) &#123; std::lock_guard&lt;std::mutex&gt; lock(mMutex); mVsyncEnabled = enabled; // 把 EventControlThread 唤醒 mCondition.notify_all();&#125; 把 EventControlThread 唤醒以后，会重新把 SurfaceFlinger 传进来的那个被 Lambda 表达式包裹的 SurfaceFlinger::setVsyncEnabled() 重新执行一下： 12345678910111213141516171819202122232425262728293031323334void SurfaceFlinger::setVsyncEnabled(int disp, int enabled) &#123; ATRACE_CALL(); Mutex::Autolock lock(mStateLock); getHwComposer().setVsyncEnabled(disp, enabled ? HWC2::Vsync::Enable : HWC2::Vsync::Disable);&#125;void HWComposer::setVsyncEnabled(int32_t displayId, HWC2::Vsync enabled) &#123; if (displayId &lt; 0 || displayId &gt;= HWC_DISPLAY_VIRTUAL) &#123; ALOGD(&quot;setVsyncEnabled: Ignoring for virtual display %d&quot;, displayId); return; &#125; RETURN_IF_INVALID_DISPLAY(displayId); // NOTE: we use our own internal lock here because we have to call // into the HWC with the lock held, and we want to make sure // that even if HWC blocks (which it shouldn&apos;t), it won&apos;t // affect other threads. Mutex::Autolock _l(mVsyncLock); auto&amp; displayData = mDisplayData[displayId]; if (enabled != displayData.vsyncEnabled) &#123; ATRACE_CALL(); auto error = displayData.hwcDisplay-&gt;setVsyncEnabled(enabled); RETURN_IF_HWC_ERROR(error, displayId); displayData.vsyncEnabled = enabled; char tag[16]; snprintf(tag, sizeof(tag), &quot;HW_VSYNC_ON_%1u&quot;, displayId); // 在 systrace 看到的就是在这里 ATRACE_INT(tag, enabled == HWC2::Vsync::Enable ? 1 : 0); &#125;&#125; 在这里，真正地去开启 HW-VSync。然后由于 SurfaceFlinger 接收了 HW-VSync，然后辗转发给 DispSync，DispSync 接收，校正 SW-VSYNC。而整个 DispSync SurfaceFlinger 部分的初始化的流程也最终完成。 注意，上面说的是 SurfaceFlinger 部分。前面提到，总共有两个 EventThread，而上面分析的都是 sfEventThread，下面简单地描述一下 appEventThread 的流程，其实 EventThread 到 DispSync 这部分都是一致的，只是 EventThread 的 Connection 的注册流程不一样。sfEventThread 是 MessageQueue 去注册 Connection，而 appEventThread 则是另一种方法。 appEventThreadSurfaceFlinger 接收 VSYNC 是为了合成，因此 sfEventThread 的 Connection 只有一个，就是 SurfaceFlinger 本身；而 app 接收 VSYNC 是为了画帧，appEventThread 会有很多很多个 Connection。 app 本身是如何在 appEventThread 注册一个 Connection 的，与这篇文章的主体有点偏移，这个可以另开一篇文章来详细说明，流程也是非常复杂，这里只简单地描述：核心就是 libgui 下面的 DisplayEventReceiver，它在初始化的时候会调用 SurfaceFlinger::createEventConnection： 12345678sp&lt;IDisplayEventConnection&gt; SurfaceFlinger::createDisplayEventConnection( ISurfaceComposer::VsyncSource vsyncSource) &#123; if (vsyncSource == eVsyncSourceSurfaceFlinger) &#123; return mSFEventThread-&gt;createEventConnection(); &#125; else &#123; return mEventThread-&gt;createEventConnection(); &#125;&#125; 然后后面的流程就跟前面的一致了。 小结通过上面的描述，依据各个类的依赖关系，其实可以总结出这么一个图： 请注意箭头方向。 运作流程前面提到，引入 DispSync 的目的是为了通过 SF-VSYNC 来模拟 HW-VSYNC 的行为并且通过加入 offset 来让通知时机变得灵活。因此理解整个 DispSync 的流程就可以归结为下面几个部分：SF-VSYNC 通知周期 mPeriod 的计算；SF-VSYNC 的模拟方式以及 SF-VSYNC 传递流程，分别来看。 mPeriod 计算逻辑前面提到，DispSync 通过接收 HW-VSYNC 并且更新计算出 SW-VSYNC 间隔—— mPeriod，首先看一下 DispSync 是如何收到 HW-VSYNC。 先看一下 SurfaceFlinger 这个类： 1234class SurfaceFlinger : public BnSurfaceComposer, public PriorityDumper, private IBinder::DeathRecipient, private HWC2::ComposerCallback SurfaceFlinger 实现了 HW2::ComposerCallback 的接口，然后当 HW-VSYNC 到来的时候，HWC 会将 HW-VSYNC 发生的时间戳发给 SurfaceFlinger，然后 SurfaceFlinger 会转发给 DispSync： 12345678910111213141516171819202122232425262728class ComposerCallbackBridge : public Hwc2::IComposerCallback &#123;public: ... Return&lt;void&gt; onVsync(Hwc2::Display display, int64_t timestamp) override &#123; mCallback-&gt;onVsyncReceived(mSequenceId, display, timestamp); return Void(); &#125; ...&#125;;void SurfaceFlinger::onVsyncReceived(int32_t sequenceId, hwc2_display_t displayId, int64_t timestamp) &#123; ... &#123; // Scope for the lock Mutex::Autolock _l(mHWVsyncLock); if (type == DisplayDevice::DISPLAY_PRIMARY &amp;&amp; mPrimaryHWVsyncEnabled) &#123; needsHwVsync = mPrimaryDispSync.addResyncSample(timestamp); &#125; &#125; // 这个很重要，后面会提到 if (needsHwVsync) &#123; enableHardwareVsync(); &#125; else &#123; disableHardwareVsync(false); &#125;&#125; 重点看 DispSync 怎么处理这些 HW-VSYNC，是在 addResyncSample() 这个函数： 12345678910111213141516171819202122232425262728bool DispSync::addResyncSample(nsecs_t timestamp) &#123; Mutex::Autolock lock(mMutex); size_t idx = (mFirstResyncSample + mNumResyncSamples) % MAX_RESYNC_SAMPLES; mResyncSamples[idx] = timestamp; if (mNumResyncSamples == 0) &#123; mPhase = 0; mReferenceTime = timestamp; mThread-&gt;updateModel(mPeriod, mPhase, mReferenceTime); &#125; if (mNumResyncSamples &lt; MAX_RESYNC_SAMPLES) &#123; mNumResyncSamples++; &#125; else &#123; mFirstResyncSample = (mFirstResyncSample + 1) % MAX_RESYNC_SAMPLES; &#125; updateModelLocked(); if (mNumResyncSamplesSincePresent++ &gt; MAX_RESYNC_SAMPLES_WITHOUT_PRESENT) &#123; resetErrorLocked(); &#125; ... bool modelLocked = mModelUpdated &amp;&amp; mError &lt; (kErrorThreshold / 2); return !modelLocked;&#125; 这里需要重点说明这里面几个变量的意义（在 DispSync.h 这个头文件里面有说明）： mPeriod这个就是 DispSync 根据 HW-VSYNC，计算出来的 SW-VSYNC 的时间间隔，单位是纳秒。这里有人可能会有疑问，这个值的意义在哪？硬件是以一个固定的时间间隔去发 HW-VSYNC，为什么还需要去计算一个新的时间间隔？直接跟 HW-VSYNC 的时间间隔一致不行吗？这个当做作业留给大家思考。 mPhase这个说实话我看了好久一直都看不懂这个值的意义 mReferenceTime这个是第一次收到 HW-VSYNC 的时间戳，用来当做 DispSync 的参考标准 mWakeupLatencyDispSyncThread 是通过睡到下一次 SW-VSYNC 应该发生的时间戳来模拟 HW-SYNC 的，但是这种“睡”到特定时间点肯定是有延迟的。通过计算睡醒的时间戳和目标时间戳就可以算出这个延迟，总延迟不能超过 1.5ms mResyncSample长度 32，用来记录收到硬件 VSYNC 的时间戳的数组，不过被解释为一个 ring buffer，新的会覆盖旧的 mFirstResyncSample记录了 mResyncSample 这个 ring buffer 的开头 mNumResyncSamples接收到硬件 VSYNC 的个数 DispSync 将从 SurfaceFlinger 发来的 HW-VSYNC 的时间戳都给记录到一个 ring buffer，当有了足够多的 HW-VSYNC 了以后（目前是 6 个即以上），就可以开始来拟合 SF-VSYNC 的间隔 mPeriod 了，是在 DispSync::updateModelLocked() 里面计算的，核心算法就在这里了。分为两部分，一部分是 mPeriod 的计算： 1234567891011121314151617void DispSync::updateModelLocked() &#123; if (mNumResyncSamples &gt;= MIN_RESYNC_SAMPLES_FOR_UPDATE) &#123; nsecs_t durationSum = 0; nsecs_t minDuration = INT64_MAX; nsecs_t maxDuration = 0; for (size_t i = 1; i &lt; mNumResyncSamples; i++) &#123; size_t idx = (mFirstResyncSample + i) % MAX_RESYNC_SAMPLES; size_t prev = (idx + MAX_RESYNC_SAMPLES - 1) % MAX_RESYNC_SAMPLES; nsecs_t duration = mResyncSamples[idx] - mResyncSamples[prev]; durationSum += duration; minDuration = min(minDuration, duration); maxDuration = max(maxDuration, duration); &#125; durationSum -= minDuration + maxDuration; mPeriod = durationSum / (mNumResyncSamples - 3); ... mPeriod 的计算十分简单，把所有的 HW-VSYNC 前后相减算出 HW-VSYNC 的时间间隔，然后去掉一个最小值和最大值，然后所有 HW-VSYNC 的时间戳之和除以总个数就是 mPeriod 了。这里有一个问题就是为什么在最后除的时候是除数是 3？其实很简单，因为前面的 for 循环是从 1 开始算起的，所以循环结束一下 durationSum 其实是 mNumResyncSamples - 1 个 HW-VSYNC 的总和，然后再去掉一个最大和最小，所以总数是 mNumResyncSamples - 3。 另一部分是 mPhase 的计算，这一块看上去好像挺复杂的，甚至还有三角函数： 1234567891011121314151617181920212223242526272829303132333435363738394041...double sampleAvgX = 0;double sampleAvgY = 0;// scale 的意义是，每 ms 代表了多少度。（总量除以总个数等于每个的值）double scale = 2.0 * M_PI / double(mPeriod);// Intentionally skip the first samplefor (size_t i = 1; i &lt; mNumResyncSamples; i++) &#123; size_t idx = (mFirstResyncSample + i) % MAX_RESYNC_SAMPLES; // sample 是误差 nsecs_t sample = mResyncSamples[idx] - mReferenceTime; // 这里 (sample % mPeriod) 看上去挺唬人的，但是其实就是保证 sample 不会大于或者等于 mPeriod，否则这里的 samplePhase 算出来就是 2π 了 // 所以这里 samplePhase 算出来的就是把误差转成度数 double samplePhase = double(sample % mPeriod) * scale; // 这两个后面是为了用来计算误差平均的度数 sampleAvgX += cos(samplePhase); sampleAvgY += sin(samplePhase);&#125;sampleAvgX /= double(mNumResyncSamples - 1);sampleAvgY /= double(mNumResyncSamples - 1);// 根据等比关系，算出平局误差度数对应的 ns 值mPhase = nsecs_t(atan2(sampleAvgY, sampleAvgX) / scale);ALOGV(&quot;[%s] mPhase = %&quot; PRId64, mName, ns2us(mPhase));if (mPhase &lt; -(mPeriod / 2)) &#123; mPhase += mPeriod; ALOGV(&quot;[%s] Adjusting mPhase -&gt; %&quot; PRId64, mName, ns2us(mPhase));&#125;if (kTraceDetailedInfo) &#123; ATRACE_INT64(&quot;DispSync:Period&quot;, mPeriod); ATRACE_INT64(&quot;DispSync:Phase&quot;, mPhase + mPeriod / 2);&#125;// Artificially inflate the period if requested.mPeriod += mPeriod * mRefreshSkipCount;mThread-&gt;updateModel(mPeriod, mPhase, mReferenceTime);mModelUpdated = true; 上面的逻辑其实可以用下图来阐述： 而 mPhase 最终是根据下面的等比公式计算出来的： $$ \frac{2\pi}{mPeriod} = \frac{Angle}{mPhase} $$ 最后，看一下 DispSync::addResyncSample 这个函数的返回值，这个返回值非常重要，当通过统计 SW-VSYNC 的误差小于阈值的时候（这个误差的计算涉及到了 Fence，目前我对这部分内容理解得还不是很透彻，等彻底理解了以后再来填坑），返回 true 给 SurfaceFlinger 的时候，SurfaceFlinger 则会调用 SurfaceFlinger::disableHardwareVsync 把 HW-VSYNC 给关了。 SW-VSYNC 的生成与传递mPeriod 计算出来以后，DispSyncThread 就可以依据这个值来模拟 HW-VSYNC 了（实际上计算流程和模拟流程是相互独立的，分别在两个不同的线程上完成），所以流程都在 DispSyncThread 的 threadLoop() 里面： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465virtual bool threadLoop() &#123; status_t err; nsecs_t now = systemTime(SYSTEM_TIME_MONOTONIC); while (true) &#123; Vector&lt;CallbackInvocation&gt; callbackInvocations; nsecs_t targetTime = 0; &#123; // Scope for lock Mutex::Autolock lock(mMutex); if (mStop) &#123; return false; &#125; if (mPeriod == 0) &#123; err = mCond.wait(mMutex); if (err != NO_ERROR) &#123; ALOGE(&quot;error waiting for new events: %s (%d)&quot;, strerror(-err), err); return false; &#125; continue; &#125; // 这里计算出下一个确切的 SW-VSYNC 的时间戳 targetTime = computeNextEventTimeLocked(now); bool isWakeup = false; if (now &lt; targetTime) &#123; if (targetTime == INT64_MAX) &#123; err = mCond.wait(mMutex); &#125; else &#123; // 睡到下一个 SW-VSYNC 为止 err = mCond.waitRelative(mMutex, targetTime - now); &#125; if (err == TIMED_OUT) &#123; isWakeup = true; &#125; else if (err != NO_ERROR) &#123; return false; &#125; &#125; now = systemTime(SYSTEM_TIME_MONOTONIC); // Don&apos;t correct by more than 1.5 ms static const nsecs_t kMaxWakeupLatency = us2ns(1500); if (isWakeup) &#123; mWakeupLatency = ((mWakeupLatency * 63) + (now - targetTime)) / 64; mWakeupLatency = min(mWakeupLatency, kMaxWakeupLatency); &#125; callbackInvocations = gatherCallbackInvocationsLocked(now); &#125; if (callbackInvocations.size() &gt; 0) &#123; fireCallbackInvocations(callbackInvocations); &#125; &#125; return false;&#125; 这里通过依据 mPeriod 算出下一个 SW-VSYNC 的时间戳，计算 SW-VSYNC 的时间戳的逻辑比较简单，就不过多描述。然后通过条件变量直接睡到下一个 SW-VSYNC，然后一个个地通过调用 DispSyncSource 的 onDispSyncEvent 回调来进行 SW-VSYNC 的通知。然后 DispSyncSource 的 onDispSyncEvent 又会调用 EventThread 的 onVSyncEvent： 12345678void EventThread::onVSyncEvent(nsecs_t timestamp) &#123; std::lock_guard&lt;std::mutex&gt; lock(mMutex); mVSyncEvent[0].header.type = DisplayEventReceiver::DISPLAY_EVENT_VSYNC; mVSyncEvent[0].header.id = 0; mVSyncEvent[0].header.timestamp = timestamp; mVSyncEvent[0].vsync.count++; mCondition.notify_all();&#125; 这里就可以回答一下在提到的问题，mVSyncEvent 和 mDisplayEventConnections 以及 signalConnections 这三个数组的意义和区别： mVSyncEvent一个长度为 NUM_BUILTIN_DISPLAY_TYPES 的数组，代表这一个 Vsync Event，这个可能是 VSYNC 事件，也有可能是屏幕插拔这种事件等。这个 NUM_BUILTIN_DISPLAY_TYPES 是一个 enum 变量： 1234567enum DisplayType &#123; DISPLAY_ID_INVALID = -1, DISPLAY_PRIMARY = HWC_DISPLAY_PRIMARY, DISPLAY_EXTERNAL = HWC_DISPLAY_EXTERNAL, DISPLAY_VIRTUAL = HWC_DISPLAY_VIRTUAL, NUM_BUILTIN_DISPLAY_TYPES = HWC_NUM_PHYSICAL_DISPLAY_TYPES,&#125;; 从这里就可以看到，至少在这个版本的 Android 除了一个 virtual display（这是 SurfaceFlinger 提供的一个非常有用的功能，很多常见的需求例如录屏就是通过 virtual display 来实现的，这里不展开，有需要的话再写一篇文章详细描述）已经是支持多屏幕了，只不过呢，目前的代码里面都是写死只处理主屏，也就是 Display 0 的事件。 mDisplayEventConnections这个就是用来存储前面提到的 EventThread::Connection 的数组，在调用 EventThread::registerDisplayEventConnection() 的时候，就会把这个 Connection 加到这个数组里面。 signalConnectionsEventThread::waitForEventLocked 最大的作用就是返回这个数组，这个数组存的是所有希望接收下一个 SW-VSYNC 的 Connection，而是否接收 Connection 的标志是 connection-&gt;count 的值：-1 代表不接收 SW-VSYNC；0 代表只接收一次，EventThread 发现 connection-&gt;count 的值为 0 的时候，会把它加到 signalConnections 以便其能够接受到这一次的 SW-VSYNC 之后，会将其 count 置为 -1；大于 0 就表明会一直接收。 onVSyncEvent 的作用是新增一个 VSyncEvent 并且把 EventThread 唤醒，EventThread 统计了所有对 SW-VSYNC 感兴趣的 Connection 并且都加到 signalConnections，最后会通过一个循环调用每个 connection 的 postEvent() 函数，SurfaceFlinger 就会开始走合成的流程，app 就会开始走渲染的流程。至此，SW-VSYNC 完成了传递的全过程。 小结当整个初始化完成以后，整个 DispSync 模型就开始运作起来了。我们先简单地把整个流程描述一下： SurfaceFlinger 通过实现了 HWC2::ComposerCallback 接口，当 HW-VSYNC 到来的时候，SurfaceFlinger 将会收到回调并且发给 DispSync。DispSync 将会把这些 HW-VSYNC 的时间戳记录下来，当累计了足够的 HW-VSYNC 以后（目前是大于等于 6 个），就开始计算 SW-VSYNC 的偏移 mPeriod。计算出来的 mPeriod 将会用于 DispSyncThread 用来模拟 HW-VSYNC 的周期性起来并且通知对 VSYNC 感兴趣的 Listener，这些 Listener 包括 SurfaceFlinger 和所有需要渲染画面的 app。这些 Listener 通过 EventThread 以 Connection 的抽象形式注册到 EventThread。DispSyncThread 与 EventThread 通过 DispSyncSource 作为中间人进行连接。EventThread 在收到 SW-VSYNC 以后将会把通知所有感兴趣的 Connection，然后 SurfaceFlinger 开始合成，app 开始画帧。在收到足够多的 HW-VSYNC 并且在误差允许的范围内，将会关闭通过 EventControlThread 关闭 HW-VSYNC。 然后这个流程我们可以得到下面这张跟初始化非常接近，只是方向相反的 SW-VSYNC 的传递图： 为什么要引入偏移写了这么多内容，可能很多人还是无法理解引入软件模型的意义所在，前面我们提到是让整个流程更加灵活这句话可能也不是很好理解，因此在这里详细描述一下。 首先呢，先来看一下 DispSync 的第一个提交的 commit message，它详细地描述了引入了 DispSync 的初衷： 12345678910111213commit faf77cce9d9ec0238d6999b3bd0d40c71ff403c5Author: Jamie Gennis &lt;jgennis@google.com&gt;Date: Tue Jul 30 15:10:32 2013 -0700 SurfaceFlinger: SW-based vsync events This change adds the DispSync class, which models the hardware vsync event times to allow vsync event callbacks to be done at an arbitrary phase offset from the hardware vsync. This can be used to reduce the minimum latency from Choreographer wake-up to on-screen image presentation. Bug: 10624956 Change-Id: I8c7a54ceacaa4d709726ed97b0dcae4093a7bdcf 意思就是希望能够通过 DispSync 来减少 app 渲染的内容到屏幕的事件延迟，也就是传说中的跟手性。这里需要说明一下从 app 渲染画面到显示到屏幕的一个简易 pipeline（这部分内容参考了这篇博客，建议细读，写得十分好！）。 首先需要说明的是，为了严格保证显示的流畅，防止画面撕裂的情况发生，画面更新到屏幕面板需要在 HW-VSYNC 开始的时候才做。 没有 DispSync 的时候： 第 1 个 HW-VSYNC 到来時, App 正在画 N, SF 与 Display 都沒 buffer 可用 第 2 个 HW-VSYNC 到来時, App 正在画 N+1, SF 组合 N, Display 沒 Buffer 可显示 第 3 个 HW-VSYNC 到来時, App 正在画 N+2, SF 组合 N+1, Display 显示 N 第 4 个 HW-VSYNC 到来時, App 正在画 N, SF 组合 N+2, Display 显示 N+1 从上面这个简易的 pipeline 可以看到，App 画的帧得得两个 HW-VSYNC 之后才能显示到屏幕面板上，也就是大概 33.3ms。但是，现在大部分的情况是，硬件的性能已经足够快了，画一帧的时间和合成的时间不需要一个 HW-VSYNC 了，这个时候 DispSync 的作用就来了。通过引入 offset，当 offset 为正值时，App 和 SurfaceFlinger 都是在 HW-VSYNC 往后 offset ms 才开始工作的，这个时候 App 画帧到最终显示到面板上的延迟就变成了 (2 * VSYNC_PERIOD - (offset % VSYNC_PERIOD))，这样就变相地减少了这个延迟，增强了跟手性，其实这个就是当初引入 DispSync 的初衷。 反过来可以这么想，假设把 offset 变为负值，这个时候 App 渲染和 SurfaceFlinger 合成可用的时间就变长了，在某些负载比较重的场景，这个可以优化渲染性能。 甚至还有这种情况，假设在某些场景，App 渲染和 SurfaceFlinger 合成的总时间都足够短，那么如果设置合理的话，例如 app 的 offset 设置为 0，SurfaceFlinger 的 offset 设置为 VSYNC_PERIOD/2，那么就能够保证 App 渲染到显示到面板的时间差在一个 HW-VSYNC 内完成。 从上面的分析就可以看到，这个就是引入软件模型的灵活性的体现，根据不同的需求对 offset 进行不同的取值，可以得到不同的效果。 有什么用？学了 DispSync 有什么用呢？其实不是说学了 DispSync 有用，而是透过 DispSync 我们学到了 VSYNC 分发的整个流程，这个能够去解释很多问题。这里举一个例子。前段时间一加 7 Pro 推出了首款 90 Hz 屏幕的手机，很多评测机构都纷纷表示，微博滑动等界面感觉更加流畅了，这背后的原理是什么呢？这个时候就可以使用前面学到的知识来分析一波了。这里的 90 Hz 指的就是 HW-VSYNC。然后根据前面的渲染 pipeline，在没有 DispSync 的情况下，由于 HW-VSYNC 的从普通的 60 Hz变成了 90 Hz，VSYNC 的时间间隔从 16.6ms 减少到了 11.1ms，从前面的 pipeline 可以得出，app 从渲染到显示的延迟减少了 10ms 左右，这个延迟减少是十分明显的，因此会有一个“流畅”的感觉。因此能否这么想的，当屏幕的刷新率变成了 90 Hz 甚至是 120 Hz 以后，DispSync 的作用可能就越来越小了，那个时候谷歌会不会把它去掉呢？这个可以看一下后面 Android 的改动，至少在目前，在这个 90 Hz 即将普及的今天，Android Q 的 DispSync 还是保留着的。]]></content>
      <tags>
        <tag>Android</tag>
        <tag>Graphic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kernel Magic——ARRAY_SIZE]]></title>
    <url>%2F2019%2F03%2F22%2FKernel-Magic-ARRAY-SIZE%2F</url>
    <content type="text"><![CDATA[如果让你在 C 语言计算一个数组的长度，那么通常的写法是：1#define ARRAY_SIZE(a) (sizeof(a)/sizeof(a[0]))这个确实是行之有效的，但是问题是什么呢？问题是ARRAY_SIZE 的参数必须是一个数组，不能是一个指针，否则就会出现问题。因为 sizeof(一个指向数组的指针) 它的值是固定的（64 位是 8,32 位是 4）。那么怎么避免这个问题的，Linux Kernel 给了一个很好的解决方法，来看 Kernel 版本的 ARRAY_SIZE: 1#define ARRAY_SIZE(arr) (sizeof(arr)/sizeof(arr[0]) + __must_be_array(arr)) 后面多了一个对于传入参数的必须是数组的保证，来看看是如何实现的： 1234/* &amp;a[0] degrades to a pointer: a different type from an array */#define __must_be_array(a) BUILD_BUG_ON_ZERO(__same_type((a), &amp;(a)[0]))# define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b)) 首先这里用到了一个 Linux Kernel 的一个宏——BUILD_BUG_ON_ZERO。需要说明的是，这个宏的命名跟他的实际意义是相反的，这个宏的意义是当传入的参数不是 0 的时候抛一个编译错误，否则返回 0，所以曾经有人建议把这个宏改名为 BUILD_BUG_OR_ZERO。这个命名就是比较贴切的，但是最终并没有被接纳，所以一直在误导新人。 先来看一个这个宏的定义： 12345/* Force a compilation error if condition is true, but also produce a result (of value 0 and type size_t), so the expression can be used e.g. in a structure initializer (or where-ever else comma expressions aren&apos;t permitted). */#define BUILD_BUG_ON_ZERO(e) (sizeof(struct &#123; int:-!!(e); &#125;)) 这里面最重要的知识点就是：逻辑取反和位域（bitfield），其中位域分为两种，命名位域和匿名位域。其中匿名位域长度可以为 0，命名位域必须大于 0。 两次逻辑取反会把非零值变为 1，所以如果 e 为非零值，那么就会是一个长度为 -1 的位域；否则就是一个长度为 0 的匿名位域，对其去 sizeof 就是 0。 这里还用到了 gcc 的一个 built-in function —— __builtin_type_compatible_p，它的作用是检查两个 type 是否一致（注意是 type，而不是表达式，所以这里用到了 typeof），一致返回 1，否则返回 0 。并且它检查的是去掉限定词之后的类型，例如 const volatile 等都会被忽略。同时对于它而言，int[] 和 int[5] 类型是一致的；以及两个 enum 将会被认为不是同一个类型。 然后就可以来看它是怎么实现检查参数是数组的。这里用到了 &amp;a[0]（说明一下 [] 的优先级比 &amp; 要高）。 如果 typeof(a) 是一个数组，此时 typeof(&amp;a[0]) 它就退化成一个指针（因为 a[0] 是一个值，对一个数值取地址就是一个指针了），所以 a 和 &amp;a[0] 不是同一个同类型，__same_type 返回 0，BUILD_BUG_ON_ZERO 返回 0 如果 typeof(a) 是一个指针，此时 typeof(&amp;a[0]) 也是一个指针，所以 a 和 &amp;a[0] 是同一个类型，__same_type 返回 1，BUILD_BUG_ON_ZERO 会抛出一个编译错误，这样就能够在编译的阶段就把问题给识别出来，非常高明。 这个是在修一个项目 bug 的时候，当时我用的是第一种写法，然后组内大佬 review 的时候说改成 ARRAY_SIZE，这个是 Kernel 的标准接口，然后去学习了一下深深地被它的优雅给震惊到，所以值得记录一番。]]></content>
      <categories>
        <category>Kernel Magic</category>
      </categories>
      <tags>
        <tag>Linux Kernel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码之外——Andriod 签名]]></title>
    <url>%2F2018%2F08%2F11%2Fall-about-signature%2F</url>
    <content type="text"><![CDATA[作为一名 Android 系统工程师，替换系统应用是常事。但是刚入门的时候总会遇到这样的情况：把自己修改过的 App push 到手机以后发现无法运行，一问老鸟他会跟你说：你是不是下载了带签名的 ROM 了？去下个不带签名的就可以。可行自然是可行，但是终究不知其所以然，索性系统化学习了一番，总结成文，造福后人。 非对称加密在了解 Android 的签名机制之前，需要了解一些基础知识。非对称加密是整个 Android 签名机制的基石，又称为公开密钥加密。它需要两个密钥，一个是所有人的都可见的，称为公钥；另一个是仅能自己可见的，称为私钥。这两个密钥的关系在维基百科中的描述是： 一个用于加密的时候，另一个则用于解密。使用其中一个密钥把明文加密后所得的密文，只能用相对应的另一个密钥才能解密得到原本的明文；甚至连最初用来加密的密钥也不能用作解密。由于加密和解密需要两个不同的密钥，故被称为非对称加密。 至于公钥和私钥是如何产生，以及一个密钥加密的密文只能通过另一个密钥来解密背后的数学原理，可以参考阮一峰的科普文章：这个还有这个，这里就不详细说明。目前我们需要记住的第一个原则是： 原则一：使用其中一个密钥把明文加密后所得的密文，只能用相对应的另一个密钥才能解密得到原本的明文，也就是说，用私钥加密之后的密文私钥自己都解不开。 签名首先我们得理解签名的作用是什么？在很多场景里面，例如在银行办理业务，都需要我们本人的签名，作用是声明这个是我本人的行为。在加密学里面，称为数字签名，也是同样的道理，对一个东西进行签名，意思就是证明当你见到的这个东西跟我想要给你看的是一个东西。这样可能说可能有点虚，用一个例子来说明： 我有两把钥匙，公钥和私钥。私钥我自己留着，公钥可以给任何人，例如我把公钥给了周杰伦 周杰伦想要跟我进行过加密通信，他把要跟我说的话写完之后，使用公钥进行加密 我收到加密后的内容，根据原则一，我可以用我的私钥去解密内容，得到原来的内容 这个时候我想要回信，为了保证周杰伦看到的内容没有被修改，我决定使用数字签名。这里得说明一下，数字签名是对非对称加密的反应用，在非对称加密的日常应用中，公钥是用来加密，私钥是用来解密的。而在数字签名中，是反过来的，具体流程是这样的： 我写完信之后，使用一个 Hash 函数生成信件的摘要（注意，信件其实可以理解成一个字符串） 然后我使用我的私钥对摘要进行加密，并且附在信件的后面 当周杰伦收到我的信件之后，为了验证内容没有被篡改，先把附在信件后面的加密后的摘要取下，使用自己的公钥进行解密，得到了信件的摘要 a。然后使用相同的 Hash 函数对信件进行哈希得到 b，如果 a 和 b 完全一致，那么说明信件没有被篡改，反之则反。 但是，上面的流程有一个问题就是。周杰伦手中的公钥其实不能够确定是不是我给他的。也就是说，坏人出现了。如果有坏人替换了周杰伦手中的公钥成坏人自己的公钥，那么周杰伦发出的公钥加密之后的信件坏人是可以直接使用自己私钥进行解密的。也就是说，上面的流程的关键是确认周杰伦手中的公钥是我的公钥。TODO 上面的例子参考了阮一峰的一篇博客。维基百科有一张图很好地解释了数字签名里签名和验签的过程： 因此我们得到了第二个原则： 原则二：签名的作用是为了验证文件的完整性，即是否被篡改 有了上面的基础知识，就可以系统地阐述 Android 签名机制了。 Android 签名机制在 Android 源码库 build/target/product/security 下面，有这么一些文件，有的是以 .pk8 为后缀，有的是以 .x509.pem 为后缀，并且会发现 .pk8 和 .x509.pem 是一一对应的。这两种文件的关系是：.pk8 文件是私钥，用来对包进行签名；.x509.pem 文件是证书，用来验证签名。原生 Android 使用了 4 类密钥： testkey platform shared media 系统自带的应用通过在 Android.mk 文件中声明 LOCAL_CERTIFICATE 来指定用那个私钥进行签名，如果不声明那么默认使用 testkey。 这里有个问题，上面说道这些 key 都是在源码中的，所有人都是可以访问的，那么这样其实是非常不安全的。任何人都可以使用这些 key 去对应用进行签名然后就可以通过系统的验证了。所以在实际的情况中，是会通过把原生的这一套 key 给替换掉。谷歌在这里提供了替换的方法。并且，在实际外发的 ROM 包中，是不会有 testkey 这个签名文件的，默认的变成了 releasekey 如果我们对一个 apk 文件进行解包，那么会发现里面有一个 META-INF 的文件夹，里面的内容根据不同的应用会有不同，但是一定会有这三个文件：MANIFEST.MF，CERT.SF 和 CERT.RSA。这三个文件就是 Android 签名机制的核心了。系统如何判断一个 apk 是不是被修改过的，就是通过这三个文件进行一系列的校验。具体这三个文件是怎么生成的，可以参考这一篇文章(下文也有很多内容是参考了这篇文章的描述，再次感谢作者)。现在我们以一个 apk 为例，简要说明这三个文件的内容和意义（注意，有一些 APK 可能 META-INF 下面的内容可能后缀名一样，但是文件名一样；又或者是不止这三个文件，这些都是正常现象）。 MANIFEST.MF这个文件的内容是当前 apk 里面所有文件的名字和文件的摘要值，例如在现在这个例子中，这个文件的内容大概长这样： 12345678910111213Manifest-Version: 1.0Built-By: Generated-by-ADTCreated-By: Android Gradle 3.0.1Name: AndroidManifest.xmlSHA1-Digest: l5LrO+0CH4QwymZEEkgof6tKJKQ=Name: META-INF/INDEX.LISTSHA1-Digest: mV/vtpP5kHRZ0ZdWNzAWUorzn/M=Name: META-INF/io.netty.versions.propertiesSHA1-Digest: fHUsZp7XXjDcmXh7h88Qxku7PaQ=... 这里我们可以来实战一下，验证这里面内容的意义。以第一个 AndroidManifest.xml 为例，首先把这个文件提取出来，然后计算一下它的 SHA-1 值，在 Linux 可以这样： 12▶ sha1sum AndroidManifest.xml 9792eb3bed021f8430ca66441248287fab4a24a4 AndroidManifest.xml 咦，值好像不一样，没事，因为上面的值是经过 Base64 编码的，我们可以在这里进行转换：对比后就可以发现跟 MANIFEST.MF 的值是一毛一样的。 CERT.SF我们初看这个文件的时候，会发现它的内容跟 MANITEST.MF 非常的接近： 12345678910111213Signature-Version: 1.0Created-By: 1.0 (Android)SHA1-Digest-Manifest: nezsP8TgzAKQ7BFky/chze3qmL0=Name: AndroidManifest.xmlSHA1-Digest: mr/1kFRAFlcWQAo9hA69M29MAYs=Name: META-INF/INDEX.LISTSHA1-Digest: YFvH0U9NaeV1BDZUz5JkpfUm9aU=Name: META-INF/io.netty.versions.propertiesSHA1-Digest: rTBpHjFlmjueKQtX0IlpTl7X4uo=... 这里面分为两部分内容：SHA1-Digest-Manifest 和 SHA1-Digest，这两部分分别是这么计算的。首先这个 SHA1-Digest-Manifest 就是对 MANIFEST.MF 计算 SHA-1之后再进行Base64编码： 12▶ sha1sum META-INF/MANIFEST.MF 9decec3fc4e0cc0290ec1164cbf721cdedea98bd META-INF/MANIFEST.MF 然后 SHA1-Digest 是对 MANIFEST.MF 里面的每一个 \r\n 分割开来的项分别进行 SHA-1 之后在进行 Bash64 编码，例如上面的 AndroidManifest.xml 的 SHA1-Digest 是怎么算出来的呢？我们可以这么来： 首先把 MANIFEST.MF 里面这个文件的内容保存一下（注意换行也要保存），例如咱们这里的是这个： Name: AndroidManifest.xmlSHA1-Digest: l5LrO+0CH4QwymZEEkgof6tKJKQ= 然后如果你是在 Linux 下，使用 unix2dos 进行转换一下 然后计算一下 sha1： ▶ sha1sum 19abff5905440165716400a3d840ebd336f4c018b 1 最后计算一下 Base64 编码就可以了 CERT.RSA简单的说，这个文件是用私钥对 CERT.SF 进行签名，并且把公钥也附在这个文件里面。 然后就需要说明，这种签名机制如何能够保证应用不被篡改呢？首先如果你修改了 APK 里面任何一个文件，那么相应的文件的 SHA1 摘要就会发生改变，那么就会跟 MANIFEST.MF 里面的值不一致；如果你不死心，修改 MANIFEST.MF 里面的内容，那么就会跟 CERT.SF 里面对应项的内容不一致；如果你还不死心，继续修改 CERT.SF 的内容，那么在 CERT.RSA 的验签那里不通过；如果依旧不死心，想要修改 CERT.RSA 的内容，能做到吗？不能，因为你没有私钥。从这里我们就可以看到，有了这三个文件的“保驾护航”，就可以达到一个效果就是，无论修改一个 Apk 里面的任何一个文件，都必须对其重新签名，否则会直接被系统识别出来，从而保证了安全性。 尾巴好了，Android 的签名机制也大概地说了一遍，感觉可以回答上面的问题了。每个手机公司的 ROM 肯定有两种，一种是内部版本，用的是咱们上面提到的 build/target/product/security 里面的 test key；另一种是外发版本，用的是另外的 key。所以呢，咱们自己本地编译的 App，在最后的签名阶段用的就是系统的 test key。那么导致的结果是，如果你用的是内部的 ROM，那么每次编译的使用用的都是系统 test key 的私钥进行签名，然后用的是 test key 的公钥进行验签，肯定能够通过。反之，如果你用的是外发的 ROM，外发的 ROM 用的是另一套 key，那么肯定会验签不通过，原因就是在这里啦。 把一件事弄懂的感觉真好。]]></content>
      <categories>
        <category>代码之外</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning By Creating 之使用 ListView 实现一个应用列表]]></title>
    <url>%2F2016%2F05%2F05%2Flearning-by-creating--tasklist-using-listview%2F</url>
    <content type="text"><![CDATA[规划既然我们要实现一个应用列表，那么我们需要什么呢？一个列表，然后列表中的每一表项都有相同的结构和各自的内容。那么这些在 Android 中究竟是如何实现的呢？查询过后发现，我们可以使用 ListView 来实现一个列表，而列表中的表项则可以用 LayoutInflater 来设置列表中的表项。好，似乎进展挺不错的，开干。 ListView我们先来新建一个 ListView 的主布局： 1234567891011121314151617&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;RelativeLayout xmlns:android="http://schemas.android.com/apk/res/android" xmlns:tools="http://schemas.android.com/tools" android:layout_width="match_parent" android:layout_height="match_parent" android:paddingBottom="@dimen/activity_vertical_margin" android:paddingLeft="@dimen/activity_horizontal_margin" android:paddingRight="@dimen/activity_horizontal_margin" android:paddingTop="@dimen/activity_vertical_margin" tools:context="io.github.simowce.tasklist.MainActivity"&gt; &lt;ListView android:id="@+id/lvTaskList" android:layout_width="fill_parent" android:layout_height="fill_parent" &gt; &lt;/ListView&gt;&lt;/RelativeLayout&gt; 很简单，没什么嘛。没错，因为我们只是建立了一个列表的布局而已，具体的表项结构和数据来源都还没有呢。好，我们接下来设置我们列表数据的类： 1234567891011public class TaskItem &#123; public String appName; public String packageName; public Drawable appImg; public TaskItem(String appName, String packageName, Drawable appImg) &#123; this.appName = appName; this.packageName = packageName; this.appImg = appImg; &#125;&#125; 也是很简单，我们想要列出我们手机里面应用的图标，名称和包名（当然你可以有你自己更多的想法 :-)）。接下来，我们来设置我们表项的布局了： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;LinearLayout xmlns:android="http://schemas.android.com/apk/res/android" android:layout_width="match_parent" android:layout_height="50dp"&gt; &lt;ImageView android:id="@+id/appImg" android:layout_width="wrap_content" android:layout_height="wrap_content" android:layout_gravity="center" /&gt; &lt;RelativeLayout android:layout_width="match_parent" android:layout_height="wrap_content"&gt; &lt;TextView android:id="@+id/lable_appName" android:layout_width="wrap_content" android:layout_height="wrap_content" android:text="App Name: " /&gt; &lt;TextView android:id="@+id/appName" android:layout_width="wrap_content" android:layout_height="wrap_content" android:layout_toRightOf="@id/lable_appName" android:layout_marginLeft="3dp" /&gt; &lt;TextView android:id="@+id/lable_packageName" android:layout_width="wrap_content" android:layout_height="wrap_content" android:text="Package Name: " android:layout_below="@id/lable_appName" /&gt; &lt;TextView android:id="@+id/packageName" android:layout_width="wrap_content" android:layout_height="wrap_content" android:layout_below="@id/appName" android:layout_toRightOf="@id/lable_packageName" android:layout_marginLeft="3dp"/&gt; &lt;/RelativeLayout&gt;&lt;/LinearLayout&gt; 也很简单。好，流程到了现在似乎很顺利。 诶诶诶，等一下。有一个问题，一个大问题！想象一下，如果我们有一个巨大无比的表项，例如 10 亿个，那么，我们如果要一次性将这么多个表项载入内存显然是不可能的，所以我们就应该只载入一部分表项，例如说可见的。但是又会有一个问题，如果只载入一部分，那么我们就必须要记录一下当前载入的表项范围。这似乎就变得有点复杂了。所以，Android 派出了一个大将来帮我们解决这个问题，那就是：Adapter。 AdapterAdapter 就是专门来解决上面的问题的。不过，Adapter 的内容还挺多的，我暂时只看了一部分，没来得及总结。将来补上。这里我们主要介绍里面的一种 —— ArrayAdapter。 我们上面说了 Adapter 只是将一部分表项显示在我们的屏幕上，具体是怎么实现的呢？这里就涉及到一个很重要的方法了：getView()。我们先来看看 API 文档是怎么描述它的： 1public abstract View getView (int position, View convertView, ViewGroup parent) Get a View that displays the data at the specified position in the data set. You can either create a View manually or inflate it from an XML layout file. When the View is inflated, the parent View (GridView, ListView…) will apply default layout parameters unless you use inflate(int, android.view.ViewGroup, boolean) to specify a root view and to prevent attachment to the root. Parameters Description position int: The position of the item within the adapter’s data set of the item whose view we want. convertView View: The old view to reuse, if possible. Note: You should check that this view is non-null and of an appropriate type before using. If it is not possible to convert this view to display the correct data, this method can create a new view. Heterogeneous lists can specify their number of view types, so that this View is always of the right type (see getViewTypeCount() and getItemViewType(int)). parent ViewGroup: The parent that this view will eventually be attached to Returns View: A View corresponding to the data at the specified position. 根据里面的描述，getView() 是用来展示由 position 指定位置的 View。我们可以在这里手动建立一个 View 或者通过 XML 来指定布局。似乎不是很清楚。还有传入的这个 position 和 convertView 究竟是什么含义呢？不急，我们等我们的代码运行起来了之后再来做个实验。现在先卖个萌先 ;-&gt; 。 看了上面的描述，知道 getView 是用来显示我们的表项的，于是乎，我们就要用到 LayoutInflater 这个东东了。这个东东有什么用呢？我们知道，我们在 onCreat() 中使用 setContext() 的时候使用的布局只是一个 ListView 而已，并没有具体的页表项。所以，如同名字一样，LayoutInflater 就是用来填充我们的 ListVIew 的。具体的代码是这样的： 12345678910111213141516171819202122232425public class TaskAdapter extends ArrayAdapter&lt;TaskItem&gt; &#123; private final String TAG = "SIMOWCE"; public TaskAdapter(Context context, ArrayList&lt;TaskItem&gt; taskitem) &#123; super(context, 0, taskitem); &#125; @Override public View getView(int position, View convertView, ViewGroup parent) &#123; TaskItem task = getItem(position); if (convertView == null) &#123; convertView = LayoutInflater.from(getContext()).inflate(R.layout.task_item, parent, false); &#125; // 具体的查找和填充操作就是在这里了 TextView appName = (TextView) convertView.findViewById(R.id.appName); TextView packageName = (TextView) convertView.findViewById(R.id.packageName); ImageView appImg = (ImageView) convertView.findViewById(R.id.appImg); appName.setText(task.appName); packageName.setText(task.packageName); appImg.setImageDrawable(task.appImg); return convertView; &#125;&#125; 好，我们再来看一下我们的数据来源。我们是要输出我们系统的应用，Android 已经贴心的给我们提供了一个类供我们查询了 —— PackageManager，具体我们在 onCreate() 方法中的代码是这样的： 1234567891011121314151617181920212223public class MainActivity extends AppCompatActivity &#123; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); ListView listView = (ListView) findViewById(R.id.lvTaskList); PackageManager pm = this.getPackageManager(); Intent mainInent = new Intent(Intent.ACTION_MAIN, null); mainInent.addCategory(Intent.CATEGORY_LAUNCHER); List&lt;ResolveInfo&gt; appInfo = pm.queryIntentActivities(mainInent, PackageManager.MATCH_DEFAULT_ONLY); ArrayList&lt;TaskItem&gt; all_task = new ArrayList&lt;&gt;(); for (ResolveInfo t : appInfo) &#123; String appName = (String) t.loadLabel(pm); String packageName = t.activityInfo.packageName; Drawable appImg = t.loadIcon(pm); all_task.add(new TaskItem(appName, packageName, appImg)); &#125; TaskAdapter adapter = new TaskAdapter(this, all_task); listView.setAdapter(adapter); &#125;&#125; 最后的一句就是将我们的 ListView 和 Adapter 给连接起来。这样，我们的代码就可以运行了。 好，我们可以来看看那个 getView() 的前两个参数的含义了，我们修改以下代码： 1234567891011public View getView(int position, View convertView, ViewGroup parent) &#123; TaskItem task = getItem(position); Log.d(TAG, "++++++++++++++++++++" + "I am " + position + " +++++++++++++++++++++"); if (convertView == null) &#123; Log.d(TAG, "-----------------------HEHE---------------------------"); convertView = LayoutInflater.from(getContext()).inflate(R.layout.task_item, parent, false); &#125; else &#123; Log.d(TAG, "************************YEYEYE*************************************"); &#125; ...... 然后我们在 Android Studio 上看看输出是什么，下面是我的输出： 123456789101112131415161718192021222324252627282930++++++++++++++++++++I am 1 +++++++++++++++++++++-----------------------HEHE---------------------------++++++++++++++++++++I am 2 +++++++++++++++++++++-----------------------HEHE---------------------------++++++++++++++++++++I am 3 +++++++++++++++++++++-----------------------HEHE---------------------------++++++++++++++++++++I am 4 +++++++++++++++++++++-----------------------HEHE---------------------------++++++++++++++++++++I am 5 +++++++++++++++++++++-----------------------HEHE---------------------------++++++++++++++++++++I am 6 +++++++++++++++++++++-----------------------HEHE---------------------------++++++++++++++++++++I am 7 +++++++++++++++++++++-----------------------HEHE---------------------------++++++++++++++++++++I am 8 +++++++++++++++++++++-----------------------HEHE---------------------------++++++++++++++++++++I am 9 +++++++++++++++++++++-----------------------HEHE---------------------------++++++++++++++++++++I am 10 +++++++++++++++++++++-----------------------HEHE---------------------------++++++++++++++++++++I am 11 +++++++++++++++++++++-----------------------HEHE---------------------------++++++++++++++++++++I am 11 +++++++++++++++++++++************************YEYEYE*************************************++++++++++++++++++++I am 12 +++++++++++++++++++++************************YEYEYE*************************************++++++++++++++++++++I am 13 +++++++++++++++++++++************************YEYEYE*************************************++++++++++++++++++++I am 14 +++++++++++++++++++++************************YEYEYE************************************* 首先声明一下，在我的手机里面 ListView 能够显示 11 行。 看了上面的 log，我想你应该明白了。getView() 是用来显示页表项的，position 是当前要显示的页表项的下标（或者我们可以理解为数组下标，因为我们使用的是 ArrayAdapter），每次要显示一个页表项，就会调用一次 getView ，同时，为了防止老是分配页表项，convertView 是系统为我们缓存的页表项，这样当我们往下滑的话，就无需在重新分配页表项了，只需要重复利用前面已经分配但是已经不再屏幕上显示的页表项了。我们要做的，只是将内容重新赋值即可。这样就可以大大地提高性能。]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[指哪打哪地进行编译工作]]></title>
    <url>%2F2016%2F05%2F04%2Fone-shot-to-compile%2F</url>
    <content type="text"><![CDATA[起因最近在看 APUE，里面有很多小的例子可以用来练手。不过这就有一个问题了：APUE 有一个公用的头文件和自己封装的错误处理例程，每次编译要指定相应的目录和文件确实是很麻烦。make 确实可以解决这个问题，但是如何指定要编译的文件就是一个问题。所以，我们现在的问题是：如何让 makefile 可以指定要编译的文件。经过一方摸索之后，总算是解决了这个问题。 Solution我的解决方法很简单，就是通过引入 shell 的环境变量。我定义了一个环境变量 CUR ，用来指定当前要编译的文件名。然后通过 makefile 里面的 shell 函数，通过 printenv CUR 在 makefile 获取当前要编译的文件名。 同时，为了方便，我想要复用这个 makefile，所以我先将这个 makefile 放在一个固定的位置，然后在我的 shell 配置文件（我用的是 zsh，所以配置文件是 .zshrc，如果你用的是 bash，那么就是 .bashrc，其他类推）使用一个函数，每次调用都使用这个 makefile 模板来编译（可以使用 make 的 -f 参数来指定 makefile）。 好了，说了这么多，我把我的 makefile 模板和 shell 相应的函数放上来跟大家一起共享，欢迎讨论。 123456789101112131415LIB_PATH = ~/Code/libsSRC_PATH = ./INCLUDE_PATH = $(LIB_PATH)LIB = $(wildcard $(LIB_PATH)/*.c)INCLUDE = $(wildcard $(LIB_PATH)/*.h)CUR = $(shell printenv CUR)DEBUG = -gall: $(LIB) $(INCLUDE) $(CUR) gcc -I $(INCLUDE_PATH) $(LIB) $(CUR) -o $(subst .c,.o,$(CUR))debug: $(LIB) $(INCLUDE) $(CUR) gcc -I $(INCLUDE_PATH) $(LIB) $(CUR) $(DEBUG) -o $(subst .c,.debug,$(CUR)) 123456789go() &#123; export CUR="$1" make -f ~/Templates/Makefile&#125;de() &#123; export CUR="$1" make -f ~/Templates/Makefile debug&#125; 这样，我们只需要在要编译的时候，使用 go 你要编译的文件名 就可以了直接编译了，使用 de 你要编译的文件名 就可以编译可以用 gdb 调试的文件了。有种指哪打哪的快感！]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知其然不知其所以然之 Hello Android]]></title>
    <url>%2F2016%2F04%2F29%2Fzero-to-one-android-first%2F</url>
    <content type="text"><![CDATA[问题起源我们（特指初学者）在最开始学习 Android 变成的时候可能是要来一个类似 Hello，Android 之类的小应用来玩一玩的。不过由于现在的 IDE 如 Android Studio 太过于强大，它帮你处理掉了很多初始化的操作，大大的提升便捷性。不过这也给我们这一些初学者留了一个坑，那么是程序运行了，不过我根本就不知道为什么，呈现一脸懵逼状。因为不想得过且过，于是便有了这篇博客。 Step By Step既然是要知其所以然，那么我们就要避免 IDE （在下文中特指 Android Studio，简称 AS）帮我们做太多事情。在新建项目的时候我们要选择 Add No Activity 。项目打开后，我们会发现项目的文件组织简洁了许多。诶，这就是我们要的效果。 新建 Activity首先，我们得先给我们的项目加一个 Activity，AS 在新建项目的时候已经帮我们把包（Package）给建立好了，所以只需要在包下面新建一个类，继承 Activity。同时，为了保证重新开始的纯粹性，我们在新建的时候，我们不选择 New -&gt; Activity ，而是选择 New -&gt; Java Class，这样就可以避免 AS 修改我们的 AndroidManifest.xml 文件。 接下来，我们让我们的类继承 Activity，并且重写 onCreate 方法（之所以要这样是因为 Activity 的生命周期决定的，这个我们在后续的博客中会说明）： 123456public class FirstActivity extends Activity &#123; @Override protected void onCreate(Bundle saveInstanceStace) &#123; super.onCreate(saveInstanceStace); &#125;&#125; 好，这些代码里面的 Bundle 啊什么的我暂时是不知道是什么东西的，不过初学的时候就不要拘泥于太多的细节问题，但是也不要得过且过，最主要的是要摸清整个的思路。这个是我一直以来的奉行的学习方法。 我们已经有一个 Activity 了，不过这个 Activity 究竟长什么样呢？诶，我们可以进入下一步了。 给 Activity 打扮 —— XMLAndroid 推荐的做法是界面代码与逻辑代码分离，所以我们必须要在别的地方来定义我们的 App 的界面。在 AS 里面，具体的位置是 res/layout，不过现在这个文件夹还不存在，所以我们要新建它。同样的原因，我们不选择 Android resource directory，而是直接选择 Directory，新建一个名为 layout 的文件夹（注意注意：如果你把 AS 的项目的文件展现方法设置成 Android 的话，那么你的新建的文件夹如果不叫 layout，那么是在 res/ 下面是显示不出来的，因为它不符合 Android 的标准。）。接下来，我们还要来新建一个 XML 文件。然后输入一下的代码： 12345678910&lt;LinearLayout xmlns:android="http://schemas.android.com/apk/res/android" android:layout_width="match_parent" android:layout_height="match_parent" android:orientation="vertical"&gt; &lt;TextView android:layout_height="match_parent" android:layout_width="match_parent" android:text="@string/hello" /&gt;&lt;/LinearLayout&gt; 没什么好说的，主要是里面的 TextView 的 text 属性有一个 @string/hello 的东西，这个就涉及到 Android 的第二个原则了，尽量避免硬编码。其实这个跟我们 C 语言的避免 Magic number 是一个道理的。推荐的做法是将我们的值存储在类似 res/values/strings.xml 里面，然后通过 Key-Value 的做法来将值导入进来。所以这里就显现了 XML 文件在 Android 的两个作用了（至少我是这么认为的）： 首先，最重要的，当然就是定义界面布局 其次，就是存储数据，这也是 W3C 里面对 XML 的一个作用说明 打通逻辑代码与界面好了，我们有了逻辑代码和界面布局了，如何将他们连接起来呢？诶，就是要在逻辑代码中加载我们的界面布局，挺简单的，只需要在 onCreate 方法中加入一句代码： 1234567public class FirstActivity extends Activity &#123; @Override protected void onCreate(Bundle saveInstanceStace) &#123; super.onCreate(saveInstanceStace); setContentView(R.layout.first_layout); &#125;&#125; 这样就打通了这两者的关系了。 好耶好耶，打通了，我们运行一下。咦，怎么报错啊： Could not identify launch activity: Default Activity not foundError while Launching activity 我们还差临门一脚，注册。 注册其实注册就是一个对外展示自己的一个流程，最重要的文件就是 AndroidManifest.xml 文件（所以我们在上面才尽量地手动操作，防止 AS 修改这个文件 :-)）。具体这个文件的描述我就不细说了，主要是为了让我们的 Activity 变成主 Activity，也就是上面报错的原因，系统需要一个默认的 Activity，在我们点击 App 图标时候展现在屏幕上。具体是在注册 Activity 的时候加入 intent-filter，而且必须是下面这两句： 1234&lt;intent-filter&gt; &lt;action android:name="android.intent.action.MAIN" /&gt; &lt;category android:name="android.intent.category.LAUNCHER" /&gt;&lt;/intent-filter&gt; 具体为什么我还不是很清楚，不过当我们点击 Run 的时候，确实能够运行了！ 好了，我们就这样走完了整个流程了。清晰了许多吧～ P.S. 我发现在上面的代码运行了之后是没有标题栏的，但是如果把我们的类的基类改成 AppCompatActivity 的话，标题栏就奇迹般地出现了，暂时不知道为什么。 知其然知其所以然，我是 simowce。]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android USB 框架 —— UsbHostManager]]></title>
    <url>%2F2016%2F03%2F20%2Fandroid-usb-framework--usbhostmanager%2F</url>
    <content type="text"><![CDATA[我们知道 USB 是主从模式（Host/Slave）的，有趣的是，Android 手机既可以被当做从设备连接到我们的 PC 被识别成为一个 USB 设备，也可以当做主设备识别外部的 USB 设备。这篇东东主要着眼于后者。我之前一直有一个疑惑，那就是 Android 上层与 Linux 内核底层究竟是如何通讯的。之前我知道了可以通过 sysfs ，或者是通过 uevent 来打通内核与上层。现在，我学习了 Android USB Host 之后，发现了还有一种新的方法 —— Android JNI 。 首先声明，我对 Android 这一块知道的确实不多，所以下面的描述肯定是会有这样那样的错误，还待日后更加深入学习。 Android JNI关于 Java JNI 的我写了另外的一篇东东，就不再赘述了。下面着重说明 Android JNI 与 Java JNI 的区别。 Android 中使用了一种不同于传统 Java JNI 的方法来定义其 native 函数。其中很重要的区别就是 Android 使用了一种 Java 和 C 函数的映射表数组，并在其中表述了函数的参数和返回值。这个数组的类型就是 JNINativeMethod ，定义如下： typedef struct { const char *name; const char *signature; void *fnPtr; } JNINativeMethod;其中，第一个变量是 Java 中的函数名，第二个变量是一个描述返回值和参数的字符串，第三个是指向 native 实现的函数指针。 比较难以理解的第二个参数，具体可以参照这篇博客。 发现 USB 设备流程首先，我先使用自顶向下来说明一下整个流程： 在 UsbHostManager 中创建一个线程来监控（注意，这里用“监控”可是很有讲究的，因为当 Android 设备处于 Host 模式时，当有 USB 设备插拔时，首先相应的是 Linux 内核，然后内核再将设备变动的信息传递给上层，而 Android 根据这些信息在进行相应的动作，所以这里使用了“监控”二字。这跟 Android 设备处于 Slave 模式下的 UsbDeviceManager 是完全不同的）设备的连接状态。该线程执行的是 JNI 层的函数，而 JNI 层则是通过调用 libusb 库的函数，利用内核提供的 inotify 机制来监控 /dev/bus/usb 下文件的变动来判断是否有新的设备的插拔。下面的图示很好地展现了这种自顶向下的路线： （Source：Unboxing Android USB） 接下来，我再以自底向上的方法来详细阐述整个流程： 首先，USB 插入设备之后，内核会在 /dev/bus/usb 这个目录下建立代表该文件的设备文件入口（device file entry）（当然，个中细节还需要花时间深入理解，这里先假设其成立），然后 Android 上层使用 libusb 这个库来监控这个目录下的文件变动，其中用到的技术就是 inotify 。关于 inotify 我也写了一篇东东来简要地说明它，可以先去看看。下面将通过分析 libusb 的源代码（具体位置在 system/core/libusb/usbhost.c）来详细说明： 首先，先初始化 inotify ，获得文件描述符，此后的所有事件都是通过读取该文件描述符中的数据来判断： 12345678910111213141516struct usb_host_context *usb_host_init()&#123; struct usb_host_context *context = calloc(1, sizeof(struct usb_host_context)); if (!context) &#123; fprintf(stderr, &quot;out of memory in usb_host_context\n&quot;); return NULL; &#125; // 初始化，获取文件描述符 context-&gt;fd = inotify_init(); if (context-&gt;fd &lt; 0) &#123; fprintf(stderr, &quot;inotify_init failed\n&quot;); free(context); return NULL; &#125; return context;&#125; 这里要说明一下 struct usb_host_context 这个数据结构： 123456789struct usb_host_context &#123; int fd; // inotify 返回的文件描述符，通过读取其中的数据来判断事件 usb_device_added_cb cb_added; // 当有 USB 设备插入是调用的回调函数，在 JNI 层赋值，后面会提到 usb_device_removed_cb cb_removed; // 同上，当有 USB 设备拔出是的回调函数 void *data; // 调用者给上面两个回调函数的参数 int wds[MAX_USBFS_WD_COUNT]; // /dev/bus/usb 下各个子目录对应的 watch descriptor int wdd; // /dev 对应的 watch descriptor int wddbus; // /dev/bus 对应的 watch descriptor&#125;; 然后，就是要开始添加要监控的目录了（即我们这里的 /dev/bus/usb）: 123456789101112131415void usb_host_run(struct usb_host_context *context, usb_device_added_cb added_cb, usb_device_removed_cb removed_cb, usb_discovery_done_cb discovery_done_cb, void *client_data)&#123; int done; done = usb_host_load(context, added_cb, removed_cb, discovery_done_cb, client_data); while (!done) &#123; done = usb_host_read_event(context); &#125;&#125; /* usb_host_run() */ 主要的工作是在 usb_host_load() 中完成的： 12345678910111213141516171819202122232425262728293031int usb_host_load(struct usb_host_context *context, usb_device_added_cb added_cb, usb_device_removed_cb removed_cb, usb_discovery_done_cb discovery_done_cb, void *client_data)&#123; int done = 0; int i; context-&gt;cb_added = added_cb; context-&gt;cb_removed = removed_cb; context-&gt;data = client_data; /* watch for files added and deleted within USB_FS_DIR */ context-&gt;wddbus = -1; for (i = 0; i &lt; MAX_USBFS_WD_COUNT; i++) context-&gt;wds[i] = -1; /* watch the root for new subdirectories */ // 如上面说说，wdd 是用来发现 /dev 下是否有子目录创建或者删除的 context-&gt;wdd = inotify_add_watch(context-&gt;fd, DEV_DIR, IN_CREATE | IN_DELETE); watch_existing_subdirs(context, context-&gt;wds, MAX_USBFS_WD_COUNT); /* check for existing devices first, after we have inotify set up */ done = find_existing_devices(added_cb, client_data); if (discovery_done_cb) done |= discovery_done_cb(client_data); return done;&#125; /* usb_host_load() */ 其中的 watch_existing_subdirs() 就是添加 /dev/bus/usb 下的文件变动的监控的。函数不难理解，主要说明一下，就是 context-&gt;wds 这个数组：wds[0] 代表的是整个 /dev/bus/usb 下子目录的变动，而 wds[] 数组中其他的代表了真正设备的变动，因为内核会在每个新设备插拔之后在 /dev/bus/bus 这个目录下增加设备对应的文件如：/dev/bus/usb/001/001 等。 在完成了文件监控之后，接下来就可以来读数据了，调用了上面的 usb_host_read_event() ，这个函数重复的地方很多，下面我只选取了关键的 USB 设备的部分来说明： 12345678910for (i = 1; (i &lt; MAX_USBFS_WD_COUNT) &amp;&amp; !done; i++) &#123; if (wd == context-&gt;wds[i]) &#123; snprintf(path, sizeof(path), USB_FS_DIR "/%03d/%s", i, event-&gt;name); if (event-&gt;mask == IN_CREATE) &#123; done = context-&gt;cb_added(path, context-&gt;data); &#125; else if (event-&gt;mask == IN_DELETE) &#123; done = context-&gt;cb_removed(path, context-&gt;data); &#125; &#125;&#125; 如果传来的数据是跟真正 USB 设备相关的（通过 watch descriptor 来判断，而且结合上面对 context-&gt;wds[] 数组的说明），那么就通过调用相应的回调函数来通知上层相应的设备变动。 好，底层的这部分说明完了，接下来就要 JNI 层了。JNI 层负责给调用上面提到的函数，指定合适的回调函数，具体位置在：frameworks/base/services/core/jni/com_android_server_UsbHostManager.cpp: 12345678910static void android_server_UsbHostManager_monitorUsbHostBus(JNIEnv* /* env */, jobject thiz)&#123; struct usb_host_context* context = usb_host_init(); if (!context) &#123; ALOGE(&quot;usb_host_init failed&quot;); return; &#125; // this will never return so it is safe to pass thiz directly usb_host_run(context, usb_device_added, usb_device_removed, NULL, (void *)thiz);&#125; 在联系我们在 libusb 上提到的 usb_host_init() 和 usb_host_run() ，有没有一种 connected 的感觉呢？ 我们前面说过，JNI 层的函数是被 UsbHostManager 调用的，我们再来看看 UsbHostManager 中具体的监控进程，位置在 frameworks/base/services/usb/java/com/android/server/usb/UsbHostManager.java： 123456789101112public void systemReady() &#123; synchronized (mLock) &#123; // Create a thread to call into native code to wait for USB host events. // This thread will call us back on usbDeviceAdded and usbDeviceRemoved. Runnable runnable = new Runnable() &#123; public void run() &#123; monitorUsbHostBus(); &#125; &#125;; new Thread(null, runnable, "UsbService host thread").start(); &#125;&#125; 那么如何打通这两者呢？哈，我们上面提到的 Android JNI 就扮演着连接者的角色。看看下面的代码你就明白了： 1234static JNINativeMethod method_table[] = &#123; &#123; &quot;monitorUsbHostBus&quot;, &quot;()V&quot;, (void*)android_server_UsbHostManager_monitorUsbHostBus &#125;, ......&#125;; 至此，Android 发现外部 USB 设备的整个流程我们就说明完了。 EOF]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android Framework</tag>
        <tag>USB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JNI 编程初探]]></title>
    <url>%2F2016%2F03%2F16%2Fjni-programming%2F</url>
    <content type="text"><![CDATA[提起 Java，我们的耳边似乎总会响起这样的论调： Java 使用虚拟机，性能效率不行，跟 C/C++ 没法比 好像说，Java 跟性能是绝缘的，C/C++ 才是性能的典范。那么，能否将两者结合起来呢？当然可以，就是我们今天的主题 —— Java JNI 。简单地说，Java JNI 就是将 Java 类中的一些方法用 C/C++ 甚至是汇编来实现，以弥补 Java 某些方面的不足。下面我们通过一个简单的 Hello world 来阐述整个流程是怎么样的。 流程首先，我们在 jni_first.java 中定义一个类，并且在类中声明一个方法，这个方法就是我们要使用 C/C++ 来实现的： 12345678910public class jni_first &#123; static &#123; System.loadLibrary("first"); &#125; public native void disp_jni(); public static void main(String[] args) &#123; new jni_first().disp_jni(); &#125;&#125; *注意，上面的 System.loadLibrary(&quot;balabala&quot;) 里面的参数是有讲究的。他就是我们 native 方法所在的 C/C++ 文件编译之后的库的文件名。所以在 Windows 下，我们的 C/C++ 文件要编译为 balabala.dll ，在 Linux 下要编译为 libbalabala.so *，具体我们后面讲。 然后我们首先要将该 Java 文件编译成 class 文件： 1javac jni_first.java 然后我们使用 javah 来生成头文件，头文件里面定义了我们 native 方法的函数名。也就是说，我们的 native 方法的函数名使不能让我们随便取的，而是要让机器生成的： 1javah jni_first 接下来，我们就会发现我们当前的目录下出现了一个 jni_first.h 的文件，打开该文件看看它的内容： 123456789101112131415161718192021/* DO NOT EDIT THIS FILE - it is machine generated */#include &lt;jni.h&gt;/* Header for class jni_first */#ifndef _Included_jni_first#define _Included_jni_first#ifdef __cplusplusextern "C" &#123;#endif/* * Class: jni_first * Method: disp_jni * Signature: ()V */JNIEXPORT void JNICALL Java_jni_1first_disp_1jni (JNIEnv *, jobject);#ifdef __cplusplus&#125;#endif#endif 于是我们可以看到我们的 native 方法应该叫什么：Java_jni_1first_disp_1jni： 接下来，实现我们的 native 方法： 12345678910#include "jni_first.h"#include &lt;stdio.h&gt;JNIEXPORT void JNICALL Java_jni_1first_disp_1jni(JNIEnv *, jobject)&#123; printf("From %s: ", __func__); printf("Hello, Java world.\n"); return ;&#125; 编译成库： g++ -shared -fPIC -I /usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux jni_first.cpp -o libfirs.so注意这里的 -I 参数，第一个 -I 指定了 jni.h 文件的位置，第二个 -I 指定了 jni_md.h 文件的位置（在 jni.h 中被引用）。同时注意最后生成的库名，在 Windows 应该以 .dll 为后缀，在 Linux 应该以 lib 为前缀，.so 为后缀。 最后，执行： java -Djava.library.path=. jni_first这里的 -Djava.library.path=. 指定了库所在的位置是当前路径，不然会报错。于是，我们就可以看到以下输出了： $ java -Djava.library.path=. jni_first From Java_jni_1first_disp_1jni: Hello, Java world.好了，至此，我们就把整个流程都走了一遍了。可以看到，Java JNI 和 Android JNI 的一个区别就是 Android JNI 可以通过 JNINativeMethod 数组来指定 native 方法的函数名，而 Java JNI 只能按照机器生成的头文件来。 EOF]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>JNI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Page Fram Reclaim Algorithm]]></title>
    <url>%2F2016%2F02%2F03%2Fpage-fram-reclaim-algorithm%2F</url>
    <content type="text"><![CDATA[一般来说，我们的手机的内存 RAM 的容量是固定的，用多少，就少多少，这是不可逆转的。为了解决这个难题，内存交换的思想应运而生，通过将一些不活动页交换到速度较慢的硬盘，SD 卡等，然后将这些内存腾出来，从而变相地达到了增大内存的效果。（然而这个并不是新东西，我读过 Linux 0.12 的源代码，发现在那个时候就已经有这个东东了） 整个内存交换分为两个部分——页面回收，页面回写。页面回收主要是通过一系列的算法判断筛选出回收页的候选者，页面回写则负责将这些页回写到交换设备中。这里面最重要和最复杂的是页面回收算法，也是这篇东东所要描述的重点。 注：在学习这一部分内容的时候，我查阅了大量的资料，并且发现而英语术语的重要性，所以对于重要的术语，我会在后面注明它的英文原文。 预备知识 匿名页（anonymous page） 和 文件页（file page） 匿名页指的是内存页的内容没有备份在文件或者其他后备设备（backup device），一般我们进程的栈和堆都属于这一类型，或者我们在使用 mmap() 进行映射的时候，加入了 MAP_ANONYMOUS 的参数。这种内存页的特点是，内存页里面的内容，丢了就没了。所以如果在将这一种内存页交换出去的时候，必须为其申请一块交换空间（swap space） 文件页则是与上面的相反，这一类内存页里面的内容都有备份到后备设备。但是这一类内存页存在一个问题就是同步，即内存里面的数据被修改了，从而导致内存页变成了脏页（dirty page），这个时候就需要进行回写（writeback），使其后备设备的内容与内存页的内容一致。 struct page 中一些重要的 flag PG_swapcache: 说明当前页是页缓存。并且当前页的 struct page 里面的 private 成员指向的是交换设备的标识符，mapping 成员指向 NULL 。（具体什么是页缓存请看下一节：重要的数据结构） PG_swapbacked: 当前页被备份在 RAM 或者是 swap 分区上。 PG_reclaim: 似乎是表明当前页是脏的，但是还没有进行回写操作。 注意：内核有专门的函数来检测这些标志位，例如 PG_swapcache 就是用 PageSwapCache()，PG_reclaim 就是用 PageReclaim() 。但是，如果你通过 grep 或者其他工具根据函数名去搜索这些函数的实现的时候，你会很神奇地发现你找不到。其实这是内核使用的一个技巧，因为这些函数大同小异，所以系统使用了宏（macro）来统一表达。所以你就会发现，这么一堆函数在内核的实现中只用了两行（具体在 include/linux/page-flags）: 12static inline int Page##uname(const struct page *page) \ &#123; return test_bit(PG_##lname, &amp;page-&gt;flags); &#125; 所以，这也给了我们一个提示，如果我们在搜索一些变量或者函数的时候找不到它的实现，那么很有可能内核使用了宏来实现，而解决方法页很简单，就是抽取出这些变量或者函数的共同处，然后使用这个共同处去搜索，会有意想不到的收获～ struct zone 中一些重要的 flag ZONE_DIRTY: 页回收扫描过程最近在 LRU 链表的尾部发现了大量的脏页 ZONE_WRITEBACK: 回收扫描阶段遇到大量正在处于回写阶段的内存页 ZONE_CONGESTED: 当前的 zone 有许多备份在已经处于拥塞（congested）的后备设备的脏页。 一些重要的内存分配器（memory allocator）的 flag 其实这里比较模糊的是 __GFP_IO 和 __GFP_FS，而要理解这两个的区别就需要知道一个知识点——交换设备是没有文件系统的（当然，交换设备有可能是存放在文件系统上的文件），所以对这些设备的写操作不需要特别繁琐的过程。所以在代码中你会看到这样的代码组合： 1(PageSwapCache(page) &amp;&amp; (sc-&gt;gfp_mask &amp; __GFP_IO)); 重要的数据结构注意，我这里提到的都是 Linux Kernel 3.x 之后变动很大的数据结构，如果没有提到的，都是基本保持不变的，直接看书即可，几乎市面上所有的内核书都是基于 Linux Kernel 2.6.x 的。 struct vm_area_struct 这个数据结构是用来描述虚拟内存区域（virtual memory area）的，关于这个数据结构的详细说明我想放在逆向映射（reverse mapping），这里主要提一点，就是内核是如何分辨一个内存映射是匿名映射（anonymous mapping）和文件映射（file mapping）。首先，每个物理页帧（page flame）都有一个 struct page 的结构体变量实例，这个结构体里面有一个成员 mapping 。每个进程的所有内存区域都是用 struct vm_area_struct 来统一管理的，进程通过 struct task_struct 里面的成员 struct vm_area_struct *mmap 来管理关联到该进程的所有 struct vm_area_struct 实例（通过链表和红黑树）。一个页帧要关联到一个进程，就是要关联到一个 struct vm_area_struct 。具体的流程是这样的：如果是匿名映射，那么将 struct page 里面的 mapping 指向所在的 struct vm_arear_struct 里面的 struct anon_vma 加上 PAGE_MAPPING_ANON ，这样通过判断 page-&gt;mapping 的最低位是否为 PAGE_MAPPING_ANON 就可以很好的区分匿名映射。这样就可以通过 page-&gt;mapping 获取页帧的 struct anon_vma ，最后通过 container_of 之类的宏就可以访问到整个 struct vm_area_struct 。 struct lruvec 在 Linux Kernel 2.6 以及之前的版本，内核试图将内存页分类到两个 LRU 链表 —— 活动的（active）和不活动的（inactive），系统是通过在每个内存域（zone）的结构体 struct zone 里面两个成员 struct list_head active_list 和 struct list_head inactive_list 来表示的。后来，Linux Kernel 又将这两个链表根据它们的来源进行了细化和划分，将每个链表分成了 file 和 anon ，再加上一个不可驱逐的 unevictable 的链表，总共有 5 个 LRU 链表。 123456789101112131415 struct lruvec &#123; struct list_head lists[NR_LRU_LISTS]; struct zone_reclaim_stat reclaim_stat; #ifdef CONFIG_MEMCG struct zone *zone; #endif &#125;; enum lru_list &#123; LRU_INACTIVE_ANON = LRU_BASE, LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE, LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE, LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE, LRU_UNEVICTABLE, NR_LRU_LISTS&#125;; 然后，需要指出的是：回收行为分为 global reclaim 和 target reclaim 。其中， target reclaim 是针对 memory cgroup 的（因为 memory cgroup 也包含有 struct lruvec 这个数据结构） ，而 global reclaim 是针对整个内存域的。 struct scan_control 这是一个很重要的数据结构，扫描的调用者通过这个数据结构向执行回收扫描的函数来控制扫描行为，这里面有几个比较重要的成员变量要说明一下（只挑选了一部分）： 123456789101112131415161718192021222324252627struct scan_control &#123; /*指明了 shrink_list() 应该回收多少内存页 */ unsigned long nr_to_reclaim; /* 这个是传给后续页面回收函数的 GFP 标志 */ gfp_t gfp_mask; /* 系统通过判断这个成员是否为 NULL 来判断 global reclaim 还是 target reclaim */ struct mem_cgroup *target_mem_cgroup; /* 扫描的数目是整个 LRU 的大小 &gt;&gt; priority （具体在 get_scan_count() 体现） * 也就是说，priority 越大，扫描的数目就越少 */ int priority;// 指定了内核是否允许将页面写出到后备设备上 unsigned int may_writepage:1; /* 指定了已经映射的页面能否被回收 */ unsigned int may_unmap:1; /* 指定了页面回收阶段是否允许页交换 */ unsigned int may_swap:1; /* 到目前为止通过 shrink_zones() 释放掉的页面数目 */ unsigned long nr_reclaimed;&#125;; 交换缓存（swap cache） 交换缓存是个很重要的数据结构，不过由于它是属于页面回写部分的，而且，它是属于页缓存的一种。所以，比较详细的实现就暂且留个 TODO ，不过，大体上还是能够说明这个交换缓存的作用的。 想象没有交换缓存的情形，现在有一个内存页被两个进程在同时使用。此时如果内核选择将这个内存页交换出去（swap out）那么这两个进程都会将该页对应的页表项修改为页面交换所在的后备设备。然后，进程 A 想要读该内存页，那么内核将会把这个内存页换回。然后问题来了，如果接下来进程 B 也想要读该内存页，但是 B 并不知道 A 已经将该页换入（swap in）了，所以势必会造成页面的重复换入，这是一种浪费的行为，也根本没必要。所以交换缓存就应运而生了。 对于交换缓存是如何解决这个问题的，我觉得没有那一本书能够讲得比《Understanding the Linux Kernel》这本书详细了，而且我对自己的表述能力肯定无法超过这本书，所以下将书中的相关内容摘录如下： Before being written to disk, each page to be swapped out is stored in the swap cache by shrink_list(). Consider a page P that is shared among two processes, A and B. Initially, the Page Table entries of both processes contain a reference to the page frame, and the page has two owners. When the PFRA selects the page for reclaiming, shrink_list( ) inserts the page frame in the swap cache, now the page frame has three owners, while the page slot in the swap area is referenced only by the swap cache. Next, the PFRA invokes try_to_unmap( ) to remove the references to the page frame from the Page Table of the processes; once this function terminates, the page frame is referenced only by the swap cache, while the page slot is referenced by the two processes and the swap cache. Let’s suppose that, while the page’s contents are being written to disk, process B accesses the pagethat is, it tries to access a memory cell using a linear address inside the page. Then, the page fault handler finds the page frame in the swap cache and puts back its physical address in the Page Table entry of process B. Conversely, if the swap-out operation terminates without concurrent swap-in operations, the shrink_list( ) function removes the page frame from the swap cache and releases the page frame to the Buddy system. 简单的说，就是进程每一次需要调入页的时候，都必须要搜索一下交换缓存是否有当前要换入的页。 详细说明shrink_lruvec()首先，shrink_lruvec() 先调用 get_scan_count() 来统计各个 LRU 链表要扫描的内存页的数目： get_scan_count()首先，判断是否需要强制扫描，这个是通过 force_scan 来表示的。需要强制扫描有以下两种情况： 当前进程是 kswapd() ，并且当前的 zone 并不需要回收 是 memory cgroup 在进行页面回收 首先说明一下强制扫描指的是什么？前面提到过，扫描行为最终在每个 LRU 链表的扫描数目是根据当前 LRU 链表的内存页数 &gt;&gt; sc-&gt;priority，所以扫描数目是有可能为 0 的，所以如果进行强制扫描的话，那么如果上面的计算结果为 0 的话，也要重新计算扫描的数目。 get_scan_count() 的职责是根据当前 zone 的情况，判断需要扫描的 LRU 链表的类型，并且是首先倾向于扫描 file 链表的（因为其有后备设备，可以直接释放），并且要以一定的比例来分别确定 file 链表和 anon 链表的数目： // 注意： // nr[0] 是 anon inactive pages 要扫描的数目; nr[1] = anon active pages 要扫描的数目 // nr[2] = file inactive pages 要扫描的数目; nr[3] = file active pages 要扫描的数目 static void get_scan_count(struct lruvec *lruvec, int swappiness, struct scan_control *sc, unsigned long *nr) { ...... // 如果指示回收过程不允许页交换，或者系统没有交换空间，则只扫描 file page ，则是非常显然的，不然没有地方给匿名页分配 swap space if (!sc-&gt;may_swap || (get_nr_swap_pages() &lt;= 0)) { scan_balance = SCAN_FILE; goto out; } // 这里首先得理解一下 swappiness 这个参数的意义。swappiness 是一个系统参数，它控制了系统交换的好斗程度（aggressive），当取值小的时候，系统倾向于不交换，取值大的时候，系统则会进行页交换。默认值是 60 。当取值为 0 的时候，系统除非为了相应 OOM（out of memory） ，否则不进行页交换。 // 然后还得注意，swappiness 有两份，一份是控制全局的，一份是每个 memory cgroup 的，在下面的这个 case 是 cgroup 的 swappiness if (!global_reclaim(sc) &amp;&amp; !vmscan_swappiness(sc)) { scan_balance = SCAN_FILE; goto out; } // 前面提到了 sc-&gt;priority 的意义，也就说，priority 越小，扫描的数目就越多，当 priority = 0 的时候，说明系统已经接近 OOM 了，那么此时除非系统 swappiness 等于 0 （也就是不进行页交换），否则，则 file page 和 anonymous page 都要扫描 if (!sc-&gt;priority &amp;&amp; vmscan_swappiness(sc)) { scan_balance = SCAN_EQUAL; goto out; } /* * Prevent the reclaimer from falling into the cache trap: as * cache pages start out inactive, every cache fault will tip * the scan balance towards the file LRU. And as the file LRU * shrinks, so does the window for rotation from references. * This means we have a runaway feedback loop where a tiny * thrashing file LRU becomes infinitely more attractive than * anon pages. Try to detect this based on file LRU size. */ // 上面的注释我看不大懂，所以保留 // 这里先说明以下 zone 里面的 watermark 的意义：这是系统根据当前 zone 的内存大小设置的三个水位线，用以区分当前内存的紧缺程度 // HIGH_WMARK: 如果超过这个值，说明当前的内存剩余是理想的，如果小于 HIGH_WMARK 则是要开始页回收了 // 不过根据下面的代码推测，似乎是如果当前 zone 的 file page 的总量加上剩余的内存页重量低于 HIGH_WMARK ，则要扫描 anon 链表，不过具体的逻辑还得进一步探究 // TODO: 理解上面的注释 if (global_reclaim(sc)) { unsigned long zonefile; unsigned long zonefree; zonefree = zone_page_state(zone, NR_FREE_PAGES); zonefile = zone_page_state(zone, NR_ACTIVE_FILE) + zone_page_state(zone, NR_INACTIVE_FILE); if (unlikely(zonefile + zonefree &lt;= high_wmark_pages(zone))) { scan_balance = SCAN_ANON; goto out; } } // 当 LRU 链表上的不活动页足够的时候，那么就只回收 file page if (!inactive_file_is_low(lruvec)) { scan_balance = SCAN_FILE; goto out; } // 默认情况是 SCAN_FRACT scan_balance = SCAN_FRACT; // 在这里，swappiness 这个变量的意义似乎就显示出来了。swappiness 控制了匿名页的优先级，如果 swappiness 等于 100 ，那么匿名页和 file page 的优先级一样。 anon_prio = vmscan_swappiness(sc); file_prio = 200 - anon_prio; anon = get_lru_size(lruvec, LRU_ACTIVE_ANON) + get_lru_size(lruvec, LRU_INACTIVE_ANON); file = get_lru_size(lruvec, LRU_ACTIVE_FILE) + get_lru_size(lruvec, LRU_INACTIVE_FILE); ......... /* * OK, so we have swap space and a fair amount of page cache * pages. We use the recently rotated / recently scanned * ratios to determine how valuable each cache is. * * Because workloads change over time (and to avoid overflow) * we keep these statistics as a floating average, which ends * up weighing recent references more than old ones. * * anon in [0], file in [1] */ // PROBLEM: 这里的逻辑还不是太懂，注释保留 spin_lock_irq(&amp;zone-&gt;lru_lock); if (unlikely(reclaim_stat-&gt;recent_scanned[0] &gt; anon / 4)) { reclaim_stat-&gt;recent_scanned[0] /= 2; reclaim_stat-&gt;recent_rotated[0] /= 2; } if (unlikely(reclaim_stat-&gt;recent_scanned[1] &gt; file / 4)) { reclaim_stat-&gt;recent_scanned[1] /= 2; reclaim_stat-&gt;recent_rotated[1] /= 2; } /* * The amount of pressure on anon vs file pages is inversely * proportional to the fraction of recently scanned pages on * each list that were recently referenced and in active use. */ // 这里依旧无法解释，不过经过大量的搜索以及跟踪 Kernel 中这一部分的改动，似乎只是为了更加合理的计算出要扫描的 file page 和 anonymous page 的数量 // 之前的 Kernel 版本中，ap 和 fp 的数量不是这么计算的，这里之所以是这么计算是为了保证当 swappiness 等于 0 的时候，ap 能够真的等于 0 。 // 具体可以查看： // 1) http://gitorious.ti.com/ti-linux-kernel/ti-linux-kernel/commit/fe35004fbf9eaf67482b074a2e032abb9c89b1dd?format=patch // 2) https://eklitzke.org/swappiness ap = anon_prio * (reclaim_stat-&gt;recent_scanned[0] + 1); ap /= reclaim_stat-&gt;recent_rotated[0] + 1; fp = file_prio * (reclaim_stat-&gt;recent_scanned[1] + 1); fp /= reclaim_stat-&gt;recent_rotated[1] + 1; spin_unlock_irq(&amp;zone-&gt;lru_lock); fraction[0] = ap; fraction[1] = fp; denominator = ap + fp + 1; out: // 这里的 some_scanned 的作用就是一个二次机会（second chance）的思想 some_scanned = false; for (pass = 0; !some_scanned &amp;&amp; pass &lt; 2; pass++) { for_each_evictable_lru(lru) { int file = is_file_lru(lru); unsigned long size; unsigned long scan; // 看，这里计算出了每个链表要扫描的页的数目 size = get_lru_size(lruvec, lru); scan = size &gt;&gt; sc-&gt;priority; // 如果 scan 等于 0，并且前面已经标记要强制扫描了，那么就在这里给它第二次机会 // 必须在第二轮，也就是 pass 等于 1 if (!scan &amp;&amp; pass &amp;&amp; force_scan) scan = min(size, SWAP_CLUSTER_MAX); switch (scan_balance) { case SCAN_EQUAL: /* Scan lists relative to size */ break; case SCAN_FRACT: // 似乎有点理解 SCAN_FRACT 的意思了。FRACT 就是 fraction，就是默认的扫描类型，主要是根据上面计算出来的 ap 和 fp 的数量，对 scan （要扫描的总量）进行等比例的分配 scan = div64_u64(scan * fraction[file], denominator); break; case SCAN_FILE: case SCAN_ANON: /* Scan one type exclusively */ // 如果是 file lru 要扫描 anon page 或者是 anon lru 要扫描 file page ，则 scan 为 0 if ((scan_balance == SCAN_FILE) != file) scan = 0; break; default: BUG(); } nr[lru] = scan; // 如果我们找到有内存页可以扫描，那么就不用使用 second chance 了 // 这里的 !! 的作用是让零值位零，非零值为一 // 前面的循环的条件是 !some_scanned some_scanned |= !!scan; } } }好，计算了各个链表应该扫描的数目之后，就要开始进行扫描了： scan_adjusted = (global_reclaim(sc) &amp;&amp; !current_is_kswapd() &amp;&amp; sc-&gt;priority == DEF_PRIORITY); blk_start_plug(&amp;plug); while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] || nr[LRU_INACTIVE_FILE]) { unsigned long nr_anon, nr_file, percentage; unsigned long nr_scanned; for_each_evictable_lru(lru) { if (nr[lru]) { // 如果 lru 链表要移除的超过了 SWAP_CLUSTER_MAX ，那么移除操作将拆成多步完成 // 至于为什么要这么做，可以参考这个链接： // http://blog.csdn.net/dog250/article/details/5303568 nr_to_scan = min(nr[lru], SWAP_CLUSTER_MAX); nr[lru] -= nr_to_scan;K nr_reclaimed += shrink_list(lru, nr_to_scan, lruvec, sc); } }从上面的代码我们可以看到，每一次扫描的数目不超过 SWAP_CLUSTER_MAX。 同时，要注意一个点，就是 scan_adjusted 这个变量的作用。我现在还不是太清楚这个变量的作用，不过通过跟踪内核上游的代码变更，我找到了加入了这个变量的几个 patch ，通过里面的 commit 好像是说为了保证一开始当 sc-&gt;priority == DEF_PRIORITY 的时候，需要扫描更多的页。所以引入了这个变量，因为在循环后面的操作会减少内存页扫描的数目。具体可以看看这几个 patch ： mm: vmscan: Obey proportional scanning requirements for kswapd performance regression due to commit e82e0561 好，上面是一个小插曲，我们接下来看看重头戏，扫描回收的主程序： shrink_list()static unsigned long shrink_list(enum lru_list lru, unsigned long nr_to_scan, struct lruvec *lruvec, struct scan_control *sc) { if (is_active_lru(lru)) { if (inactive_list_is_low(lruvec, lru)) shrink_active_list(nr_to_scan, lruvec, sc, lru); return 0; } return shrink_inactive_list(nr_to_scan, lruvec, sc, lru); }如果当前是 active lru 并且 inactive lru 里面的内存页太少了，那么将调用 shrink_active_list(): shrink_active_list()这个函数的主要作用是从 active lru 移动一些内存页到 inactive lru。为了避免锁的麻烦，shrink_active_list() 和 shrink_inactive_list() 都使用了一个 isolate_lru_pages() 来将扫描的内存页隔离到一个临时的链表中。这个函数没有什么好说的，不过有一点比较重要，就是可能存在页结合（conbine page）的情况，这个在下面会提到： static void shrink_active_list(unsigned long nr_to_scan, struct lruvec *lruvec, struct scan_control *sc, enum lru_list lru) { ...... LIST_HEAD(l_hold); /* The pages which were snipped off */ LIST_HEAD(l_active); LIST_HEAD(l_inactive); ...... // 这个函数是是将每个 CPU 的页向量写回到 lru 链表上 // 页向量是一个 Per-CPU 的变量 // 这个在变动不大，所以就不详细说明了，看书就好 lru_add_drain(); ...... // 从当前的 lru 页表隔离 nr_to_scan 个页面到 l_hold 链表上 // @nr_to_scan: 计划要扫描的页的数目 // @nr_scanned: 实际扫描的数目。注意，这里是忽略了 combine page 的，也就是说，不管是多少页合并到一起，都是算作一页 // @nr_taken: 扫描的总页数。这里是考虑 combine page 的 nr_taken = isolate_lru_pages(nr_to_scan, lruvec, &amp;l_hold, &amp;nr_scanned, sc, isolate_mode, lru); ...... while (!list_empty(&amp;l_hold)) { cond_resched(); page = lru_to_page(&amp;l_hold); list_del(&amp;page-&gt;lru); if (unlikely(!page_evictable(page))) { // 将当前页移回 lru 链表上 putback_lru_page(page); continue; } // 这一块也不是很清楚 if (unlikely(buffer_heads_over_limit)) { if (page_has_private(page) &amp;&amp; trylock_page(page)) { if (page_has_private(page)) try_to_release_page(page, 0); unlock_page(page); } } // TODO: 逆向映射是个大工程啊，得花很长的时间去说明，这里先跳过 // page_referenced() 的返回值是当前页的引用数目 if (page_referenced(page, 0, sc-&gt;target_mem_cgroup, &amp;vm_flags)) { nr_rotated += hpage_nr_pages(page); /* * Identify referenced, file-backed active pages and * give them one more trip around the active list. So * that executable code get better chances to stay in * memory under moderate memory pressure. Anon pages * are not likely to be evicted by use-once streaming * IO, plus JVM can create lots of anon VM_EXEC pages, * so we ignore them here. */ // 根据注释，好像是为了防止一些页被加入到 inactive list ，然后给他们第二次机会 // 也就是说，只要当前页有被引用到，那么就有机会重新回到 active list 上 if ((vm_flags &amp; VM_EXEC) &amp;&amp; page_is_file_cache(page)) { list_add(&amp;page-&gt;lru, &amp;l_active); continue; } } ClearPageActive(page); /* we are de-activating */ list_add(&amp;page-&gt;lru, &amp;l_inactive); } ...... // l_active 移到 lru 活动链表，l_inactive 移动到 lru 不活动链表 // 同时，上面的操作会将内存页的引用次数减去一，并且判断是否为 0 。并移到相应的 LRU 链表中 // 如果为 0 的话，那么就去掉这个函数之后加入的 PG_lru 标志以及 PG_active 标志，从 LRU 链表中移除，并重新放回原来的 l_hold 链表中 move_active_pages_to_lru(lruvec, &amp;l_active, &amp;l_hold, lru); move_active_pages_to_lru(lruvec, &amp;l_inactive, &amp;l_hold, lru - LRU_ACTIVE); __mod_zone_page_state(zone, NR_ISOLATED_ANON + file, -nr_taken); spin_unlock_irq(&amp;zone-&gt;lru_lock); mem_cgroup_uncharge_list(&amp;l_hold); // 依然就在 l_hold 链表的内存页都是引用次数为 0 的内存页，所以将他们释放回伙伴系统 free_hot_cold_page_list(&amp;l_hold, true); }如果 inactive lru 的内存页数目充足，那么就开始调用 shrink_inactive_list()： shrink_inactive_list()*这个函数的主要作用是从 inactive list 上的内存页释放掉一些返回给伙伴系统 *，这里面涉及到的逻辑很多，我们详细地说明： static noinline_for_stack unsigned long shrink_inactive_list(unsigned long nr_to_scan, struct lruvec *lruvec, struct scan_control *sc, enum lru_list lru) { LIST_HEAD(page_list); unsigned long nr_scanned = 3000; unsigned long nr_reclaimed = 0; unsigned long nr_taken; unsigned long nr_dirty = 0; unsigned long nr_congested = 0; unsigned long nr_unqueued_dirty = 0; unsigned long nr_writeback = 0; unsigned long nr_immediate = 0; isolate_mode_t isolate_mode = 0; int file = is_file_lru(lru); struct zone *zone = lruvec_zone(lruvec); struct zone_reclaim_stat *reclaim_stat = &amp;lruvec-&gt;reclaim_stat; lru_add_drain(); ...... nr_taken = isolate_lru_pages(nr_to_scan, lruvec, &amp;page_list, &amp;nr_scanned, sc, isolate_mode, lru); nr_reclaimed = shrink_page_list(&amp;page_list, zone, sc, TTU_UNMAP, &amp;nr_dirty, &amp;nr_unqueued_dirty, &amp;nr_congested, &amp;nr_writeback, &amp;nr_immediate, false);函数首先的套路跟 shrink_active_list() 是一模一样的，都是调用 isolate_lru_pages() 从当前的 lru 链表中移除一部分的内存页到一个临时链表，然后，整个重头戏来了，执行最终的回写释放操作都在接下来调用的 shrink_page_list() 这个函数中，可以说，这个函数是连接页面回收和页面回写的中间桥梁。 shrink_page_list()这个函数将从 shrink_inactive_list() 传来的临时链表上的内存页进行回写操作。当然有一些内存页是不需要回写并且需要重新放回 LRU 链表的。 static unsigned long shrink_page_list(struct list_head *page_list, struct zone *zone, struct scan_control *sc, enum ttu_flags ttu_flags, unsigned long *ret_nr_dirty, unsigned long *ret_nr_unqueued_dirty, unsigned long *ret_nr_congested, unsigned long *ret_nr_writeback, unsigned long *ret_nr_immediate, bool force_reclaim) { LIST_HEAD(ret_pages); LIST_HEAD(free_pages); int pgactivate = 0; unsigned long nr_unqueued_dirty = 0; unsigned long nr_dirty = 0; unsigned long nr_congested = 0; unsigned long nr_reclaimed = 0; unsigned long nr_writeback = 0; unsigned long nr_immediate = 0; cond_resched(); while (!list_empty(page_list)) { struct address_space *mapping; struct page *page; int may_enter_fs; enum page_references references = PAGEREF_RECLAIM_CLEAN; bool dirty, writeback; cond_resched(); //取出一页，将其从当前链表删除 page = lru_to_page(page_list); list_del(&amp;page-&gt;lru); //锁定当前页，具体做法是在 page-&gt;flag 打上 PG_lock 的标志 if (!trylock_page(page)) goto keep; sc-&gt;nr_scanned++; // 当前页不可驱逐 // 一个页面是不可驱逐的原因有两个： // 1. 当前页面的映射是不可驱逐的：mapping-&gt;flag // 2. 当前页面是 mloacked VMA 的一部分 if (unlikely(!page_evictable(page))) goto cull_mlocked; // 当前 scan_control 表示不能回收已经 map 的页，但是当前页已经 mapped 了 if (!sc-&gt;may_unmap &amp;&amp; page_mapped(page)) goto keep_locked; /* Double the slab pressure for mapped and swapcache pages */ // PROBLEM：这一句不是很清楚，这跟 slab 有什么关系 if (page_mapped(page) || PageSwapCache(page)) sc-&gt;nr_scanned++; // 后续操作中可能进行 IO 操作？这里就是我们上面提到的 __GFP_FS 和 __GFP_IO 的区别所在了 may_enter_fs = (sc-&gt;gfp_mask &amp; __GFP_FS) || (PageSwapCache(page) &amp;&amp; (sc-&gt;gfp_mask &amp; __GFP_IO)); // 页面是脏的或者正处在回写状态 page_check_dirty_writeback(page, &amp;dirty, &amp;writeback); if (dirty || writeback) nr_dirty++; // 页面是脏的并且还没有在进行回写 // 回写肯定是要在等待队列中排队，所以这里是 unqueued if (dirty &amp;&amp; !writeback) nr_unqueued_dirty++; mapping = page_mapping(page); // 如果当前页有被映射，但是后备设备正处在拥塞状态 if ((mapping &amp;&amp; bdi_write_congested(mapping-&gt;backing_dev_info)) || (writeback &amp;&amp; PageReclaim(page))) nr_congested++; ...... if (!force_reclaim) references = page_check_references(page, sc);在这里要说明一下 page_check_references() 这个函数。我们在看《深入理解 Linux 内核》或者《深入 Linux 内核架构》这些内核书的时候，会看到这样的一个名词——第二次机会（second chance）算法。这个函数就充分地表现了这个算法。这个函数只要发现当前的内存页被引用（reference）了，那么就会试图让当前页不被回收而是重新回到 LRU 链表上。这个算法也是避免页颠簸（page thrashing）的一个重要的体现。我们既要回收不活动的内存页，也要避免将活动页回收。这也是为什么这一部分的代码十分繁杂的缘故了。因为他要考虑到很多很多中情况。 // 根据当前页的状态，如果是 RECLAIM 状态的，就在接下来进行回收，否则则跳出进行处理 switch (references) { case PAGEREF_ACTIVATE: goto activate_locked; case PAGEREF_KEEP: goto keep_locked; case PAGEREF_RECLAIM: case PAGEREF_RECLAIM_CLEAN: ; /* try to reclaim the page below */ } if (PageAnon(page) &amp;&amp; !PageSwapCache(page)) { if (!(sc-&gt;gfp_mask &amp; __GFP_IO)) goto keep_locked; if (!add_to_swap(page, page_list)) // INPORTANT：为当前页分配一个 swap entry 并且把它加入到 swap cache，具体请看上面关于页缓存的描述 goto activate_locked; may_enter_fs = 1; /* Adding to swap updated mapping */ mapping = page_mapping(page); } // 当前内存页被映射了，调用 try_to_unmap() 来解除映射 if (page_mapped(page) &amp;&amp; mapping) { switch (try_to_unmap(page, ttu_flags)) { case SWAP_FAIL: goto activate_locked; case SWAP_AGAIN: goto keep_locked; case SWAP_MLOCK: goto cull_mlocked; case SWAP_SUCCESS: ; /* try to free the page below */ } } if (PageDirty(page)) { // 如果当前页是脏的，那么必须回写 // 根据注释说明，只有 kswapd 能够回写备份在文件系统的内存页（page backed by a regular filesystem）。 // 或者是回收扫描操作遇到了许多的脏页（由 ZONE_DIRTY 标志位标志） // 所以如果当前当前页是备份在文件系统但是当前进程不是 kswap ， // 或者是当前还没有扫描到足够多的脏页，都不能进行回写 if (page_is_file_cache(page) &amp;&amp; (!current_is_kswapd() || !test_bit(ZONE_DIRTY, &amp;zone-&gt;flags))) { /* * Immediately reclaim when written back. * Similar in principal to deactivate_page() * except we already have the page isolated * and know it&apos;s dirty */ inc_zone_page_state(page, NR_VMSCAN_IMMEDIATE); SetPageReclaim(page); goto keep_locked; } if (references == PAGEREF_RECLAIM_CLEAN) goto keep_locked; if (!may_enter_fs) goto keep_locked; if (!sc-&gt;may_writepage) goto keep_locked; // 脏页在这里尝试写回 switch (pageout(page, mapping, sc)) { case PAGE_KEEP: goto keep_locked; case PAGE_ACTIVATE: goto activate_locked; case PAGE_SUCCESS: if (PageWriteback(page)) goto keep; if (PageDirty(page)) goto keep; if (!trylock_page(page)) goto keep; if (PageDirty(page) || PageWriteback(page)) goto keep_locked; mapping = page_mapping(page); case PAGE_CLEAN: ; /* try to free the page below */ } } ...... if (!mapping || !__remove_mapping(mapping, page, true)) goto keep_locked; __clear_page_locked(page); // 要释放的内存页添加到 free_pages 链表，要放回的放到 ret_pages 链表 free_it: nr_reclaimed++; list_add(&amp;page-&gt;lru, &amp;free_pages); continue; cull_mlocked: if (PageSwapCache(page)) try_to_free_swap(page); unlock_page(page); putback_lru_page(page); continue; activate_locked: if (PageSwapCache(page) &amp;&amp; vm_swap_full()) try_to_free_swap(page); VM_BUG_ON_PAGE(PageActive(page), page); SetPageActive(page); pgactivate++; keep_locked: unlock_page(page); keep: list_add(&amp;page-&gt;lru, &amp;ret_pages); VM_BUG_ON_PAGE(PageLRU(page) || PageUnevictable(page), page); } mem_cgroup_uncharge_list(&amp;free_pages); free_hot_cold_page_list(&amp;free_pages, true); // free_pages 链表上的内存页会被上面的 free_hot_cold_page_list() 释放掉 // 上面循环未处理的内存页所在的 ret_pages 将移到 page_list 链表（即调用者传进来的参数），然后在本函数的调用者那里被重新移到不活动链表上 list_splice(&amp;ret_pages, page_list); count_vm_events(PGACTIVATE, pgactivate); *ret_nr_dirty += nr_dirty; *ret_nr_congested += nr_congested; *ret_nr_unqueued_dirty += nr_unqueued_dirty; *ret_nr_writeback += nr_writeback; *ret_nr_immediate += nr_immediate; return nr_reclaimed; }这里要说明一下上面那几个 nr_ 变量的意义： nr_dirty: 当前页是脏的或者处于回写状态nr_unqueued_dirty: ** 当前页是脏的并且没有处于回写状态*nr_congested: * 当前页已经被映射并且该页的后备设备正处在阻塞状态（congested），或者是当前页正处在回写状态并且该页即将被回收*nr_immediate: * 好像是某一页 **marked for immediate reclaim and under writeback(nr_immediate).，也就是说，当前页即将被回收，而且正处在回写状态（这个跟上面是不一样的，只有当前 zone 标记了 ZONE_WRITEBACK 才将内存页计入这里）*nr_reclaimed: * 总共回收的页数 最后，ret_pages 链表中要返回的内存页重新放到了传进来的参数 page_list ，交给调用者去重新放回 LRU 链表。 然后，回到 shrink_inactive_list() ，回写操作完成之后，必须根据本次的回写情况重新界定当前 zone 的情况，这就要用到上面提到的那几个 nr_ 变量了。后面的代码比较的清晰直接，就补贴代码了。 我们重新回到 shrink_lruvec() ，当所有的 LRU 链表扫描完了之后，我们要重新计算下一次的扫描数目了。前面提到，整个扫描活动都是遵循则比例扫描的原则，计算方法： nr_file = nr[LRU_INACTIVE_FILE] + nr[LRU_ACTIVE_FILE]; nr_anon = nr[LRU_INACTIVE_ANON] + nr[LRU_ACTIVE_ANON]; // 如果某一个链表需要扫描的数目已经为 0 ，那么跳出循环，本次页面回收结束 if (!nr_file || !nr_anon) break; // 下面的做法是将剩下需要扫描数目少的清零，并且标志那个较大的 LRU 链表（lru 变量） // percentage：已经扫描的内存页相对于 get_scan_count() 计算出来的需要扫描的内存页的占比 // 用于提供下面的比例扫描的重新计算 if (nr_file &gt; nr_anon) { unsigned long scan_target = targets[LRU_INACTIVE_ANON] + targets[LRU_ACTIVE_ANON] + 1; lru = LRU_BASE; percentage = nr_anon * 100 / scan_target; } else { unsigned long scan_target = targets[LRU_INACTIVE_FILE] + targets[LRU_ACTIVE_FILE] + 1; lru = LRU_FILE; percentage = nr_file * 100 / scan_target; } // 少的我们就不扫描了 nr[lru] = 0; nr[lru + LRU_ACTIVE] = 0; // 根据占比重新计算需要扫描的数目 lru = (lru == LRU_FILE) ? LRU_BASE : LRU_FILE; nr_scanned = targets[lru] - nr[lru]; nr[lru] = targets[lru] * (100 - percentage) / 100; nr[lru] -= min(nr[lru], nr_scanned); lru += LRU_ACTIVE; nr_scanned = targets[lru] - nr[lru]; nr[lru] = targets[lru] * (100 - percentage) / 100; nr[lru] -= min(nr[lru], nr_scanned);在这里我想提一点，在我看这些源代码的过程中，经历了一个痛苦的过程，就是我自己掉入了一个逻辑怪圈——我看得懂代码，但是却参不透其背后的逻辑，然后就大量地进行搜索，依旧没有解答。后来开始跟踪 Linux Kernel 社区的邮件列表和 patch 变动，我终于得出了一个结论 —— 我们现在说看到的所有的代码的正确性都是相对的，也就是说，是这个 patch 的提交者认为这样写能够 work right ，我们自己也可以有自己的，不同的想法和结论。不要去纠结于里面的个中细节！！！！体会思想才是关键！！！！！整个页面回收算法体现了几个思想（我觉得叫做思想十分地贴切，或者用大白话说就是大方向），比例扫描，第二次机会，当然还有最重要的 LRU （least recently used），而内核代码的提交者都是在用他们自己的方式去描述他们心中的这些思想，这或许就是 Linux Kernel 自由的意义所在吧。 以上。 EOF]]></content>
      <categories>
        <category>Linux kernel</category>
      </categories>
      <tags>
        <tag>Linux kernel</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 内核代码风格检查]]></title>
    <url>%2F2015%2F11%2F03%2Flinux-kernel-code-check%2F</url>
    <content type="text"><![CDATA[Linux 作为一个人人都可以参与的大型项目，就必须规定其自身的代码风格来规范统一管理。在内核代码树里由一份文档 Documents/CodingStyle 详细地描述了这个代码风格的各种细节，我这里就不赘述了，我想来说一说写完代码之后如何检查是否符合规范。当然，人肉查询是完全可以的，不过内核代码树提供了一个检查的脚本可以供我们使用，路径在 scripts/checkpatch.pl ，一般可能会出现以下几种错误： ERROR: Does not appear to be a unified-diff format patch 出现这个错误只要加上 -f / –file 就可以，意思是：treat FILE as regular source file 如果是说编码错误的话，那么可以使用 vim 来进行转换。用 vim 打开源文件，然后使用这个命令： 1:set ff=unix WARING: Prefer [subsystem eg: netdev]_dbg([subsystem]dev, … the dev_dbg(dev, … then pr_debug(… to printk(KERN_DEBUG … 这个的话好像是说不要使用 printk , 目前我也没用到其他的子系统，所以都是使用 pr_debug 来代替 printk 的。 还有，在那个脚本我一般都加了如下的几个选项： 1./scipts/checkpatch.pl --file --terse --no-tree 差不多就是这样。 EOF]]></content>
      <categories>
        <category>Linux kernel</category>
      </categories>
      <tags>
        <tag>Linux kernel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 内存管理 -- Slab 分配器和 kmalloc]]></title>
    <url>%2F2015%2F11%2F01%2Flinux-memory-managent-slab-allocator-and-kmalloc%2F</url>
    <content type="text"><![CDATA[郑重说明，这一篇以及后序的博文我都尽量少贴代码，而是尽我的描述能力之所能把我自己的理解和思考的过程总结表达出来，千万别指望着看完我的某一篇博客就能彻底地理解某一个知识点，至少我是没有这样的表达能力的。想要知其所以然，唯一的途径就是： Read the F**king source code 正如侯捷在某一本书的扉页中写道的那样： 代码面前，没有秘密 以上 为什么要有 slab一般来说，一个新东西的产生总是为了解决某一个现有的问题的。那么，slab 是为了解决什么问题呢？我们知道，在 Linux 内核中的内存管理是使用伙伴系统 (Buddy system)，但是这个系统有一个问题就是，它的最小单位是页，即 PAGE_SIZE ，在 x86 架构中这个常数一般是 4k 。但是很多情况下我们要分配的单元大小是远远小于 4k 的，如果使用伙伴系统的话，必定是会产生很大的浪费的。所以，一个粒度更加小的分配器呼之欲出，slab 就是为了解决这个小粒度内存分配的问题的。 如何解决以及结构组织既然 slab 分配器已经定下了这样的一个目标，那么它的策略是什么呢？ 答曰，slab 分配器是基于所谓“面向对象”的思想，当然，这里的“面向对象”跟 C++ 和 Java 等的“面向对象”是不一样的。这里的“面向对象”更加确切的说法是“面向对象类型”，不同的类型使用不同的 slab ，一个 slab 只分配一种类型。而且，slab 为了提高内核中一些十分频繁进行分配释放的“对象”的分配效率， slab 的做法是：每次释放掉某个对象之后，不是立即将其返回给伙伴系统（slab 分配器是建立在伙伴系统之上的），而是存放在一个叫 array_cache 的结构中，下次要分配的时候就可以直接从这里分配，从而加快了速度。 这里必须明确几个概念，说明如下： 缓存(cache) : 这里的缓存只是一个叫法而已，其实就是一个管理结构头，它控制了每个 slab 的布局，具体的结构体是 struct kmem_cache 。（注意，虽然现在几乎所有的书或者博客都将这一个结构称为“缓存”，不过我觉得在这里称为“管理结构头”是更为合适的，所以下文中统一将“缓存（cache）”称为“管理结构头”。） slab: 从伙伴系统分配的 2^order 个物理页就组成了一个 slab ，而后续的操作就是在这个 slab 上在进行细分的，具体的结构体是 struct slab 。 对象(object) : 上面说到，每一个 slab 都只针对一个数据类型，这个数据类型就被称为该 slab 的“对象”，将该对象进行对齐之后的大小就是该“对象”的大小，依照该大小将上面的 slab 进行切分，从而实现了我们想要的细粒度内存分配。 per-CPU 缓存：这个就是上面提到的 array_cache ，这个是为了加快分配，预先从 slab 中分配部分对象内存以加快速度。具体的结构体是 struct array_cache，包含在 struct kmem_cache 中。 具体的结构如下图： 还有，我们在用户态编程的时候，需要分配内存的时候，一般都是使用 malloc() 函数来实现。那么在内核态编程中，如果我们要分配内存，而且又没有必要使用上面的基于某个特定对象的，内核给我们提供了一个类似 malloc() 的接口—— kmalloc() 。值得注意的是，其实 kmalloc() 也是基于 slab 分配器的，只不过它所需要的管理结构头已经按照 2^n 的大小排列事先准备好了而已，这个管理结构体数组是 struct cache_sizes malloc_sizes[] 。 还有，每个“对象”的缓存被组织成一个链表——cache_chain，然后每个缓存的 slab 被组织了三个不同的链表——slab_full，slab_partial和 slab_free，这三个链表有何不同应该可以见名知意，就不赘述了。 然后，你可能就会发现了，在对象那一点，很可能出现一种情况，那就是 slab 的大小跟 object 的大小不整除，也就是说有不足于一个 object 的大小的空间剩余，怎么办，浪费掉吗？肯定不是！内核很好地利用了这些剩余的空间，提出了“缓存染色(cache coloring)”的概念。当然，这里的染色不是真的去染成红绿蓝等颜色，这只是一种说法而已。具体地将，就是让每个缓存的 slab 在页的起始位置有不同的偏移，以缓解缓存过热的问题。注意，这里提到的“缓存过热”中的“缓存”是真的 CPU 的物理缓存。具体的我后面会详细说明，这里只是综述一下。 初始化—— kmem_cache_init()想要让 slab 分配器工作起来，必须进行一系列的初始化。不过这里存在一个“鸡生蛋蛋生鸡”的问题。我们前面说过，每个缓存需要一个管理结构头，而建立缓存的实质就是分配一个管理结构头 struct kmem_cache 来描述 slab 的布局，以指导后续的分配行为，这个建立缓存的过程是用函数 kmem_cache_create() 来实现的，这里这是点一下，下面会详细说明。很明显这个管理结构头的大小小于一页，那么是十分适合使用 slab 分配器进行分配的，但是问题是此时 slab 分配器还没有初始化完成，怎么办？内核的做法是直接静态分配一个 struct kmem_cache 类型的变量——cache_cache（不得不说，这个变量名起得真好！），然后呢，整个初始化的过程分为六步： 初始化 struct kmem_cache 变量 cache_cache 。该变量之所以重要，是因为它是以后所有的对象的管理结构头的管理结构头，专业一点的话可以称作是“元管理结构头”，然后上面提到的 array_cache 和三个链表 kmem_list3 都是这个结构体里面的成员，初始化也都是静态分配的，对应的静态变量分别是 initarray_cache 和 initkmem_list3 。 建立 kmalloc() 的 struct array_cache 对应大小的管理结构头。为什么要进行这一步呢，因为下面的步骤是要完整地建立其 kmalloc() 支持的所有 2^n 大小的管理结构头，完成这一步就相当于 kmalloc() 完全可用了。但是每个管理结构头都必须要 struct array_cache 和 struct kmem_list3 这两个辅助管理结构头，就必须要建立这两个对应大小的 kmalloc() 的管理结构头以便能够使用 kmalloc() 动态分配！这里之所以要提到动态分配是因为在给 struct array_cache 对应大小建立 kmalloc() 的管理结构头的时候，其自己的 struct array_cache 和 struct kmem_list3 也是静态分配的，所以后续将会把它们使用动态分配的空间替换掉。 建立 kmalloc() 的 struct kmem_list3 对应大小的管理结构头以及剩下的 2^n 对应大小的管理结构头。第二点已经说明过了，就不重复了。 （其实包括第五步）此时 kmalloc() 已经可用了，所以如上面所说要使用 kmalloc() 分配的动态内存去替代前面所有的静态内存，需要替换的对象有：cache_cache 的 struct array_cache ，struct array_cache 对应大小的 kmalloc() 的管理结构头的 struct array_cache 和 struct kmem_list3，以及 struct kmem_list3 对应大小的 kmalloc() 的管理结构头的 struct kmem_list3 。 重新调整各个管理结构体的 struct array_cache 中的 entry[] 的数目。 然后这里有一个问题就是，内核是如何知道当前是属于哪一个阶段呢？为了解决这个问题，内核使用了一个枚举变量 g_cpucache_up ，其接受的变量范围有：NULL , PARTIAL_AC , PARTIAL_L3，PEARLY , FULL 。当第二步完成的时候，标记为 PARTIAL_AC ，此时意味着以后的 struct array_cache 都可以使用 kmalloc() 分配了；当第三步完成的时候，标记为 PARTIAL_L3 ，此时意味着以后的 struct kmem_l3 都可以使用；当第四和第五步完成的时候，标记为 PARTIAL_EARLY ，此时意味着 kmalloc() 已经支持所有其支持的 2^n 大小的内存分配了。 然后有一个很重要的点就是，如果你读过 mm/slab.c 的源代码，你就会发现，在建立 struct array_cache 的管理结构头和为 kmalloc() 各个管理结构头的 struct array_cache 的时候，*内核使用的是 struct arraycache_init 而不是 struct array_cache *，这究竟是怎么一回事？其实这是很有讲究的，且听我慢慢到来。 其实 struct initarray_cache 的完整结构是这样的： 1234struct arraycache_init &#123; struct array_cache cache; void *entry[BOOT_CPUCACHE_ENTRIES];&#125;; 从上面的结构可以看出，struct arraycache_init 就只是比 struct array_cache 多了一个 void * 的数组而已，这个究竟有什么区别呢？诶，别着急，我们再来看一下 struct array_cache 的结构就清楚了： 12345678struct array_cache &#123; unsigned int avail; unsighed int limit; unsigned int batchcount; unsigned int touched; spinlock_t lock; void *entry[];&#125;; 联系上文我提到 Per-CPU 的时候说过，为了加快速度将 slab 的一部分内存分配到 array_cache ，而事实上就上面我们看到的 struct array_cache 的结构，只是有一个 void * 的数组而已，而且是个伪数组，并没有数组项。其实细想这是一种十分优美的实现方法。因为各个管理结构头所需要的“一部分内存”是不一样的，这样就保证了一个通用性，每次要访问 array_cache 里面的内存的时候，只需要进行 array_cache-&gt;entry[下标] 就可以了。然后我们再来解决上面提到的 struct arraycache_init 的问题。内核静态分配了这样的一个 struct arraycache_init 的静态变量： 12static struct arraycache_init initarray_cache __initdata = &#123; &#123;0, BOOT_CPUCACHE_ENTRIES, 1, 0&#125; &#125;; 对照结构体的成员我们发现，struct array_cache 的 batchcount 被被赋值为 BOOT_CPUCACHE_ENTRIES ，这个 batchcount 是什么来头呢？这个变量就是控制着我们上面提到的“一部分内存”的具体量了。更重要的是，我们可以看到，struct arraycache_init 多出来的那个 void * 数组的个数，就是 BOOT_CPUCACHE_ENTRIES 。也就是说，struct array_cache 的管理结构头的 struct array_cache 里面的 entry 的真正空间就在这里了。 提到这里的话，那么初始化的第六步就可以彻底地理解了：为每个 kmalloc() 的管理结构头的 array_cache 重新调整 entry 的个数，具体的函数调用是 enable_cpucache()-&gt;do_tune_cpucache()-&gt;alloc_arraycache() 。 创建管理结构头—— kmem_cache_create()因为在初始化的过程中已经静态分配了管理结构头的管理结构头—— cache_cache，所以可以直接使用 kmem_cache_alloc() 给提供的对象建立其自己的管理结构头 struct kmem_cache ，而这个函数的作用也是如此。除了如此外，这个函数还有一个十分重要的功能就是创建 slab 的布局，即—— 应该占用页帧的阶数，slab 管理头应该在页内还是页外，剩余空间是多少，染色的个数，染色的大小（这两个说法在这里可能有点奇怪，不过等我们提到“染色”的时候就清楚了）等，具体的函数调用是 calculate_slab_order()-&gt;cache_estimate() ，过程比较简单，就不提了。 最后调用 enable_cpu_cache() 配置 struct array_cache 和 struct kmem_list3 ，这一步类似于我们在初始化那一节提到的第六步，也就不在重复了。 最后建立的管理结构头加入到 cache_chain 这个链表。 分配内存—— kmem_cache_alloc()kmem_cache_alloc() 这个函数进过多层的封装，最终调用的是 ____cache_alloc() ，kmalloc() 也是如此。我们先来看一下这个函数： 1234567891011121314151617181920static inline void * ____cache_alloc(struct kmem_cache *cachep, gfp_t flags)&#123; void *objp; struct array_cache *ac; check_irq_off(); ac = cpu_cache_get(cachep); if (likely(ac-&gt;avail)) &#123; STATS_INC_ALLOCHIT(cachep); ac-&gt;touched = 1; objp = ac-&gt;entry[--ac-&gt;avail]; &#125; else &#123; STATS_INC_ALLOCMISS(cachep); objp = cache_alloc_refill(cachep, flags); &#125; kmemleak_erase(&amp;ac-&gt;entry[ac-&gt;avail]); return objp;&#125; 这里首先涉及到我当初一直困惑的一个点：在使用 kmem_cache_create() 建立相关的管理结构之后，究竟有没有分配真正的内存空间呢？在通读了相关的代码之后，我得到了答案：没有，也没必要。因为存在这么一种情况：如果使用 kmem_cache_create() 之后还分配了真正的内存空间之后，如果该 slab 一直不使用，那么岂不是浪费了很多宝贵的内存了吗？ 解决上面的这一个疑惑之后，我们就可以知道在上面的代码中，第一次我们是走 else 那个分支了，也就是调用了 cache_alloc_refill() 来分配空间： 1234567891011121314151617static void *cache_alloc_refill(struct kmem_cache *cachep, gfp_t flags)&#123; ...retry: ... while (batchcount &gt; 0) &#123; struct list_head *entry; struct slab *slabp; entry = l3-&gt;slabs_partial.next; if (entry == &amp;l3-&gt;slabs_partial) &#123; l3-&gt;free_touched = 1; entry = l3-&gt;slabs_free.next; if (entry == &amp;l3-&gt;slabs_free) goto must_grow; &#125; &#125; 这里我再插一句：在 Linux 内核中的链表的实现是十分简洁优美的，它只有两个指针，并不存在数据域。这么做是为了通用性，即任何结构都可以组织自己的链表，然后在结构体中嵌入 struct list_head 即可。然后可能有人会问了，如果我有一个 struct list_head ，那么如何才能访问到该链表的起始结构呢？内核十分贴心的给我们准备了一个宏：container_of： 1234#define container_of(ptr, type, member) (&#123; \ const typeof((type *)0-&gt;member)*__mptr = (ptr); \ (type *)((char *)__mptr - offsetof(type, member)) \ &#125;) 简单来说，这个宏的作用就是：我有一个类型为 (type *)-&gt;member 的变量，想要得到包含该变量的 type 类型变量的地址。 所以在这里，如前面所说，这里 l3 的三个链表都是连接 struct slab 类型的，而这个类型就是内存空间的真正所在，也就是我们前面所说的 2^order 个物理页组成的 slab 。 在 kmem_cache_create() 的过程中，在分配 struct kmem_list3 的时候调用了 kmem_list3_init() (line 3844) 将 l3 的三个链表全都置为首尾相连的空链表，所以上面的函数在初次运行的时候最终将跳转到 must_grow 这个标签： 12345678910111213141516must_grow: l3-&gt;free_objects -= ac-&gt;avail;alloc_done: spin_unlock(&amp;l3-&gt;list_lock); if (unlikely(!ac-&gt;avail)) &#123; int x; x = cache_grow(cachep, flag | THISNODE, node, NULL); ac = cpu_cache_get(cachep); if (!x &amp;&amp; ac-&gt;avail) return NULL; if (!ac-&gt;avail) goto retry; &#125; 很明显，函数将调用 cache_grow() 来进行真正的内存分配： 123456789101112static int cache_grow(struct kmem_cache *cachep, gfp_t flags, int nodeid, void *objp) &#123; ... offset = l3-&gt;colour_next; l3-&gt;colour_next++; if (l3-&gt;colour_next &gt;= cachep-&gt;colour) l3-&gt;colour = 0; offset *= cachep-&gt;colur_off; ...&#125; 诶，到这里，总算可以说到前面铺垫了很久的所谓的“缓存着色”了，不过我觉得我得先简单地提一下 CPU cache 的工作原理才行： CPU cache 就是我们常看到的一级缓存，二级缓存，三级缓存啊，引入这些缓存是因为内存 RAM 的速度相较于 CPU 的速度而言，是在是太慢了，所以为了提高速度，所以 CPU 制造商提供了速度接近于 CPU 的小容量缓存，以便加速 CPU 与 RAM 的数据交换。具体的策略是： 每块 cache 会被分为更小的 cache line ，每个 cache line 的容量是一样的，然后 CPU 将虚拟地址分成三部分—— data, index, tag ，其中 data 长度是 cache line 的长度，index 的长度是 cache 的长度减去 data 的长度，最后 tag 的长度是虚拟地址的长度减去 data+index 的长度。举个例子，在 x86 的机器中，虚拟地址的地址空间是 32bit=2^32 ，假设我们的一级缓存有 4MB=2^22，cache line 的长度是 64bit=2^6 ，所以，data 就是 6 位，index 就是 (22-6) = 12 位，tag 就是 32-22=10 位。然后得到这些位之后，CPU 的每一个虚拟地址，将其分成上面的三部分之后，将按照 index 作为索引存入到 cache 中，然后在看需要的内容是否在 cache 中，这回比较需要的虚拟地址的 tag 与 cache 对应索引 index 的 tag 是否一致，如果一致说明 cache hit ，否则说明 cache miss 。 有点啰嗦，不过这些知识准备是必须的，然后我们就可以来具体阐述了。 我们在前面提到，slab 利用剩余的不足一个 object 的空间来进行缓存染色。具体说来，就是以平台的 cache line 的长度（存储在 cachep-&gt;colour_off）为偏移值（这一点非常重要！），计算出剩余的空间有多少个偏移值 cachep-&gt;colour ，然后就从 0 到 cachep-&gt;colour - 1（这个值是 l3-&gt;colour_next），每次就偏移 colour_next * colour_off 。这样，根据我们上面的叙述，每个 slab 将最终被放到不同的 cache line ，从而缓解了缓存过热的问题。 不过，如果你细心一点的话，你也可以发现，其实这个方法并不是特别的有效，因为它的有效范围只有 colour 个，也就是说，colour 个之后，还是会发生覆盖的问题，所以我在上面才用了缓解一词。 总之，上面的代码就计算了下一个偏移值 offset ，那么真正的偏移在那里呢？请看后面的代码： 1234567if (!objp) objp = kmem_getpages(cachep, local_flags, nodeid);if (!objp) goto failed;slabp = alloc_slabmgmt(cachep, objp, offset, local_flags &amp; ~GFP_CONSTRAINT_MASK, nodeid); 真正的页分配就在这里了—— kmem_getpages() ，而真正的偏移就在 alloc_slabmgmt() 这个函数： 1234567891011121314151617static struct slab *alloc_slabmgmt(struct kmem_cache *cachep, void *objp, int colour_off, gfp_t local_flags, int nodeid)&#123; struct slab *slabp; if (OFF_SLAB(cachep)) &#123; ... &#125; else &#123; slabp = objp + colour_off; colour_off += cachep-&gt;slab_size; &#125; slabp-&gt;inuse = 0; slabp-&gt;colouroff = colour_off; slabp-&gt;s_mem = objp + colour_off; ...&#125; 诶，这里就可以很明显的看出来了整个 slab 的布局了：在连续页 objp 的起始，先是 colour_off 的偏移，然后是 slab 的一些管理头（管理头的大小是 cachep-&gt;slab_size），最后就是 object 的起始地址 slab-&gt;s_mem 了。 接下来的具体工作就是设置 slab 的 bufctl 控制数组，这里简单地提一下：slab 控制头有一个成员是 slabp-&gt;free ，意义是当前可用的 object 的索引，而 bufctl 控制的则是当前可用的下一个 object 的索引。 然后，新建立的 slab 加入到 l3 的 slabs_free 链表（这很重要！）。然后 cache_grow() 函数结束，返回 cache_alloc_refill() ，注意，此时我们只是分配了一个新的 slab ，还没有分配出去。具体就是在 goto retry 重新回到 cache_alloc_refill() 那里重新分配，因为此时我们的 slabs_free 已经不是空的了，所以函数接下来将 batchcount 个 object 移到 array_cache 中，然后修改 slab 的 bufctl 数组。最后，看 slab 是否所有 object 都分配完了，如果是，则移到 l3-&gt;slabs_full，否则则移到 l3-&gt;slabs_partial 。 然后，在后续的操作中，如果 array_cache 中有空间，则从其直接分配，否则就看 slabs_partial 或者是 slabs_free 是否有足够的 object ，在不然，就再次重复上面分配 slab 的操作了。 释放内存—— kmem_cache_free()有了上面已经十分详细的阐述之后，释放内存和后面的销毁就显得简单许多了，就是从 array_cache 移回 slab 并且修改 bufctl 控制数组而已。就不赘述了。 销毁内存—— kmem_cache_destroy()同样不赘述了。 好了，整个 slab 分配器我大概就简单地说到这里。下面我说一下我自己的看法。我们可以发现，slab 为了加快分配速度，使用了很多的管理结构，其中花销最大的就是那个 bufctl 数组，所以如果是分配小的 object 的话，那么这个 bufctl 数组占用的空间还是相当可观的。这也是它的一个主要的缺点。而后来的 slub ，就是针对这个缺点进行了有效的改进，而这，我在后面的博客中，将会详细讲解。 EOF]]></content>
      <categories>
        <category>Linux kernel</category>
      </categories>
      <tags>
        <tag>Linux kernel</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[显示 Makefile 中的变量名]]></title>
    <url>%2F2015%2F10%2F31%2Fshow-variable-in-the-makefile%2F</url>
    <content type="text"><![CDATA[最近在编译内核的时候出现了一些错误，最后定位在了某个点，不过出现了很多的变量名不知道，这样很难找出问题，所以我在 Makefile 里面加入了一个可以打印变量值的函数，如下：12print-%: @echo $* = $($*)这样，每一次只要使用 make print-你要打印的变量 就可以打印出其变量名了，还是挺有帮助的～ EOF]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Linux kernel</tag>
        <tag>Makefile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Kernel 内存管理]]></title>
    <url>%2F2015%2F10%2F14%2Flinux-kernel-memory-management%2F</url>
    <content type="text"><![CDATA[自学操作系统一年多以来，遇到的最大的坎就是内存管理。最近捧着《深入Linux内核架构》还有《Understand the Linux Kernel》这两本大块头的书死命地吭，然后联系之前的已学的内容以及模糊的地方，总算是有点清晰的感觉了，赶紧记下来。各种地址内存管理一开始让人很困惑的一点就是有很多以“地址”结尾的术语，让人有点摸不着北，所以我想根据自己的理解总结一下让人困惑的“各种地址”： 逻辑地址（Logical Address）为了保证向后的兼容性，Intel 根据 8086 在 16bit 时代的寻址模式——段加偏移，设计出了分段模式，所有的段地址都保存在了 GDT 里（当然还有 LDT 和 IDT 等，不再详述），偏移地址则一般在通用寄存器，在开启了保护模式之后，由于所有的通用寄存器的字长都是 32 bit，所以可以寻址的地址区间增加到了 4G 。这里的段地址是 16bit，偏移地址是 32bit，所以最终这两个地址共 48bit 组成了逻辑地址。 虚拟地址/线性地址 (Virtual Address/Linear Address)看了很多资料，既有说这两者是一样的，又有说这两者有些许的区别。不管，我个人是倾向于前者的。要理解这两个概念，我们得先来理解另外一个有点类似的概念：地址空间和线性地址空间(Address space/Linear Address)。关于这个概念，CSAPP 书中的解释十分的精彩，这里让我来重新表述一下： 地址空间，就是一个集合，集合的成员是说能够寻址的地址。如果成员是呈线性增长的，那么就成为线性地址空间。 有了这个定义之后，我们以后遇到类似的说法就不怕不怕啦。例如后文会出现的内核线性地址空间(Kernel linear address space) 我们就可以清楚地知道它代表的就是内核说能够寻址的范围。 在理解了上面的两个重要概念之后，虚拟地址与线性地址的含义也应该很清楚了。一般而言，在 32bit 的平台里，虚拟地址/线性地址也是 32bit 长的，同样的 64bit 的也是如此。 然后再说一下，上面的逻辑地址的两个部分直接相加起来（没错，跟 8086 的先偏移在相加是不一样的）就是虚拟地址/线性地址。 物理地址（Physical Address）物理地址就是机器本身 RAM 的真实地址。也是我们寻址的最终意图。这里有一点要提一下，物理地址空间跟线性地址空间是没有关心的，如果你的机器是 32bit 的，那么无论你的 RAM 是多大，线性地址空间都是 0 ～ 0xffffffff 。 分段与分页这两种模式其实都是为了满足日益增长的内存容量而提出的寻址模式。不过现在分页模式已经可以很好地满足需要了，但是为了保证兼容性，IA 平台还是保留了分段机制。不过，为了不与分页机制相冲突，Intel 提出了平坦内存模式（flat mode），具体做法是将段地址全部置为零。不过这里有一个问题，为了保证内核线性地址都是在 3GB 以上（关于这个我下面会详细说明），在编译链接的时候，所有的符号都加上 3GB 的偏移（具体可以看一下内核的编译脚本 vmlinux.lds.S）。但是这就出现问题了：在开启分页模式之前，线性地址 = 物理地址，即段地址 + 偏移地址 = 物理地址，但是我们提到，在平坦模式下，段地址都是 0 ，所以这里偏移地址都是物理地址，从而导致了最终的物理地址都大了 3GB ，所以内核在未开启分页模式前，为了保证正确寻址，使用了一个宏 pa 来减去这多出来的 3GB。 然后，开启了分页模式之后，便可以放心的使用虚拟地址了。在分页机制中使用页表来管理虚拟地址，一般页表大小为 4KB （也有 4MB 的大页表，不过这里不讨论），这样问题就来了，如果 RAM 有 4GB 的话，那么将需要一百万个页表，这样就造成了巨大的空间浪费。所以现行采用的是多级页表的方式。在 32bit 一般是 2 个页表，一个称作页目录（Page Directory），一个称作页表（Page Table Entry）；64 bit则是 4 个，其中三个页目录（Page Global Directory，简称 PGD，Page Upper Directory/PUD，Page Middle Directory/PMD），一个页表。多级页表和单个页表其实可以用乘法和加法来类比，前者相当于 1024x1024，后者则是 1048676 个 1 相加，这样前者只需 1024x2 = 2048 个页表，后者则需要 1048576 个页表！ 然后，一个虚拟地址就被分割成 n + 1 段了，n 是页表的个数，最后的那个 1 是偏移（offset ）。这样，各个页表的内容都指向了下一个页表的地址，当然，页表项指向的是物理页。哦，前面忘记说了，物理页我们成为页帧（frame ）。]]></content>
      <categories>
        <category>Linux kernel</category>
      </categories>
      <tags>
        <tag>Linux kernel</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装操作系统的最后一根救命稻草]]></title>
    <url>%2F2015%2F09%2F21%2Fnew-post%2F</url>
    <content type="text"><![CDATA[不知道你是否有这样的经历，一台老电脑，装有 XP ，然后想要换个 Linux 系统，但是情况是：USB 无法启动，没有光驱，无法没有网络，这时该如何解决这个问题呢？这个时候就该祭出 Grub 大法了。不过，我写这一篇博客，无意于介绍各种安装操作系统的细节，只是想记录一下 Grub 的一些神奇的用法，仅此而已。 下面开始。 为了方便，我使用的是 Neo grub ，不过好像说 grub 2有更加诱人的特性，以后将会深入的研究。 下面是具体的方法： 将下载好的 Linux 镜像复制到某一个盘的根目录下，然后解压到其他的位置，将里面的 casper 目录的 initrd.lz 还有 vmlinuz （注意，在最新的 Ubuntu 系统这个文件被命名为 vmlinuz.efi ，去掉拓展名即可）一并复制到刚才的根目录下。 接下来修改 Neo grub 自动生成的 menu.lst ，进行修改： 12345root (hda,b) //这里有两个注意点，第一是，那个 a 和 b 要根据具体情况，第二是，**逗号和b之间是没有空格的！！！**kernel (hda,b)/vmlinuz boot=casper iso-scan/filename=/你的镜像名 ro quiet splash nomodeset（注意，如果你使用的是 Nvidia 的显卡的话那么一定要加这一句，不然百分之百会花屏的！） locale=zh_CN.UTF-8initrd (hda,0)/initrd.lz 这样修改之后，重启，选择相应的启动项即可～ 最近被 Grub 这个神器迷住了，以后会深入地学习它的！ EOF]]></content>
      <categories>
        <category>日常</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[让 Nvidia 显卡驱动在 Linux 4.2 不再出错]]></title>
    <url>%2F2015%2F09%2F14%2Flet-nvidia-driver-run-in-kernel-42%2F</url>
    <content type="text"><![CDATA[最近自己编译了一下最新的 Linux kernel 4.2 ，一切都还算顺利，不过出现了一个很大的问题就是，在编译内核最后一步：1make install老是 Nvidia 驱动出错，最后不得已删掉 N 的驱动，终于编译安装成功。不过，之后我尝试着重新安装 N 卡的驱动的时候，依然报错。一直无解，无奈求助 Google , 发现 Geforce 论坛上面有人也提到了这样的问题，而且给出了临时的 workaround ，方法十分的 tricky： 只要将 你的内核路径/kernel/workqueue.c 里面的： 1EXPORT_SYMBOL_GPL(flush_workqueue); 改成： 1EXPORT_SYMBOL(flush_workqueue); 然后重新编译一下，重启之后，发现驱动就正常运行了。 这真的是太神奇了。 不过感觉要理解这一句改动所带来的变化还是着实不易，不过，这个事件可以成为一个突破口，让我知道接下来要学什么东西了～ 不断地出现问题然后解决问题真的是学习的不二法宝！ EOF]]></content>
      <categories>
        <category>Linux kernel</category>
      </categories>
      <tags>
        <tag>Linux kernel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初试 Linux 内核编译]]></title>
    <url>%2F2015%2F09%2F12%2Fhow-to-compile-a-linux-kernel%2F</url>
    <content type="text"><![CDATA[这两天在试着编译一下 Linux 内核，过程十分的刺激，随手记下来： 首先去 官方网站 下载最新版的 Linux 内核，然后解压，进入到内核目录里 下面是整个过程中最复杂的一步了：配置内核。想要让内核依照你的电脑配置发挥出最大的功效，那么就必须为为你的电脑量身定做一套配置。然而，看那配置列表一万多个配置选项真的是会被吓到的。总不能一个一个选吧，而且大部分选项根本就看不懂。所以我使用了最保险的做法，去 Ubuntu 官网下载一个配置文件，保存在内核顶层目录，重命名为 .config ，然后使用： 1make oldconfig 来选择那些配置文件中没有的选项。这样，一份 “勉强能用” 的配置文件总算是有了。 接下来的事情就简单多了，先是使用 1make 进行编译工作。我的电脑比较老，所以整个过程持续了三个多小时。。。然后，安装模块： 1make modules_install 最后，完成最后的安装，使用： 1make install 当然我知道这种方式安装内核太自动化了，无法了解到其内部的整个工作流程，学不到什么东西的。不过，我现在的目标是，先让内核能够跑起来，然后在接下去深入！ 好了差不多就是这样了，将来等学习深入了，再写一篇详细原理的。 EOF]]></content>
      <categories>
        <category>Linux kernel</category>
      </categories>
      <tags>
        <tag>Linux kernel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式 \b 和 \B 辨析]]></title>
    <url>%2F2015%2F09%2F08%2Fhow-to-understand-b-and-b-in-regular-expression%2F</url>
    <content type="text"><![CDATA[我想很多初学正则表达式的人都会跟我一样，一开始在接触到 \b 的时候，由于一些例子错误引导，可能会认为跟正则表达式的另一个元字符 ^ 是一样的，都是匹配单词开头的，其实不然，不信，我们来举个例子。如果正文的内容是这样的： -simowce go: Hello, steve jobs-steve jobs: Hey, simowce. Hope you can change the world 接下来我们要匹配句子开头的 -simowce ，我们使用 grep 来进行匹配，输入如下的命令： grep &quot;\b-simowce&quot;结果是什么？你可能会很惊奇，什么都没有输出！这说明了 \b 和 ^ 是有根本性的区别的，下面我来详解一下： \b 里面 b 代表着 word boundry ，那么什么可以被成为 word boundry 呢，有以下三种： 字符串的首字符，如果该首字符属于 word character 字符串的尾字符，如果该尾字符属于 word character 字符串的中间两个字符，其中一个属于 word character，另一个不属于 word character 说明一下，一般而言 word character 就相当于 \w ，即 [0-9A-Za-z_] ，不过似乎有一些还有拓展。 还有，\b 匹配的是位置，所以它是不占用长度的。这一点很重要！ 有了这一些说明了之后，我们就 可以解释上面的奇异现象了！ 关键点就在于，- 不属于 word character 所以即使 - 是字符串的首字符，\b 还是会把它跳掉。 至于 \B，就是与 \b 相反，即 \b 不能匹配的地方 \B 就能匹配。 我们来看一个例子，正文的内容如下： Please enter the nine-digit id as itappears on your color - code pass-key 然后我们的命令是： grep &quot;\B-\B&quot;你可以猜一下结果是什么，没错，只有color - code 中的那个 - 匹配到了。 原因就是，当正则表达式引擎匹配到 color - code 的那个 - 的时候，此时 - 前面是一个空格，属于 word character ，而 - 不属于 word character ，所以符合了 \B 的要求，又因为 \B 不占用长度，所以此时是用 color - code 去匹配 \B 后面的 - 的 ，然后后面的那一个 \B 一样的道理，就不赘述了。 至此，说明完毕！ EOF]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 0.12 备忘录 -- 第八章]]></title>
    <url>%2F2015%2F07%2F25%2Flinux-kernel-012-reminder-sys%2F</url>
    <content type="text"><![CDATA[比较仔细地看完了这一部分的内容，收获还是蛮大的，知道了很多不知道的知识，理解了很多已经知道的知识，例如：为什么 fork 子进程的时候会有多个输出？ 信号究竟是什么？ 等等。下面我主要说一说几个方面：系统调用 system_call，signal 机制以及其原理，进程调度函数 schedule， 退出函数 exit，等待子进程结束函数 wait_pid ， 增加子进程函数 fork ，这一些在平常的系统编程里面都是非常常用的东西，理解其实现过程能够更加清楚地明白自己究竟在干什么～ system_call首先要说明一点，系统调用其实就是通过 int 0x80 中断实现的，eax 存储的是功能号。而且，我们用户进程是以特权级 3 中运行，int 0x80 中断的特权级是 0 ，所以调用会产生特权级转换，堆栈会切换，并且将当前的 ss esp eflags cs eip 等寄存器的值依次压入进程的内核栈 （关于内核栈和用户栈的区别我后边会说到 :-) ）。然后就是将所有通用寄存器都压入栈中，设置 ds 和 es 分别指向内核段，fs 指向本地段，然后调用相关的系统调用函数，完成后将返回值 （放在 eax 寄存器）也压入栈。接下来 line 101 ～ line 106 由于还没有读到相关方面的内容，所以暂时不理解，先留个坑。 ** 接下来主要就是处理进程的信号了。这里处理的是进程信号位图已被标记并且没有被屏蔽的信号。找到第一个符合条件的信号，调用 do_signal 进行 处理。这里要说明一下的是，在获取了信号位图号之后（放在 ecx），之所以要 +1 ，是因为在 Linux 实际的信号值定义都是从 1 开始的，信号位图（就是一个 int 变量）是从 0 开始的。之后，判断是否需要继续处理信号还是结束系统调用。具体我们放到下面具体讲，然后恢复之前保存的寄存器的值，恢复。注意，最后的一句 iret 实际上不是回到调用系统调用的进程，而是进行了一些有趣的事。** 具体我们下面讲。 从上面的描述中我们可以了解到一些什么呢？那就是系统是在什么时候处理信号的。答案就是在进行系统调用，或者是在进程切换（_timer_interrupt 最后会跳转到 ret_from_sys_call ）的时候。 signal通过这个部分，我们可以了解到很多原理的东西，以及之前的很多问号：“ ×× 为什么是这样，为什么要这样？” 为了说明这一点，我将通过我们之前在 signal 编程一些说法或者是一些限制要求来进行详细说明： 1. 使用 signal 函数或者 sigaction 函数进行信号处理句柄的修改，不过，signal 函数的修改是一次性的，sigaction 函数是永久的 首先说明一下，信号处理句柄，或者说 handler 的修改，其实是系统进程结构体 struct task_struct 里面的一个成员 sigaction ，这个成员是一个 struct sigaction 数组，数组总共有 32 项，代表了 32 个信号。signal 函数或者是 sigaction 函数就是修改这个数组里面对应项的来修改信号处理句柄的～ 然后，之所以 signal 函数是临时赋值，sigaction 是永久赋值，是因为 signal 在对 sigaction 数组进行赋值的时候，打上了 SA_ONESHOT 的标志（signal.c line 93），而 sigaction 函数没有，在真正处理信号的 do_signal 函数中，如果发现了 SA_ONESHOT 的标志，则将信号处理句柄清空。 2. 信号处理句柄是在什么时候执行的？ 要解决这个问题，我们就必须知道整个信号从发出到执行的全过程，简单地说明下： 用户进程调用 signal 函数或者是 sigaction 函数对相应的信号进行捕获（即处理句柄的赋值，不过 SIGKILL 和 SIGSTOP 不可捕获！！！）；对当前进程发送一个信号，被进程捕获到，加入到其进程结构的信号位图中；当进程调用系统调用或者是或者是发生进程切换的时候，找到第一个接收到且没有被屏蔽的信号值，进入 do_signal 函数进行处理；do_signal 的主要功能是：如果要处理的信号的处理句柄是 SIG_IGN （忽略该信号），直接返回 1 ；如果处理句柄是 SIG_DEF （默认处理） ，则在进行相应的处理之后（其中有一些默认处理需要我们的注意，具体我们放到 wait_pid 函数那里细讲），也返回 1；这两个都会导致在 ret_from_call 继续处理剩余的信号。否则，检查是否有 SA_ONESHOT 的标志（即是否是 signal 函数处理的信号句柄），如果有则清空。接下来，就是最重要的，也是最精彩的部分了。函数在一开始就把原来压入内核中的 eip 另存为 old_eip ，然后把 eip 赋值为相应信号的处理句柄，之所以这么做是为了在 ret_from_sys_call 返回了之后，不是恢复到原来的进程，而是去执行信号处理句柄。然后将保存的用户栈顶指针上移（这里的上移对应这栈顶指针的减少，因为栈是逆增长的～），手动将一些要恢复用户进程的值压入用户栈中，这里重点注意的是压入后用户栈的栈顶元素是一个叫做 sa_restorer 的系统库函数，这个函数在信号处理句柄结束后被调用。就是这一个函数，最终恢复用户进程。然后，do_signal 函数返回 0 ，ret_from_sys_call 结束，恢复之前压入内核栈的通用寄存器的值。注意，最后的 iret 中恢复的 eip 实际上以及被 do_signal 修改成信号处理句柄的地址了。于是，从这里结束后，接着运行的就是信号处理句柄了。再次注意，在执行信号处理句柄的时候已经是用户态了，堆栈也都是用户栈了。句柄结束后，由于 ret 会将用户栈顶的值作为接下来的执行值，而我们在上面说过，用户栈已经被我们手动压入了一些值，而且栈顶元素就是 sa_restorer 。所以，接下来运行的就是 sa_restorer 函数。而这个函数实际上就是将我们手动压入的一些值进行恢复，完完全全地恢复成原来的用户进程，old_eip 就是在这里被恢复的。 至此，比较完整地描述了一遍信号处理的这一个过程。具体请看书中代码！！！ schedule对于这个函数，具体的调度算法细节我就不细说了，因为这个算法本身并不高明，而且在 Linux 内核 2.6 以后这个东东已经被替换成了 CFS 了，这也是我以后要钻研的东东！这里我想把重点放在这个函数最后的 switch_to 这个宏。这也是进程切换的一个重点所在。他的具体内容如下： 1234567891011121314#define switch_to(n) &#123;\struct &#123;long a,b;&#125; __tmp; \__asm__("cmpl %%ecx,_current\n\t" \ "je 1f\n\t" \ "movw %%dx,%1\n\t" \ "xchgl %%ecx,_current\n\t" \ "ljmp %0\n\t" \ "cmpl %%ecx,_last_task_used_math\n\t" \ "jne 1f\n\t" \ "clts\n" \ "1:" \ ::"m" (*&amp;__tmp.a),"m" (*&amp;__tmp.b), \ "d" (_TSS(n)),"c" ((long) task[n])); \&#125; 具体就是将要跳转的进程数组下标（注意进程数组下标和进程号的区别，后面会提到这一点）对应的 TSS 赋给一个临时结构体的成员 b 的低 16 位（其他的无用），然后 ljmp 跳转到这个 TSS 。可别小看这个跳转，其实在背后做了很多的工作，（具体请看书中第四章），例如将当前的通用寄存器等硬环境保存到当前进程的 TSS 中，然后切换到指定的 TSS ，并且将该 TSS 中的保存的内容赋值到相应的位置，实现任务的切换。不过这个任务切换的全过程确实值得说道说道： 时钟中断发生，调用处理该中断的程序 _timer_interrupt 并且最终调用 do_timer 里面的 schedule 函数 ，schedule 函数 在选择了下一个要执行的进程 b 之后，调用 switch_to 宏 ，在执行完 ljmp %0\n\t 这一句之后，将当前进程 a 的硬环境保存在当前进程的 TSS （利用 TR 寄存器，因为此时 TR 寄存器的内容还是进程 a 的）中并且将 b 的 TSS 的内容恢复到相应的位置中，实现进程切换。当下一次使用 switch_to 宏 恢复到原来的进程 a 的时候，在 ljmp %0\n\t 这一句执行完了之后，原来的进程 a 恢复，继续执行的是 ljmp %0\n\t 的下一句。然后必须要记住的是 switch_to 是一个宏，编译的时候是直接展开在原来的位置的，所以不需要 ret ，所以，当 switch_to 执行完毕了之后，schedule 函数 也执行完毕，回到进程 a 在时钟中断发生时接下去的代码继续运行。 这就是这个进程切换的全过程，相当地清晰！ exit这部分的内容的难点在于对于孤儿进程组的理解与处理。下面说说我的理解： 对于孤儿进程组的判断使用的是 is_orphaned_pgrp 这个函数，在这个函数里面，给出了一个不是孤儿进程组的条件： 1((*p)-&gt;p_pptr-&gt;pgrp != pgrp) &amp;&amp; ((*p)-&gt;p_pptr-&gt;session == (*p)-&gt;session) 其实这还是比较好理解，如果该进程组里面的所有进程的父进程都是该进程组的，那说明整个进程组与外界就没有联系了，所以肯定就是孤儿进程组了。 然后，do_exit 这个函数里面有一个比较难以理解的就是：如何判断要退出的进程是该进程组与外界的唯一联系？ 其实只要注意到几个点就可以比较清晰地理解了。首先注意到在 line 281 已经将当前进程的状态改成了 TASK_ZOMBIE 僵死状态了，而在 line 291 还有 line 292 这两句： 123if ((current-&gt;p_pptr-&gt;pgrp != current-&gt;pgrp) &amp;&amp; (curent-&gt;p_pptr-&gt;session == current-&gt;session) &amp;&amp; ..... ) 这两句代码就是在判断当前进程不是孤儿进程。然后接下来调用 is_orphaned_pgrp 函数 来判断当前进程所在的进程的组是否为孤儿进程组。注意，在 is_orphaned_pgrp 这个函数中，如果扫描到的进程的状态是 TASK_ZOMBIE 僵死状态是直接跳过的，而当前进程的状态恰好就是 TASK_ZOMBIE ，所以，如果此时当前进程所在的进程组依然将是孤儿进程组，那么说明当前进程就是与外界唯一的联系。 然后呢，如果判断当前进程所在进程组将成为孤儿进程组，那么将向整个进程组发送 SIGHUP 还有 SIGCONT 这两个信号，具体为什么还值得深究，暂时就理解成是 POSIX 的规定～ 接下来，给当前进程的父进程发送 SIGCHILD 这个信号，告诉父进程我已经退出了。请注意，这一点很重要，这个在后面的 wait_pid 函数中理解的一个关键点。 最后，让 init 进程成为当前进程子进程的新父亲。同时这里出现了第二个判断孤儿进程组的 case 2,。有了上面的基础，理解这一个就比较简单了，我们只需要知道一点就是：在 case 2 中，当前进程相当于 case 1 中当前进程的父进程，即当前进程的某一个子进程是其所在进程组与外界的唯一联系，理解了这一点，就能够很好地理解 case 2 了。 最后的最后，就是连接 init 进程的子进程的双向链表还有当前进程的子进程的双向链表～ waitpid这个函数主要是将当前进程挂起，并且根据 pid 找到符合条件的子进程， fork如果你有过 Linux 系统编程的经验，你就会知道，在使用 fork() 创建新进程的时候，要根据 fork 的返回值来判断是当前进程还是新的子进程。通过这个 fork 函数源码的学习，我们就可以清晰的了解为什么是这样的～ 首先我们得先说一下前面提到过的 进程号和进程结构数组下标的不同： 函数 find_empty_process 就是为新进程取得新进程号，并且返回数组项的，从这个函数我们可以了解到，进程号其实就是一个 int 数，而在 Linux 0.12 中，进程结构数组的大小 —— NR_TASKS —— 其实只有 64 个。所以我们可以得出这样的结论，至少在 Linux 0.12 ，进程号是一个递增的数，而数组号是可以回滚的～ 然后我们来解释 “双返回” ： 首先我们在实际编程的时候一般都是这么写的： 123int pid = fork();.... 这里的 fork 其实是一个系统调用，最终运行的是 copy_process 函数，这个函数就是为我们的新进程准备各种硬环境，其中包括 TSS 的各项赋值，其中就有两句是重点： 1234p-&gt;tss.eip = eip;....p-&gt;tss.eax = 0;.... 所以，新进程的开始就是父进程接下去要运行的语句，即 fork 函数返回值的赋值，而 fork 函数的返回值是放在 eax 寄存器的，所以父进程 fork 函数的返回值是子进程的进程号，而子进程的返回值是 0 ！而双返回的原因也就在这里了～]]></content>
      <categories>
        <category>Linux kernel</category>
      </categories>
      <tags>
        <tag>Linux kernel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Thinking in Data Structure -- 红黑树（思考）]]></title>
    <url>%2F2015%2F06%2F09%2Fthinking-in-data-structure----red-black-treethinking%2F</url>
    <content type="text"><![CDATA[红黑树，是二叉搜索树的一种。跟 AVL树 都属于平衡二叉树。我们将一棵树定义为红黑树当且仅当其满足下面的性质： 树的节点要么是黑的，要么是红的 树的根节点是黑的 树的叶节点是黑的 如果一个节点的颜色是红的，那么它的两个儿子的颜色都是黑的 对于每一个节点，从该节点（不包含该节点本身）到其所有后代叶节点的简单路径，均包含相同数目的黑色节点。该数目称为 黑高。 需要注意一点的是，与普通的树不同，红黑树的叶节点不是出度为零的节点，而是空节点的另一个说法。或者说，如果一个节点的某一个子节点为空，那么该子节点就是一个叶节点。 在上面的 5 个性质中，第 5 点是保证红黑树的平衡性的关键。这里涉及到一些数学上的证明，《算法导论》里面有，我会在别的文章中进行适当的说明，这里先跳过。 然后是红黑树的一系列操作，查询的我就不说了，跟二叉搜索树是一模一样的，这里重点说一下更新操作 – 插入和删除 插入首先，插入的第一步是找到要插入的位置，这一点跟二叉搜索树是一样的。但是接下来就有问题了，红黑树的节点都是有颜色的，那么新的节点的颜色应该是什么呢？然后，插入新节点之后，上面的 5 个性质还能保持吗？ 首先，新节点的颜色我们选择红色，因为从性质 5 我们知道，每一个节点的所有黑高都必须一样，而红节点并不影响黑高，所以选择红色将不会改变性质 5 。更重要的是，一旦某个节点的性质 5 被改变，那么所有包含该节点的性质 5 都将不符合，这样到来的代价将是巨大的。 但是，选择红色也会出现问题，那就是新节点的父节点如果是红色的，那么将违反性质 4 ，怎么办呢？所以，插入之后，我们必须进行调整，而插入的重点也就在这了。我们来看一下 《算法导论》 的伪代码(这是我自己写的，形式采用了类 C 的写法，不过思想是一样的)： 12345678910111213141516171819RB-Insert-FIX(T, x)while x.p.color == RED if x.p.p.left == x.p y = x.p.p.right if y.color == RED //case 1 x.p.color = y.color = BLACK x.p.p.color = RED x = x.p.p else if x = x.p.right //case 2 x = x.p LEFT_ROTATE(T, x) x.p.color = BLACK //case 3 x.p.p.color = RED RIGHT_ROTATE(T, x.p) else //(将上面的 left 和 right 互换即可) T.root.color = BLACK 从上面的伪代码我们可以看到，插入分为三种情形，这里我们将 y 称为 x 的叔节点，所以这三种情况可以这样分： x 的叔节点是红色的 x 的叔节点是黑色的且 x 是父节点的右节点 x 的叔节点是黑色的且 x 是父节点的左节点 值得说明的是，我们在 RB-Insert-FIX 执行的过程中，x.p.color 一直都是红色的，这也是 RB-Insert-FIX 的循环条件，为什么是这样的呢？因为如果 x.p.color 是黑色的，那么根据我们前面提到的，插入的时候性质 5 是一直保持的，而性质 1,2,3 也是一直都保持的，唯一有可能违反的就是性质 4 了。如果 x.p.color 是黑的，那么性质 4 也将符合，那么说明整棵树满足性质 1-5，已经是一棵红黑树了。 然后我们来分析一下这三个 case： case 1: x 的叔节点和父节点都是红的，注意此时并不关心 x 是父节点的左儿子还是右儿子，选择的策略是将 x 的红色上移，具体做法是，将 x 的叔节点和父节点都赋值为黑色，x 的爷爷节点赋值为红色，并将 x 重新指向为 x 的爷爷节点。同时，现在的 x 节点的父节点也有可能是红的，循环将继续 case 2: x 的叔节点是黑的且 x 是父节点的右儿子，这个 case 是为了将其转化为 case 3，然后统一处理。 case 3: x 的叔节点是黑的且 x 是父节点的左儿子，这个 case 跟上面 case 2 的区别是，case 2 中 x，x.p，x.p.p 的排列是弯曲 zig-zag 型的，而 case 3 则是直线 zig-zig 型的。 具体操作就在那里，多看即便就可以理解，必须记住的是无论进行什么操作都是为了恢复被破坏的性质。 删除相比插入而言，删除就难多了，不过也是个人觉得红黑树最出彩的地方了，进过一系列的操作之后，then it works! 确实不得不佩服前人的智慧！ 红黑树的删除跟二叉搜索树的删除是差不多的，都是先找到该点所在的位置，然后根据其儿子的个数进行相应的变换。只不过多了一个恢复红黑性质的操作。 接下来，我想根据我看《算法导论》是遇到的问题以及我是如何解决这些问题的来说明删除这一操作。 首先，我们来看一下删除的伪代码： 123456789101112131415161718192021222324252627RB-Delete(T, z)y = zy-initial-color = y.colorif z.left == T.nil x = z.right RB-TRANSPLANT(T, z, z.right)else if z.right == T.nil x = z.left RB-TRANSPLANT(T, z, z.left)else y = TREE-MINIMUN(z.right) y-initial-color = y.color x = y.right if y.p = z x.p = y else RB-TRANSPLANT(T, y, y.right) y.right = z.right y.right.p = y RB-TRANSPLANT(T, z, y) y.left = z.left y.left.p = y y.color = z.color if y-initial-color == BLACK RB-DELETE-FIX(T, x) 这里要注意的是，x 这个变量的含义是： 如果 y 的某个儿子非空，那么 x 总是指向 y 非空的儿子，并且在 RB-TRANSPLANT 执行之后指向 y 原来的位置。之所以是这样我们可以注意到 RB-TRANSPLANT 这个函数的第三个参数总是跟 x 相等的。否则，y 的两个儿子皆为空，那么 x 指向 y 的右儿子，并且在 RB-TRANSPLANT 执行之后指向 y 原来的位置，原因跟上面的一样。 同时，要注意到 x.p 的赋值，当 y.p != z 时，x.p 总是指向 y 的父节点，而且该过程是在 RB-TRANSPLANT 的最后一句实现的。当 y.p == z 时，x.p 将赋给 y。为什么是这样呢？这是因为当 z 有两个儿子的时候且 y.p != z 时，y 就是 z 的后继（跟二叉搜索树是一样的），此时需要进行的操作是：x 代替 y，y 接手 z 的右儿子，y 代替 z，y 接手 z 的左儿子，所以此时 x.p 将指向 y 的父节点。但是，如果 y.p == z 的话，只需要将 y 直接替代 z 并且接手 z 的左儿子，y 的右儿子（即 x）依旧保持不变，所以 x.p 将指向 y 。 还有要注意，这里的删除相当于一种 lasy delete，当 y == z 时，只是将 z 的父节点指向 z 的非空儿子而已；当 y != z 时，它只是将 y 的值赋给 z 的值，z 的颜色将保持不变，这一点很重要！ 最后，注意到调用 RB-DELETE-FIX 的条件是：y-initial-color == BLACK 。为什么是这样呢？因为如果 y 是红色的，删除它并不影响黑高。要阐述这一点，得分情况讨论： y == z. 这种情况是在 z 的儿子数小于 2 的时候。又因为 z 的某一个儿子为空，所以左右黑高都为 1，且由于 z == y 为红节点，所以只可能是左右都为空，此时 x 为叶节点，替代 y 后，黑高仍为 1 。 y != z. 这种情况发生在 z 的儿子树为 2 且 z.right == T.nil 的时候。前面提到，这种情况下的删除只是将 y 的值赋给 z 而已，z 的颜色并没有改变，所以这种情况下相当于在树中删掉一个红节点，而且，因为这种情况下 y 的左儿子必为空（可分为两种情况讨论，y.p == z 或者 y 是 z 的后继），所以黑高必为 1 。删掉该红节点之后，因为 T.nil 的黑高也为 1，所以性质 5 没有违背，不需要修复！ 接下来，就是删除的重点——RB-DELETE-FIX 了，我们先来看一下伪代码： 12345678910111213141516171819202122232425RB-DELETE-FIX(T, x)while x != T.root &amp;&amp; x.color == BLACK if x == x.p.left w = x.p.right if w.color == RED //case 1 w.color = BLACK x.p.color = RED LEFT-ROTATE(T, x.p) w = x.p.right else if w.left.color == BLACK &amp;&amp; w.right.color == BLACK // case 2 w.color = RED x = x.p else if w.right == BLACK // case 3 w.left.color = BLACK w.color = RED RIGHT_ROTATE(T, w) w = x.p.right w.color = x.p.color // case 4 x.p.color = x.right.color = BLACK LEFT_ROTATE(T, x.p) x = T.root else //（将上面的 left 和 right 调换即可）x.color = BLACK 在这个过程中，有几个点要注意一下： RB-DELETE-FIX 的循环条件是：x != T.root &amp;&amp; x.color == BLACK，这里解释一下第二个条件。我们之所以需要这一个函数来修复红黑性，是因为删除掉一个黑节点将会导致部分子树的黑高减少，而 x 代替的是 y 原来的位置，所以如果 x.color == RED 的话，那么只需将 x.color 赋值成 BLACK 即可恢复性质 5 了。 《算法导论》中提到只有 case 2 将会导致循环，我自己试了一下，发现确实是这样的，而且如果红黑树是一个所有节点都为黑色的满二叉树，那么 case 2 将会一直循环到根节点为止。 有一个点要特别注意，就是 case 4 的第一句：w.color = x.p.color 。为什么要这样呢？那是因为 x.p.color 的值不知道，而且这步操作之后是要将原子树变成以 w 为根的新子树，而我们知道原子树的父节点以及其祖宗都是满足所有性质的，所以，如果 w 保持 x.p 原来的颜色而且满足了性质 5 ，那么，这棵树就满足红黑性了！ 最后，还是那一句。上面的所有操作都是为了让子树恢复红黑性，所以，主要是理解为主。虽然，我也为了理解每一步究竟为什么要这要做思考了很多，也有所收获，但是最后发现，这个东西真的只能理解，写是写不出来的，或者说，无法十分严谨地写出来，所以便作罢了。 好了，写得够多了。再一次说明，上面的所有内容都是我自己的思考，可能会有错，不过我会一直维护，希望能够为大家学习红黑树这个有用的数据结构带来一点帮助，那我也就心满意足了～ EOF]]></content>
      <categories>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>红黑树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Thinking in Data Structure -- Binary Tree]]></title>
    <url>%2F2015%2F06%2F01%2Fall-about-binary-tree%2F</url>
    <content type="text"><![CDATA[二叉树是树结构中比较容易实现的一种，有关它的操作也有很多，在接下来的几天，我将陆续更新，先写个目录吧： 已知二叉树的前序和中序，要求重新构造出该二叉树 我们知道，前序遍历是所谓的 “根左右”，中序遍历是所谓的 “左根右”。注意，重点来了，中序遍历的根节点将整棵二叉树分成了左子树和右子树，而前序遍历因为总是先访问根节点，所以可以在前序遍历的第一个节点肯定是根节点，然后在根据这个根节点找到中序遍历的两棵子树，然后以此类推。即可以使用递归。 说到递归，确实是一个神奇的东西，它极大地降低了编码的难度，同时也增大了阅读的难度。不过要写出一个好的递归算法还是需要一定的技巧的，一般我是这么考虑递归的： 考虑递归是否需要返回值 考虑递归结束的条件，这一个非常的重要 考虑子问题，这里要注意的是如果是有返回值的递归要对每一种情况都进行返回的 这样说比较虚，我们具体来看这道题： 既然是要重新构造二叉树，说明是从无到有的，所以要么在递归函数的参数中加入一个节点参数，要么选择有返回值的递归函数，我倾向于第二种，因为如果是引入节点参数的话，必须使用二级指针，（关于二级指针的内容我接下来将会写一篇使用底层汇编来解释二级指针的文章，尽请期待），比较麻烦，而且多加一个参数的话如果递归深度过高将会大大增加暴栈的可能性。 递归结束条件，很明显，如果根节点没有左子树和右子树，那么就可以将该节点直接返回。 子问题，我们可以将树的左子树和右子树看成一棵新的树，然后新的树也将有根节点，左子树和右子树，于是树的长度变小，直至为 1 好了，其实这个问题真的不难，说了这么多只是想把自己的思考过程写出来而已，下面看代码，自己感觉写的还是挺优美的～ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include "binary_tree.h"#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#define N 100b_tree recover(char *preorder, char *inorder, int len)&#123; b_tree node = (b_tree)malloc(sizeof(bin_tree)); int i; int root_pos; node-&gt;data = preorder[0]; node-&gt;lchild = node-&gt;rchild = NULL; if (len == 1) &#123; return node; &#125; for (i = 0; i &lt; len; i++) &#123; if (inorder[i] == preorder[0]) &#123; root_pos = i; break; &#125; &#125; if (root_pos == 0) &#123; node-&gt;rchild = recover(preorder+1, inorder+1, len-1); &#125; else if (root_pos == len-1) &#123; node-&gt;lchild = recover(preorder+1, inorder, len-1); &#125; else &#123; node-&gt;lchild = recover(preorder+1, inorder, root_pos); node-&gt;rchild = recover(preorder+root_pos+1, inorder+root_pos+1, len-root_pos-1); &#125; return node;&#125;int main(void)&#123; b_tree root; char preorder[N] = &#123;0&#125;; char inorder[N] = &#123;0&#125;; int len; printf("Please enter the preorder: "); scanf("%s", preorder); printf("Please enter the inorder: "); scanf("%s", inorder); len = strlen(preorder); root = recover(preorder, inorder, len); preorder_recur(root); putchar(10); inorder_recur(root); putchar(10); return 0;&#125;]]></content>
      <categories>
        <category>Data Structure</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[动态链接库 .so 的编译和使用]]></title>
    <url>%2F2015%2F05%2F31%2Fcompile-code-to-so-library%2F</url>
    <content type="text"><![CDATA[今天写代码的时候，发现需要之前的一些实现，有不想直接将代码复制过来，于是就想把之前实现的代码编译成一个动态链接库，这样显得更加的优雅。几番摸索之后，终有所获，遂记之~首先，我们约定要编译的文件名是 code.c ,编译后的动态库名是 libcode.so (注意，共享库的命名一般都是 libxxx 的)，目标文件是 target.c 。于是，编译命令如下： 1gcc -fPIC -shared libcode.so code.c 这里解释一下： -fPIC: 这个选项告诉编译器，产生与位置无关的代码（Position-Independent-Code）。也就是说，产生的代码中，没有绝对地址，之后相对地址，因此代码可以被加载器加载到内存的任意位置，都可以正确运行。而这正是共享库所需要的，因为共享库在被加载的时候，在内存的位置是不固定。 -shared: 这个选项指定生成动态链接库 然后我们需要将生成的动态库链接到我们的目标文件上，编译命令如下： 1gcc target.c -o target -L ./ -lcode 解释一下： -L: 这一个跟 -I 很像，就是表示要链接的库在参数指定的位置中 -lcode: 用来指定库名，而库名的命名规则是 libxxx 的，所以这个选项只需要后面的部分，即 -lxxx 注意，在编译目标文件的时候可能会出现下面的错误： 1error while loading shared libraries 说明系统还是找不到我们的动态库路径，那么有两种解决方法： 将我们编译的 .so 文件复制到 /usr/lib 或者是 /usr/local/lib 将当前路径加入到 LD_LIBRARY_PATH 变量，使用如下命令： 1export LD_LIBRARY_PATH=./ 至此，所有工作完成，编译成功～]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>动态链接库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Thinking in Data Structure -- AVL Tree]]></title>
    <url>%2F2015%2F05%2F27%2FThinking-in-Data-Structure--AVL%20Tree%2F</url>
    <content type="text"><![CDATA[AVL 树， 是在二叉搜索树的基础上，在插入操作上加入了旋转的操作，从而避免了一棵树退化成一个链表或者树的深度过长导致搜索的时候效率太低的坏处，而旋转也是 AVL 树 中最重要的操作，也是本博文的重点内容。AVL 树 在普通二叉搜索树节点的基础上加入了一个 buf 值，意思是左孩子的高度减去右孩子的高度，而 buf 值的合法范围是 -1， 0， 1，如果不是这几个值，那么说明树不平衡，就要进行旋转。 接下来我们重点说说旋转操作。 首先，要注意的是，在实现中并不是等到节点的 bf 值变为 +2 或者 -2 的时候才进行旋转。相反，当父节点的 bf 为 +1 或者 -1 时，就认为下一次插入的是必将导致不平衡，所以就进行旋转了。 以左失衡为例，分为 LL 和 LR，即在父节点的左子树的左节点插入或者是父节点的左子树的右节点插入。究竟在插入的时候是如何判断这两种情况的，我们来看一张图 从图中我们可以看到，当父节点的左儿子的 bf 是 +1 时，属于 LL 的情况， -1 的时候，属于 LR 的情况。那么继续分析下去， bf 等于 0 的时候是属于那种情况？稍加思考便可发现，这一种情况不存在，因为我们这里讨论的父节点的左儿子的 bf 值是在插入新节点之后导致失衡的前提下，如果左节点的左儿子在插入新节点之后导致失衡而且 bf 为 0，也就是在插入新节点之前以及之后父节点的左子树的高度没有发生变化，那么在插入新节点之后树发生了失衡，那么说明在插入之前树也是失衡的。 有了上面的思考，接下来我们来分析一下旋转之后各节点的 bf 值如何调整，LL 的比较简单，重点来看 LR 的，还是来看一个图： 从图中我们可以看到，LR 分为三种情况，这三种情况分别是通过父节点左子树的右节点或者说是 “孙子节点” 插入后的 bf 值来区分的。 而右失衡的操作，跟左失衡类似，在这里就不再赘述了。 下面是代码的实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155#include "avl.h"#include &lt;stdlib.h&gt;void insert(p_avl *t, int data)&#123; if (*t != NULL) &#123; if (data &lt; (*t)-&gt;data) &#123; insert(&amp;(*t)-&gt;l_child, data); if (unblance) &#123; switch ((*t)-&gt;buf) &#123; case 0: (*t)-&gt;buf = 1; break; case -1: (*t)-&gt;buf = 0; unblance = false; break; case 1: left_rotation(t); break; &#125; &#125; &#125; else if (data &gt; (*t)-&gt;data) &#123; insert(&amp;(*t)-&gt;r_child, data); if (unblance) &#123; switch ((*t)-&gt;buf) &#123; case 0: (*t)-&gt;buf = -1; break; case 1: (*t)-&gt;buf = 0; unblance = false; break; case -1: right_rotation(t); break; &#125; &#125; &#125; else &#123; unblance = false; fprintf(stderr, "The key has already in the tree"); &#125; &#125; else &#123; unblance = true; *t = (p_avl)malloc(sizeof(avl)); if (*t == NULL) &#123; fprintf(stderr, "Out of memory!\n"); exit(1); &#125; (*t)-&gt;data = data; (*t)-&gt;buf = 0; (*t)-&gt;l_child = (*t)-&gt;r_child = NULL; &#125;&#125;void right_rotation(p_avl *parent)&#123; p_avl child; child = (*parent)-&gt;r_child; if (child-&gt;buf == -1) &#123; p_avl child_left; child_left = child-&gt;l_child; (*parent)-&gt;r_child = child_left; child-&gt;l_child = (*parent); (*parent)-&gt;buf = 0; (*parent) = child; &#125; else &#123; p_avl grandchild = child-&gt;l_child; p_avl grandchild_right = grandchild-&gt;r_child; child-&gt;l_child = grandchild_right; grandchild-&gt;r_child = child; (*parent)-&gt;r_child = grandchild-&gt;l_child; grandchild-&gt;l_child = (*parent); switch (grandchild-&gt;buf) &#123; case 0: (*parent)-&gt;buf = child-&gt;buf = 0; break; case 1: (*parent)-&gt;buf = 0; child-&gt;buf = -1; break; case -1: (*parent)-&gt;buf = 1; child-&gt;buf = 0; break; &#125; (*parent) = grandchild; &#125; (*parent)-&gt;buf = 0;&#125;void left_rotation(p_avl *parent)&#123; p_avl child; child = (*parent)-&gt;l_child; if (child-&gt;buf == 1) &#123; p_avl child_right; child_right = child-&gt;r_child; (*parent)-&gt;l_child = child_right; child-&gt;r_child = (*parent); (*parent)-&gt;buf = 0; (*parent) = child; &#125; else &#123; p_avl grandchild = child-&gt;r_child; p_avl grandchild_left = grandchild-&gt;l_child; child-&gt;r_child = grandchild_left; grandchild-&gt;l_child = child; (*parent)-&gt;l_child = grandchild-&gt;r_child; grandchild-&gt;r_child = (*parent); switch (grandchild-&gt;buf) &#123; case 0: child-&gt;buf = (*parent)-&gt;buf = 0; break; case 1: child-&gt;buf = 0; (*parent)-&gt;buf = -1; break; case -1: child-&gt;buf = 1; (*parent)-&gt;buf = 0; break; &#125; (*parent) = grandchild; &#125; (*parent)-&gt;buf = 0;&#125;void pre_order(p_avl t)&#123; if (t) &#123; printf("%d ", t-&gt;data); pre_order(t-&gt;l_child); pre_order(t-&gt;r_child); &#125;&#125;int main(void)&#123; p_avl root = NULL; int key; while ((scanf("%d", &amp;key)) &amp;&amp; key != '#') &#123; insert(&amp;root, key); &#125; pre_order(root);&#125;]]></content>
      <categories>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>AVL Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[其实我根本不懂 C]]></title>
    <url>%2F2015%2F05%2F17%2Fi-dont-know-c%2F</url>
    <content type="text"><![CDATA[char *p 和 char p[] 的区别： 问题的来源是基于这样的一段代码：12345678910111213#include &lt;stdio.h&gt;#include &lt;string.h&gt;int main(void) &#123; char *p = "Hello world"; printf("%s\n", p); strcpy(p, "123"); printf("%s\n", p); return 0;&#125; 编译运行，发现产生了段错误。 相反，将 1char *p = "Hello world"; 改成 1char p[] = "Hello world"; 则不会有任何问题。 为什么会这样呢？我们使用 GDB 来看一下究竟是发生了什么事？ 经过 GDB 调试，我们会发现，当运行到 1strcpy(p, "123"); 这一句的时候，会产生一个错误信息： Program received signal SIGSEGV, Segmentation fault. 关于 Linux 的信号这一方面我还很薄弱，在此不展开。百度之后发现这一个信号是这样的： SIGSEGV：在POSIX兼容的平台上，SIGSEGV是当一个进程执行了一个无效的内存引用，或发生段错误时发送给它的信号。SIGSEGV的符号常量在头文件signal.h中定义。因为在不同平台上，信号数字可能变化，因此符号信号名被使用。通常，它是信号#11。 于是便引出了问题： char *p 和 char p[] 有什么不同？ C/C++ 程序占用的内存分为下面的几个区域： 栈: 函数参数，局部变量都属于该区域 堆：C 的 malloc 和 C++ 的 new 分配的内存都属于这个区域 全局区（静态区）：全局变量和静态变量 文字常数区：字符串常量 程序代码区： 所以我们就可以分析了。char *p = &quot;Hello world&quot; 这一句，&quot;Hello world&quot; 是字符串常量，所以在文字常数区，p 是局部变量，是在栈区。然后我们使用 strcpy 函数用 “123” 去修改一个字符串常量，所以会导致错误。 而 char p[] = &quot;Hello world&quot; 则不同，它是一个数组，内存分配在栈区，不存在不可写的问题，所以一切正常。 写到这，我有想到了一个问题，就是效率的问题，我们来看一下下面的代码: 1234567891011121314#include &lt;stdio.h&gt;int main(void)&#123; char a; char *s1 = "HeHe"; char s2[] = "123456789"; a = s1[1]; a = s2[1]; printf("%p and %p", s1, s2); return 0;&#125; 我们使用 objdump 来查看编译后的汇编代码： 1234567891011121314151617181920212223242526int main(void)&#123; 40059d: 55 push %rbp 40059e: 48 89 e5 mov %rsp,%rbp 4005a1: 48 83 ec 30 sub $0x30,%rsp 4005a5: 64 48 8b 04 25 28 00 mov %fs:0x28,%rax 4005ac: 00 00 4005ae: 48 89 45 f8 mov %rax,-0x8(%rbp) 4005b2: 31 c0 xor %eax,%eax char a; char *s1 = "HeHe"; 4005b4: 48 c7 45 d8 a4 06 40 movq $0x4006a4,-0x28(%rbp) 4005bb: 00 char s2[] = "123456789"; 4005bc: 48 b8 31 32 33 34 35 movabs $0x3837363534333231,%rax 4005c3: 36 37 38 4005c6: 48 89 45 e0 mov %rax,-0x20(%rbp) 4005ca: 66 c7 45 e8 39 00 movw $0x39,-0x18(%rbp) a = s1[1]; 4005d0: 48 8b 45 d8 mov -0x28(%rbp),%rax 4005d4: 0f b6 40 01 movzbl 0x1(%rax),%eax 4005d8: 88 45 d7 mov %al,-0x29(%rbp) a = s2[1]; 4005db: 0f b6 45 e1 movzbl -0x1f(%rbp),%eax 4005df: 88 45 d7 mov %al,-0x29(%rbp) 从上面的代码中我们可以看到，char *s1 只是将 “HeHe” 这个字符串常量的地址压入栈，而 char s2[] 则是将 “123456789” 整个数组的内容都压入了栈（或者说，在栈内分配了空间），于是 s1[1] 的操作就需要三步：将栈中的地址赋给 rax （因为我是 64 位的机器） ,从 rax 中的地址找到字符串的内容并赋给 eax ，再将 eax 的取出一个 char 赋给 a （a 也在栈中）。 而 s2[1] 的操作只需两步，由于内容就在栈中，不需要从别的地方找，所以只需定位栈中位置，赋值即可。 至此，应该算是比较清晰了。 (PS 我花费了将近 1 个小时在上面的汇编代码中，就只是因为对 ASCII 码不熟悉导致不知道 0x3837363534333231 这个东东是什么意思…)]]></content>
      <categories>
        <category>思</category>
      </categories>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[POJ 1102 -- LC-Display]]></title>
    <url>%2F2014%2F12%2F27%2FLCD-display%2F</url>
    <content type="text"><![CDATA[已经很久没有刷题了。前几天在上 EDA 实验课的时候，突然想到了以前一道没有 AC 的题目，并且突然顿悟，想到了可能是正确的解决方法。然后今天着手实现。总算是 1A 。虽然已经退出了 ACM ，不过 AC 的感觉还是跟以前一样。嗯，我有点怀念以前那段岁月了。题目描述题意很简单。就是给你一串数，要你模拟出这串数在 LC 屏幕的输出。不过有一个要求，就是还有一个 size 值。 解决方法常规的按一个数一个数去输出肯定是不行的，因为你无法让光标向上移动，所以，我们需要按层去输出。例如，我是将其分为 5 层。然后，每一层建立一个数组，然后一层一层的输出。这样就可以了。 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include &lt;stdio.h&gt;#include &lt;string.h&gt;#define L 5char row1[] = &#123;'-', ' ', '-', '-', ' ', '-', '-', '-', '-', '-'&#125;;char row2[] = &#123;'|', '|', ' ', '|', ' ', '|', ' ', '|', '|', '|', '|', ' ', '|', ' ', ' ', '|', '|', '|', '|', '|'&#125;;char row3[] = &#123;' ', ' ', '-', '-', '-', '-', '-', ' ', '-', '-'&#125;;char row4[] = &#123;'|', '|', ' ', '|', '|', ' ', ' ', '|', ' ', '|', ' ', '|', '|', '|', ' ', '|', '|', '|', ' ', '|'&#125;;char row5[] = &#123;'-', ' ', '-', '-', ' ', '-', '-', ' ', '-', '-'&#125;;int main(void)&#123; int times; char num[10] = &#123;0&#125;; int i; int j; int k; int m; int n; while (scanf("%d%s", &amp;times, num) &amp;&amp; times != 0) &#123; for (i = 0; i &lt; L; i++) &#123; if (i == 0 || i == 2 || i == 4) &#123; int len = strlen(num); for (j = 0; j &lt; len; j++) &#123; for (k = 0; k &lt; times; k++) &#123; if (k == 0) &#123; putchar(' '); &#125; switch (i) &#123; case 0: printf("%c", row1[num[j] - '0']); break; case 2: printf("%c", row3[num[j] - '0']); break; case 4: printf("%c", row5[num[j] - '0']); break; &#125; if (k == (times - 1)) &#123; putchar(' '); &#125; &#125; printf("%c", (j == len - 1) ? '\n' : ' '); &#125; &#125; else &#123; for (j = 0; j &lt; times; j++) &#123; int len = strlen(num); for (k = 0; k &lt; len; k++) &#123; switch (i) &#123; case 1: printf("%c", row2[(num[k] - '0') * 2 + 0]); break; case 3: printf("%c", row4[(num[k] - '0') * 2 + 0]); break; &#125; for (m = 0; m &lt; times; m++) &#123; putchar(' '); &#125; switch (i) &#123; case 1: printf("%c", row2[(num[k] - '0') * 2 + 1]); break; case 3: printf("%c", row4[(num[k] - '0') * 2 + 1]); break; &#125; printf("%c", (k == (len - 1)) ? '\n' : ' '); &#125; &#125; &#125; &#125; putchar(10); &#125; return 0;&#125; EOF]]></content>
      <categories>
        <category>ACM</category>
      </categories>
      <tags>
        <tag>水题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个操作系统的实现 -- IPC]]></title>
    <url>%2F2014%2F12%2F18%2Fchapter-8-IPC%2F</url>
    <content type="text"><![CDATA[终于，时隔这么多天，总算是把 第八章 完成了。从下篇开始，作者的写作风格大变，上篇基本上是把大部分的代码都弄出来讲，不过下篇的话，作者只拿了一些关键的代码，而且很多地方都做了修改，但是他却没有提到，所以，你只能自己去扒代码。在实现的过程中，出现了超级多的 bug ，debug 到要吐。不过，我不会像以前一样只会抱怨说：“这不科学！” 在这快一个星期的改错过程中，我学到了很多东西。不要总是抱怨改 bug辛苦， bug 都是自己制造出来的，这个就只能怪自己了。最重要的一点，那就是，所有的 bug 都是思维的漏洞。所以，在你的思想体系还没有十分清晰的情况下，不要轻易写代码，不然，出现 bug 是很正常的事情。 好了，闲话不多说了。下面进入正题。 综述这一章有几个比较大的改动。首先，上一章的 write 系统调用改成了 printx ，使之能够对 assert() 和 panic() 这两个函数进行特殊的处理，其他的功能基本一致。然后就是用 IPC 重新实现了 get_ticks() 这个函数。 IPC这个是微内核的核心，它使得我们的系统系统调用可以大大减少。当初在看书的时候也确实花费了相当长的时间在理解这个东西。下面分别来说一下个中细节。 msg_send首先，通讯通讯，肯定是有收有发。我们的进程通过 send_recv 这个函数发送消息，最终就是调用 msg_send 。首先判断是否死锁，之后再判断发送的目的地是否是接收消息的状态(即是否有 RECEIVING 标志)，并且是否是接收来自当前进程的消息或者是任意进程的消息(就是那个 ANY)，如果是，则将消息复制到接收端，清除接收端的 RECEIVING 标志，待接收信息指针指向空；否则，给当前的进程打上 SENDING 标志，待发送的消息赋给消息指针，待接收端赋给 p_sendto (也就是将所有的信息保存在进程体里面。)接下来有做的是就是将当前进程加入到目标进程的消息发送队列了。这里有两种情况，躲过目标进程的消息接收队列为空，那么直接将当前的进程加入到目标进程的 q_sending ，否则的话，就要找到当前消息队列的最后一个(也就是找到那个 next_sending 为 0 的那个)，然后把当前进程赋给队列最后一个的 next_sending 。然后，都要将消息队列的最后一个(也就是当前进程)的 next_sending 赋值为 0 。最后，调用 block() 函数，并最终调用了 schedule() 函数切换进程。注意，这里有一个很重要的点，就是它是如何实现进程切换的。我们知道，我们调用发送消息是通过系统中断来实现，在中断处理程序 sys_call 中的 save() 函数，会根据当前的 k_reenter 的值来分别将不同的东西压栈，而压入的内容，其实就是最终的进程切换程序 restart() ，并且最终在 sys_call 的最后一句 ret 来跳转到这个进程切换程序。 完整的消息发送过程就是这样。 msg_receive接下来就是消息接收了。首先判断当前进程是否接收来自任意进程的消息，如果是，则判断当前接收消息的进程的消息队列是否有在等待发送的进程，如果有的话，就把消息队列的第一个赋给 p_sendto 变量，以便接下来的处理；如果不是接收任意进程的消息，而是有特定的发送对象，那么就检查那个特定对象是否在当前进程的消息发送队列中(也就是检查特定对象是否有 SENDING 标志位并且 p_sendto 指针是否指向当前进程)，如果是，那么就遍历整个消息发送队列，如果是第一个，那最好，直接赋给 p_sendto 变量，否则，找到特定对象以及排在它前面的进程(这个排在前面的进程是有用的，后面会提到)，分别赋给变量 p_sendto 以及 prev 。接下来就是真真正正的消息传递了，如果当前进程接收任意进程的消息或者接收特定进程的消息并且该进程在在消息发送队列的第一个，那么就将该进程从队列中提出来，并且将队列整体前移。否则，为了从夹在消息队列的中间的特定进程从队列中抽出来，并且不影响队列的整体，这时前面的那个 prev 就派上了用场，将 p_sendto 的 p_sendto 指针指向 prev 的 p_sendto 指针，这样就能够实现将其从队列中抽出来的目的了(如果你还是不懂的话，自己画个图就很容易懂了～)。然后就是一些收尾的工作，清除当前进程的 RECEIVING 标志，清空指向消息的指针。最后的最后，如果是最坏的情况，那就是既不接收任意进程的消息，同时特定进程不在消息队列中，那么，就将当前进程打上 RECEIVING 的标志位，将消息等各种信息保存在进程体中，然后阻塞进程。 全过程好了，各种细节解释完了，接下来我们来看一个 IPC 的具体例子：就是用 IPC 来替换系统调用 get_ticks ，接下来，我将详细描述整个过程，以便大家能够有一个比较清晰的理解： 首先，由于 task_sys 系统进程，所以比用户进程先执行。整个进程其实就是一个守护进程，它是一个死循环，不断地接收信息，处理完之后再发送，然后继续如此。首先它先接收任意进程的消息，但是很明显，当前还没有进程给它发送消息，所以 task_sys 被阻塞了。然后进程切换到用户进程， 用户进程调用函数 get_ticks()， get_ticks() 函数初始化一个消息变量，并且将消息类型设置为 GET_TICKS ，然后调用一个封装函数 send_recv() ，并且最终将调用系统调用 sys_sendrecv。因为是 BOTH 类型的，所以是先发送，然后在接送。首先是发送消息，由于上面的 task_sys 已经执行，也就是说，接收端表明它在接收一个函数，将消息复制给 task_sys 的消息指针，然后消除 task_sys 的 RECEIVING 标志位，注意，因为此时 task_sys 的 RECEIVING 标志位已经被消除了，所以 task_sys 已经被解除阻塞了，只是进程切换还没有运行而已 。至此，消息发送成功。然后，即使等待接收，很明显，虽然 task_sys 已经被解除阻塞了，但是进程切换还没有运行，所以，消息还没哟被处理，所以，用户进程被阻塞了。接下来，进程切换，** task_sys 从上次被阻塞的地方继续运行(这一点很重要，我一开始就是忽略了这一点，导致了一直无法正确理解)**。因为已经成功地接收了消息，接下来，将 ticks 赋给消息，然后发送消息给用户进程，上面说道，用户进程此时是 RECEIVING 的状态，所以，将消息复制给用户进程，然后消除 RECEIVING 的标志位，跟上面说的一样，此时用户进程已经被解除阻塞了，只是进程切换还没有运行而已。消息发送成功。然后，进程切换，切换到用户进程，从上次被阻塞的地方继续。最后用户进程的 send_recv 成功执行，get_ticks 将消息中的 ticks 返回，至此，一个消息传递的全过程完整结束了！ 简单来说，整个过程其实就是这样： task_sys 等待接收消息并且被阻塞 -&gt; 用户进程调用 get_ticks() 并且最终调用 msg_send() 还有 msg_receive() -&gt; 先是 msg_send() ，将消息复制给 task_sys ，消息发送成功 -&gt; 然后是 msg_receive() ，用户进程等待接收消息并且在该处被阻塞 -&gt; task_sys 解除阻塞，将消息处理完之后发送给用户进程，用户进程被解除阻塞 -&gt; 用户进程继续从被阻塞的地方运行，并且最终成功返回。 写得有点多，不过这个 IPC 机制真的很重要！ 遇到的 BUG一开始一运行就老是报 #PAGE FAULT 的错误，不知道是为什么？最后终于发现了 BUG 出现在 ./kernel/main.c 文件上，总共有两个疏忽的点： 就是在判断是是用户进程还是好任务的时候，如果是用户进程，一开始我是这样写的： 1p_proc = user_proc_table + i; 这其实是不对的，得这样： 1p_proc = user_proc_table + i - NR_TASKS; 这样就好了。 就是在给每一个进程赋初始的 TTY 时，也出现了像上面一样的疏忽： 1proc_table[NR_TASKS + 0] = 0; 而我就是忘记了把那个 NR_TASKS 加上去了。。。 总之，都是疏忽，都是思维漏洞！！！ 好了，第八章总算是告一段落了，接下来就要向我最期待的第九章 – 硬盘进军了！ EOF]]></content>
      <categories>
        <category>一个操作系统的实现</category>
      </categories>
      <tags>
        <tag>一个操作系统的实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个操作系统的实现 -- printf]]></title>
    <url>%2F2014%2F12%2F09%2Fchapter-8-printf%2F</url>
    <content type="text"><![CDATA[关于 vsprintf今天花了很长的时间去理解 vsprintf 这个函数并且自己把它实现了一遍。果然，比对着书打难多了。不过，确实是收获特别多，而且自己实现的话，能够加深理解，而不仅仅是从书上理解那么片面。先说一下 vsprintf() 这个函数的作用。它是 printf() 的一个处理函数，接受 3 个变量：buf 是用来存放处理完的字符串，或者是说，将 printf 的第一个参数里面的 %d 啊， %s 等用后面对应的参数替换之后的字符串。fmt 很明显，就是 printf() 的第一个参数。arg 这个参数，是指向 printf() 第二个参数以及后面参数的一个指针。理解这一点很重要。 然后这个函数的流程就是这样的，不断地扫描 fmt ，如果不是 % ，则直接赋给 buf ；如果是 % 的话，那么就看他的下一位是什么，是 % 的话，说明只是 % 的一个转义符，直接赋给 buf 。否知，如果是非零的数字，那么说明是一个 宽度说明 ，就必须计算宽度的长度。之后肯定就是控制格式符号 c，s，x 等了。在这些控制格式符中，d 和 x 由于存在进制的转换，所以要有相应的函数来处理进制转换，i2a() 就实现了这一个功能。这个函数，不得不说，是一个十分优美的实现，采用了递归。相当于先从低位到高位依次入栈，然后在依次弹栈，并不难理解，不过，我想说一点的是，这个函数的第三个参数：char **ps, 这是一个二级指针。里面的一句 1*(*ps)++ = remain; 这一句一开始老是理解不了，高级次的指针真的有点难以理解。不过，最后总算是理解了： 这一句的作用就是改变 ps 这个指针的指针指向的指针，或者说，指向的字符串 没错，指针真的十分的优美！不过，指针也十分的危险，所以，只有正确地理解并且运用，才能发挥指针的强大功能！]]></content>
      <categories>
        <category>一个操作系统的实现</category>
      </categories>
      <tags>
        <tag>一个操作系统的实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个操作系统的实现 -- IO]]></title>
    <url>%2F2014%2F11%2F29%2Fchapter-7-input%2F</url>
    <content type="text"><![CDATA[输入综述这里面有两个核心函数，keyboard_handler() 和 keyboard_read() 。其中，keyboard_handler() 是键盘中断的处理程序，作用是将扫描码加入到缓冲区中。这个缓冲区，其实就是一个 队列 ，p_tail 指向下一个要被处理的字节， p_head 指向下一个空闲的空间。而 keyboard_read 是我们的 tty 进程的守护程序，功能是从缓冲区读取扫描码并解析。 代码解析在 keyboard_read() 这个函数中有一句 make = (scan_code &amp; FLAG_BREAK ? FALSE : TRUE) ;这一句的作用是判断扫描码是 make code 还是 break code，至于 Why it works ? ，解释如下： 纵观 make code 的键盘扫描码，除去 0xE0 0xE1 开头的，没有一个可打印的扫描码是 0x8X 或者以上的，或者说，两位十六进制的扫描码化成 8 位二进制后最高位没有是 1 的。所以，与上 FLAG_BREAK : 0x80 或者 10000000 能够将 make code 清零。 之前我在看这一部分的内容的时候，有一个问题总是不能理解，就是程序究竟是如何处理 shift 被长按的呢？下载终于有了解答： 我们的 keyboard_read() 每一次都只从缓冲区里面读出一个扫描码，所以如果是按下 shift 然后按下其他的键的，第一次首先处理 shift ，标志变量 shift_l 或者 shift_r 被置位，然后读取下一个扫描码，注意由于此时 shift 键没有松手，所以下一个读到的扫描码是你按完 shift 之后按的那个讲的扫描码，而不是 shift 键的 break code。由于 shift_l 或者 shift_r 已经被置位，所以在 扫描数组读的是第二列。注意，虽然我们的扫描数组是一个一维数组，不过这个是为了方便，操作时我们还是将它视作一个二位数组，变量 column 就是用来控制列的。这样，就可以处理 shfit 键被按下了。 TTY目前 TTY 的作用就是能够控制输出的位置，它将我们的可用的 32k 显存分成了 3 块，每一块就是一个 TTY 。因为多了这样的一层，所以，在处理上也就多了一层。之前我们是这样子处理的： 按下键盘 -&gt; 触发中断 -&gt; 调用 keyboard_handler() -&gt; keyboard_handler() 函数不断地将 扫描码 加入到缓冲区中 -&gt; 任务 TTY （注意，它现在只是名字叫 TTY 而已，还不具备 TTY 的功能）不断地调用 keyboard_read() ，解析扫描码并且输出。 现在，我们有了 TTY ，可以实现 多终端 了，由于每一个 终端 都有自己的显示区间，所以，在显示的时候，过程就是这样： 按下键盘 -&gt; 触发中断 -&gt; 调用 keyboard_handler() -&gt; keyboard_handler() 函数不断地将 扫描码 加入到缓冲区中 -&gt; 任务 task_tty 设置好当前的终端 console ，再进行一系列的初始化 -&gt; 其实 task_tty() 的任务主线就是一个守护进程， 不断轮询所有的 console **，对每个 console 都进行tty_do_write() 还有 tty_do_read 不停地切换。tty_do_read() 的作用是如果轮询到的 console 是当前终端的话，那么就从键盘缓冲区中读取一个扫描码并且进行解析，然后交给 in_process() 函数进行处理，这里的处理跟我们没有 多终端 的时候有一点区别。我们之前是直接输出，现在的话，是将解析之后的 key 放在当前 console 自己的缓冲区。** tty_do_write() 的作用就是在当前 console 的缓冲区里面读取一个已经解析好的 key 值，然后交给 out_char() 进行相应的输出。 这样，这个 TTY 的过程应该就相当的清晰了～ 遇到的问题 今天，在编译的时候遇到了一个很奇怪的问题，一直报下面的错误: static declaration of ‘tty_do_read’ follows non-static declaration 一直无解，百度以后，发现了错误的根源： 出这个问题是把实现放在调用后面了C 语言里面要么需要先申明函数，要么就必须把函数实现放到函数调用之前 经验前几天在实现 printf 的时候，出现了一个问题，然后解决无果。因此懈怠了很多天，今天终于把问题解决了。我发现，解决问题的方法就是从问题本身出发，根据问题找到相应的出错点，在找到相应的代码，一点一点地找，总会找到的。不过，这需要你对你的代码相当地熟悉。 这一章总算是结束了，向 下篇 进军！！！ EOF]]></content>
      <categories>
        <category>一个操作系统的实现</category>
      </categories>
      <tags>
        <tag>一个操作系统的实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个操作系统的实现 -- 系统调用]]></title>
    <url>%2F2014%2F11%2F28%2Fsystem-call%2F</url>
    <content type="text"><![CDATA[综述首先我们来看看我们的系统调用是怎么工作的。我们的程序要进行一个系统调用，得有东西让他调用吧。所以我们必须有一个 系统调用的封装 。在我们的例子中，就是 get_ticks() 这个函数。系统调用一般都是通过中断来实现的，所以我们要统一一个系统调用的中断号，初始化 idt ，指明该中断的对应处理程序。然后，一个中断肯定不止实现一个功能，所以，我们要同一个 量 来告诉系统我们要的是哪个功能，在我们的实现中，我们是通过对 eax 这个寄存器赋予不同的值来指明对应的功能的。由于是中断，所以当中断发生了之后，我们当然要保存被中断的进程的信息，所以我们要用到之前用到的 save 函数，但是由于函数里面已经对 eax 进行了赋值，所以我们必须将 save 中与 eax 的相关寄存器都换掉。然后，我们前面说过要实现多个功能，所以我们建立了一个数组 sys_call_table ，根据 eax 的值在数组中找到相应的处理程序。所以，简单来说，系统调用就是这样的： 用户调用系统调用的封装函数 -&gt; 封装函数赋值 eax ，执行中断 -&gt; 中断处理程序在 sys_call_table 数组中根据 eax 找到最终要执行的程序，并且执行之。至此，一个完整的中断结束~ 遇到的一些问题昨天在实现的时候，发现了输出的 ticks 一直都是 0 ，一开始也不知道为什么，还稀里糊涂地在那里 Debug 里半天，直到刚才才找到了错误之处。很简单，我错误地将 sys_get_ticks() 的返回值弄成了 void 。。。 一开始的时候，一启动就会产生 #GP 的错误，之后才发现，我在 init_decriptor() 初始化 sys_call 的地方，习惯性地将 attr 弄成 PRIVILEGE_KERL ，其实应该是 PRIVILEGE_USER 。既然犯错误了，顺便复习一下 DPL 的知识。之所以会产生 #GP 的错误，主要是 DPL 在起作用。对于调用门来说，DPL 规定了当前执行的程序或任务可以访问此调用门的最低特权级，而调用这个中断的 —— 我们的进程 —— 特权级是 1，很明显是低于 0 的，所以才会产生这个错误。 今天在实现的时候，发现有出现了很奇怪的 #GP 错误（这是我用 bochs 的 snapshot 截下来的）： 123456789101112131415Exception! --&gt; #GP General Protection ßEFLAGS:0x11096CS:0x8EIP:0x306F4Error code:0xCBASEADDRL BASEADDRH LENGTHLOW LENGTHH TYPE00000000h 00000000h 0009F000h 00000000h 00000001h0009F000h 00000000h 00001000h 00000000h 00000002h000E8000h 00000000h 00018000h 00000000h 00000002h00100000h 00000000h 01EF0000h 00000000h 00000001h01FF0000h 00000000h 00010000h 00000000h 00000003hFFFC0000h 00000000h 00040000h 00000000h 00000002hRAM SIZE : 01FF0000h-----&quot;cstart&quot; begins----------&quot;cstart&quot; ends-----~~~~~~~~~~~ kernel_main ~~~~~~~~~~~~B. 一开始我一直不知道什么怎么解决。然后我就乱弄，然后错误就解决了。虽然我现在还是不知道究竟是什么错误，不过我说一下我的解决方法，用同样的问题可以参考一下。其实没有什么的，就是用 bximage 重新建立一个 a.img ,问题就解决了。 下午在实现的时候，出现了一个很奇怪的 bug ，输出到一半就停止了，又是长时间的 Debug ，总算找到了，又是一个低级的粗心错误。我感觉到，*粗心的程序员注定是短命的，因为他们把大部分时间花在了毫无意义的 Debug *。这个一定要改！！！！ 就是把 schedule() 函数写错了： 1234567891011121314151617181920PUBLIC void schedule()&#123; PROCESS *p; int greatest_ticks = 0; while (!greatest_ticks) &#123; for (p = proc_table; p &lt; proc_table + NR_TASKS; p++) &#123; if (p-&gt;ticks &gt; greatest_ticks) &#123; greatest_ticks = p-&gt;ticks; p_proc_ready = p; &#125; &#125; &#125; if (!greatest_ticks) &#123; for (p = proc_table; p &lt; proc_table + NR_TASKS; p++) &#123; p-&gt;ticks = p-&gt;priority; &#125; &#125;&#125; 不知道你看出来了没有，就是把 if 这一行弄在了 while 循环之外了。。。 顺便解释一下这个函数的作用吧。其实就是在进程表中找到 ticks 最大的，最为下一个要运行的进程。如果所有的进程的 ticks 都为 0，那么就将所有的进程的 ticks 初始化为 priority 。而上面的错误之所以会发生，就是当所有的进程的 ticks 都为 0 的时候，由于 greatest_ticks 一直为零，所以就死循环了。 好了，第六章到此结束了。错误不少，收获不少。向第七章进军！ EOF]]></content>
      <categories>
        <category>一个操作系统的实现</category>
      </categories>
      <tags>
        <tag>一个操作系统的实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个操作系统的实现 -- 多进程]]></title>
    <url>%2F2014%2F11%2F25%2Fmuti-process%2F</url>
    <content type="text"><![CDATA[单进程 赋值 tss.esp0 首先我们要理解整个进程与中断的过程： 在 restart() 函数运行之后，我们的进程开始运行，系统现在是在 ring1 特权级上。由于我们已经开启了时钟中断，所以，时钟中断会以一定频率发生。注意，要好好理解 中断 这个概念。一开始我一直无法理解，为什么我们的第一个进程是一个死循环，但是中断依旧能够发生。其实，中断就是将当前的进程终止，去执行其他任务，当该任务完成了之后，就会从当前进程停止处继续。这一点很重要，到后面的 IPC 都要依靠这个概念。 由于中断发生时出现了特权级转换： ring1 -&gt; ring0 所以存在堆栈的转换。（具体请看《一个操作系统的实现》 P99）堆栈的 ss 和 esp 存储在 tss 里面。在 restart() 我们已经准备好了 tss0.esp ，是指向当前进程的 STACK_FRAME 的最高处。至于为什么要指向这里，那是因为 ring1 -&gt; ring0 的转换，不仅存在堆栈的切换，而且还会将 eip cs eflags esp ss 这些压入新的堆栈中，这跟我们的 struct stack_frame 的顺序完全一致。最后一个疑问，为什么在中断处理程序中要一直给 tss.esp0 赋值？因为我们现在是单进程，所以这个赋值只是一个重复而已。不过，后面我们实现了多进程了之后，这个赋值操作就十分地重要了。 内核栈 首先，中断处理程序的堆栈就是 tss.esp0 ，而这里面存有我们进程的所有信息，所以如果我们的中断处理程序要进行一些函数操作，堆栈里面的信息可能会被破坏掉，所以，我们要切换到 内核栈 。这样，我们就可以放心地进行一系列的函数操作了。 多进程由于目前来说，所谓的 多进程 其实除了 进程体，进程的堆栈，进程的 ldt selector 不一样以外，其他的都是一样的，所以我们可以用一个循环来进行批量的初始化。我们把 进程体 用一个新的数组来存储： task_table[] ，每一次循环，就将 进程表 里的 eip 赋为 task_table[] 相应的元素。 同时，由于出现了多进程，所以自然而然地有了所谓的 进程调度 了。目前的进程调度没有任何高明之处，就是一个顺序循环调度。进程调度了之后，必须重新切换 ldt ，并且准备好 tss.esp0 ，以便下一次时钟中断发生后，能够将进程的状态储存在正确的堆栈里。这里要注意，进程的状态都是储存在自己的堆栈的。 添加进程的流程： 在 kernel/main.c 添加一个进程体 在 include/proto.h 声明进程体 在 kernel/global.c 的 task_table 添加一个对应的项 增加 include/proc.h 中的 NR_TASKS 以及对应的栈以及总栈大小 Minix 的中断处理不得不说，看了 Minux 的中断处理之后发现原来代码可以这么优美的。这里主要解释三点： 关于 retadr在中断处理程序一开始，我们是调用了 save 这个函数，然后注意，由于 call 操作，所以，会将下一条要执行的指令的 eip 压入栈中，又由于中断处理程序存在 ring1-&gt;ring0 ，所以会将 ss sp eflag cs eip 进行压栈，所以在 *下一条要执行的指令的 eip 就是之前我们一直忽略的 retadr *。 关于 中断重入就提一点，如果是发生了中断重入的话，则回复的是上一次被重入的中断状态 hwint00 中的最后一句 ret这又是一个非常优美的一个做法，主要到我们在 save 里面有 push 东西的，而 ret 就是把栈中的赋给 eip 而已。所以，这给了我们一个启示，ret 不一定要依赖 call ，可以按需进行压栈，需要的时候， ret 就行了 至此，进程告一段落，开始系统调用。 EOF]]></content>
      <categories>
        <category>一个操作系统的实现</category>
      </categories>
      <tags>
        <tag>一个操作系统的实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个操作系统的实现 -- 单进程]]></title>
    <url>%2F2014%2F11%2F24%2Fchapter-6-single-process%2F</url>
    <content type="text"><![CDATA[经过了一天的努力，在刚才，总算出现了下面的激动人心的画面了：真的很高兴，趁着还热乎，赶快记下来：综述第六章是 进程 。不过现在这一章的 进程 其实是有点取巧的，它是事先将一个进程需要的寄存器，栈，LDT，代码等准备好，弄成一个进程表，一切都准备完了之后，将 esp 指向进程表的顶部，然后一个一个地 pop ，然后一个跳转，我们的第一个进程就开始运行了。 由于涉及到特权级的转换，所以需要 tss 来记录每个特权级的 ss 和 esp，以便当低特权级到高特权级的切换时，堆栈也能够切换。 由于现在我还没有实现中断，所以，这个进程是一直都在运行的，没有被打断的。不过这种情况将会改变，不过得明天吧。 代码解读一开始的这些代码没有什么难的，主要提一点： 在 /kernel/main.c 的 line 27 和 line 29 这两行代码中，有一个我之前一直无法理解的，就是 &gt;&gt; 3 。现解释如下： 每个描述符的大小是 8 字节，&gt;&gt; 3 就相当于 除以8 ，这样就能够得到相应的描述符在 gdt数组 的具体位置了。 犯过的错误以及总结 typedef 之后忘记了一个最重要的 ; . 导致出现了很多匪夷所思的错误，例如提示说 #define EXTERN extern 这一句有错等等。 每增加一个新的函数就要在 inlcude/proto.h 里面添加相应的声明。 有关字符串的函数声明在 include/string.h 里面 用汇编实现的函数如果要被其他文件调用，*千万记得要用 global *。不然肯定是会报 undefined reference to xxx 的！ 一开始出现了 #GP 的错误，提示说在 cs:0x8 eip:0x3059E 这里出现了错误，反汇编了一下，发现这个位置对应的代码是： lldt [esp + P_LDT_SEL] 根据 p_proc-&gt;regs.eflags = 0x1202 ，找到了 restart() 的位置：0x30598 . 不过后来发现了，又是一个粗心导致的错误，我忘记填充 GDT 中的 LDT 的描述符了。不过，发生 #GP 的结果跟我查文档的结果是一样的。 收获又是花了很多时间在 Debugging 上，不过这一次总算有所收获。 bochs 的 sreg 指令能够查看 LDTR 。 print-stack 后面加个数字能够控制输出的栈的长度 最后，如果你们也想要体验一下 Debug 的话，我把我的 Debug 的过程公开出来： b 0x7d12 （找到 loader 之后的跳转）cb 0x9027e （跳入保护模式）cb 0x9039b （跳入内核）cb 0x310c9 （restart()） 声明，上面的地址只对于我的代码有效，你自己实现的代码与我的可能有出入 下面是原书代码的 Debug 过程： b 0x7d0a （找到 loader 之后的跳转）cb 0x9027f （跳入保护模式）cb 0x90393 （跳入内核）cb 0x3071c （这一句是根据 kernel/main.c 中很明显的 0x1202 找到的，下面几行就是 restart() 了） 大概就是这些了。明天继续～ EOF]]></content>
      <categories>
        <category>一个操作系统的实现</category>
      </categories>
      <tags>
        <tag>一个操作系统的实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[四个月整，我总算是看完了《一个操作系统的实现》]]></title>
    <url>%2F2014%2F11%2F15%2Fi-have-finished-reading-the-oranges%2F</url>
    <content type="text"><![CDATA[2014年7月15日——2014年11月15日，4个月，123天，300个小时，11章，469页。这就是我这四个月所做的事。终于，在今天完结了。虽然我知道，这只是一小步，但这仍然值得我感到十分的高兴，并且有理由让我相信这也将是实现我的操作系统的梦想的重要的一步。回想这四个月，真的有点感慨万千。用一句话来总结，那就是 Linus 的那句名言： Read the FUCKING source code ! 确实是这样子的。这四个月我做的最多的事就是：读代码。从一开始的晦涩难懂的汇编，到后面的十分灵活且技巧性十足的 C ，经常让我抓狂不止。然而，抱怨过后，还是得硬着头皮读下去。各种各样的新东西： 保护模式 , GDT , IDT , Descriptor , CPL , DPL , IRQ , ELF , 进程 , IO , IPC , 硬盘驱动 , 内存管理 …… 每一个都让我头疼不已。不过，现在再来回看过去，真的就如苏东坡的那阕词一般： 回首向来萧瑟处 归去 也无风雨夜无晴 其实，这四个月我也有偷懒过。例如暑假回家的那一个月就基本上没有看过书，有的时候被某一个点卡住，搞得心情很不好，就去睡觉，睡醒了继续看，还是看不懂，继续睡 …… 接下来我的计划是，从书本跨向实践。我要开始着手实现我的操作系统的代码了。之前国庆放假7天，我花了6天在打代码，已经实现了一部分。不过那个时候是因为看不下书，后来由于又看懂了，实现便懈怠了。 现在距离 2014 年还剩下 一个多月，希望能够在今年内真真正正地实现一个 操作系统 的原型。因为，我已经 20 岁了，周杰伦在 20 岁的时候发布了第一张个人专辑，乔布斯在 20 岁的时候创办了 苹果公司 。这两个我最尊敬的人在他们的 20 岁都已经很有所成就了。我想，我也行。 话说周杰伦今年的 12 月份会发布新专辑，所以那个时候，边听杰伦的新歌，边敲代码，肯定是一件非常爽的事情~ 就是这样~ （P.S. 我开这个博客以来到现在访问者只有我一个人，我不知道第一个访问者会是谁？期待ing ）]]></content>
      <categories>
        <category>感</category>
      </categories>
      <tags>
        <tag>一个操作系统的实现</tag>
      </tags>
  </entry>
</search>
